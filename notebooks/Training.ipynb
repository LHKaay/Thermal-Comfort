{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 1.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gating_scores = torch.tensor([[0.5,0.6,0.87]])\n",
    "_, topk_indices = gating_scores.topk(2, dim=1, sorted=False)\n",
    "torch.zeros_like(gating_scores).scatter_(1, topk_indices, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device !!!\n"
     ]
    }
   ],
   "source": [
    "device = (\"cuda\" if torch.cuda.is_available()\n",
    "          else \"cpu\")\n",
    "print(f\"Using {device} device !!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_mode = [\"ta\", \"tc\", \"ts\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2843, -0.3040, -1.1227],\n",
      "        [ 0.7276,  0.0094,  0.0984]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.5555, 0.3085, 0.1360],\n",
       "        [0.4949, 0.2413, 0.2638]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = torch.nn.Softmax(dim=1)\n",
    "input = torch.randn(2, 3)\n",
    "print(input)\n",
    "output = m(input)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4432,  0.7736,  0.1146,  0.6335, -0.6774,  0.6784,  0.8558,  1.0508,\n",
       "          0.7136, -1.4981,  0.3902, -0.2166,  0.4850, -0.7565, -0.3012, -1.0641,\n",
       "         -0.3352, -0.7550,  0.2941,  0.5162,  0.5933, -1.1094, -0.3217,  0.0125,\n",
       "          0.4780, -0.9503,  0.4873,  0.7150, -0.1319,  0.8677,  0.8407,  0.5326,\n",
       "          0.0919, -0.3411,  0.2880, -0.7073, -0.0218,  1.2305,  0.3827,  1.6726,\n",
       "          0.4008, -1.0680, -1.2793, -0.9068,  0.1043,  0.3520,  0.4219]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_input = torch.randn(1,47,requires_grad=False).to('cuda')\n",
    "torch_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 47])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = MoE.MixtureOfExperts(input_dim=47,hidden_dim=128, output_dim=3, num_experts=5, k=2).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.1117, -0.8290, -2.6413, -0.3264, -0.6822],\n",
      "        [ 0.3927,  2.4550,  0.1521, -0.8914,  1.0866],\n",
      "        [ 0.9241,  1.1422, -1.7986,  1.1306,  0.0728]], requires_grad=True) tensor([[-1.6564, -0.5222, -0.0663, -1.0255,  1.0090],\n",
      "        [ 1.5325, -1.3520,  0.3681, -0.9482, -0.9569],\n",
      "        [-0.4652, -0.2086, -0.6829, -0.2027, -0.3209]])\n",
      "tensor(2.4881, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss = torch.nn.MSELoss()\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.randn(3, 5)\n",
    "\n",
    "print(input, target)\n",
    "output = loss(input, target)\n",
    "output.backward()\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.6719313263893127]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch_input).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.onnx.export(model, torch_input, 'MoETransformerModel.onnx', input_names=[\"features\"], output_names=[\"ThermalVotes\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThermalDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df:pd.DataFrame):\n",
    "        self.df = df\n",
    "        self.features = df[['month', 'day', 'hour', 'minute',\n",
    "                'room_height', 'room_length', 'room_width', \n",
    "                'person_age', 'person_height', 'person_weight', 'person_cloth', 'person_activity',\n",
    "                'indoor_operative_temperature', 'indoor_mean_radiant_temp', 'indoor_asymmetry_temp', 'indoor_humidity', 'indoor_air_velocity',\n",
    "                'ashrae_predicted_mean_vote', 'ashrae_predicted_percentage_of_dissatisfied',\n",
    "                'person_gender_Female', 'person_gender_Male', \n",
    "                'season_Summer Season', 'season_Transition Season', 'season_Winter Season',\n",
    "                'climate_zone_Cold zone',\n",
    "                'climate_zone_Hot summer and cold winter zone',\n",
    "                'climate_zone_Severe cold zone', 'building_type _Educational',\n",
    "                'building_type _Office', 'building_type _Residential',\n",
    "                'building_function_Bedroom', 'building_function_Classroom',\n",
    "                'building_function_Living room', 'building_function_Office',\n",
    "                'building_operation_mode_Air conditioning heating',\n",
    "                'building_operation_mode_Ceiling capillary heating',\n",
    "                'building_operation_mode_Cold radiation ceiling cooling',\n",
    "                'building_operation_mode_Convection cooling',\n",
    "                'building_operation_mode_Convection heating',\n",
    "                'building_operation_mode_Naturally Ventilated',\n",
    "                'building_operation_mode_Others',\n",
    "                'building_operation_mode_Radiant floor cooling',\n",
    "                'building_operation_mode_Radiant floor heating',\n",
    "                'building_operation_mode_Radiator heating']]\n",
    "        self.thermal_accept = df[[\"label_thermal_acceptability_vote\"]]\n",
    "        self.thermal_comfort = df[[\"label_thermal_comfort_vote\"]]\n",
    "        self.thermal_sensation = df[[\"label_thermal_sensation_vote\"]]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__ (self, index:int):\n",
    "        features = torch.tensor(self.features.loc[index].to_list())\n",
    "        \n",
    "        thermal_accept = torch.tensor(self.thermal_accept.loc[index].to_list()[0]).type(torch.LongTensor)\n",
    "        thermal_comfort = torch.tensor(self.thermal_comfort.loc[index].to_list()[0]).type(torch.LongTensor)\n",
    "        thermal_sensation = torch.tensor(self.thermal_sensation.loc[index].to_list()[0]).type(torch.LongTensor)\n",
    "        \n",
    "        sample = {\"features\":features, \"thermal_accept\":thermal_accept, \"thermal_comfort\":thermal_comfort, \"thermal_sensation\":thermal_sensation }\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training shape: (2987, 47) \n",
      "Validation shpae: (996, 47) \n",
      "Testing shape: (996, 47)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\LHK\\Study\\Lab\\SmartCity\\3.Others\\ThermalComfort\\.conda\\lib\\site-packages\\numpy\\core\\fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"Dataset\\clean_data\\chinese_ver0.1.csv\")\n",
    "df_train, df_valid, df_test = np.split(df.sample(frac=1, random_state=42), [int(.6*len(df)), int(.8*len(df))])\n",
    "\n",
    "\n",
    "train_data = ThermalDataset(df_train.reset_index(drop=True))\n",
    "valid_data = ThermalDataset(df_valid.reset_index(drop=True))\n",
    "test_data = ThermalDataset(df_test.reset_index(drop=True))\n",
    "\n",
    "print(f\"Training shape: {df_train.shape} \\nValidation shpae: {df_valid.shape} \\nTesting shape: {df_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2987"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(train_data, batch_size= BATCH_SIZE)\n",
    "valid_dataloader = torch.utils.data.DataLoader(valid_data, batch_size= BATCH_SIZE)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_data, batch_size= BATCH_SIZE)\n",
    "\n",
    "examples = iter(test_dataloader)\n",
    "example_data = next(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'features': tensor([[0.7273, 0.7333, 0.6250,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.3636, 0.2000, 0.6250,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.4667, 0.4167,  ..., 0.0000, 0.0000, 1.0000],\n",
       "         ...,\n",
       "         [1.0000, 0.9667, 0.9167,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [1.0000, 0.4667, 0.5000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.4333, 0.7917,  ..., 0.0000, 0.0000, 0.0000]]),\n",
       " 'thermal_accept': tensor([3, 1, 3, 3, 1, 3, 1, 1, 1, 3, 3, 1, 1, 3, 3, 3, 1, 1, 1, 3, 2, 2, 1, 3,\n",
       "         1, 1, 3, 3, 3, 3, 3, 1]),\n",
       " 'thermal_comfort': tensor([0, 1, 1, 0, 0, 0, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,\n",
       "         1, 1, 0, 0, 0, 0, 0, 1]),\n",
       " 'thermal_sensation': tensor([3, 3, 4, 2, 3, 2, 3, 6, 2, 5, 3, 3, 3, 2, 3, 3, 3, 4, 2, 2, 5, 3, 2, 3,\n",
       "         3, 3, 4, 4, 3, 3, 3, 2])}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThermalNeuralModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ThermalNeuralModel, self).__init__()\n",
    "        \n",
    "        # self.net = torchvision.models.resnet18(pretrained=True)\n",
    "        self.n_features = 44\n",
    "\n",
    "        self.fc1 = torch.nn.Linear(self.n_features,self.n_features)\n",
    "        self.relu1 = torch.nn.ReLU()\n",
    "        self.final1 = torch.nn.Linear(self.n_features, 4)\n",
    "\n",
    "        self.fc2 = torch.nn.Linear(self.n_features,self.n_features)\n",
    "        self.relu2 = torch.nn.ReLU()\n",
    "        self.final2 = torch.nn.Linear(self.n_features, 5)\n",
    "\n",
    "        self.fc3 = torch.nn.Linear(self.n_features,self.n_features)\n",
    "        self.relu3 = torch.nn.ReLU()\n",
    "        self.final3 = torch.nn.Linear(self.n_features, 7)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out1 = self.fc1(x)\n",
    "        out1 = self.relu1(out1)\n",
    "        thermal_accept = self.final1(out1)\n",
    "\n",
    "        out2 = self.fc2(x)\n",
    "        out2 = self.relu2(out2)\n",
    "        thermal_comfort = self.final2(out2)\n",
    "\n",
    "        out3 = self.fc3(x)\n",
    "        out3 = self.relu3(out3)\n",
    "        thermal_sensation = self.final3(out3)\n",
    "\n",
    "        return thermal_accept, thermal_comfort, thermal_sensation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hard Parameter Sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThermalNeuralModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ThermalNeuralModel, self).__init__()\n",
    "        \n",
    "        # self.net = torchvision.models.resnet18(pretrained=True)\n",
    "        self.n_features = 44\n",
    "\n",
    "        self.inputlayer = torch.nn.Linear(self.n_features,60)\n",
    "        self.tanh1 = torch.nn.Tanh()\n",
    "        self.hiddenlayer1 = torch.nn.Linear(60,80)\n",
    "        self.tanh2 = torch.nn.Tanh()\n",
    "        self.hiddenlayer2 = torch.nn.Linear(80,100)\n",
    "        self.tanh3 = torch.nn.Tanh()\n",
    "        self.hiddenlayer3 = torch.nn.Linear(100,120)\n",
    "        self.tanh4 = torch.nn.Tanh()\n",
    "        self.hiddenlayer4 = torch.nn.Linear(120,150)\n",
    "        self.tanh5 = torch.nn.Tanh()\n",
    "\n",
    "        self.accept = torch.nn.Linear(150, 4)\n",
    "\n",
    "        # self.fc2 = torch.nn.Linear(self.n_features,self.n_features)\n",
    "        # self.relu2 = torch.nn.ReLU()\n",
    "        self.comfort = torch.nn.Linear(150, 5)\n",
    "\n",
    "        # self.fc3 = torch.nn.Linear(self.n_features,self.n_features)\n",
    "        # self.relu3 = torch.nn.ReLU()\n",
    "        self.sensation = torch.nn.Linear(150, 7)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.inputlayer(x)\n",
    "\n",
    "        out = self.tanh1(out)\n",
    "        out = self.hiddenlayer1(out)\n",
    "\n",
    "        out = self.tanh2(out)\n",
    "        out = self.hiddenlayer2(out)\n",
    "\n",
    "        out = self.tanh3(out)\n",
    "        out = self.hiddenlayer3(out)\n",
    "\n",
    "        out = self.tanh4(out)\n",
    "        out = self.hiddenlayer4(out)\n",
    "\n",
    "        out = self.tanh5(out)\n",
    "\n",
    "        thermal_accept = self.accept(out)\n",
    "        thermal_comfort = self.comfort(out)\n",
    "        thermal_sensation = self.sensation(out)\n",
    "\n",
    "        return thermal_accept, thermal_comfort, thermal_sensation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net = torchvision.models.resnet34(pretrained=True)\n",
    "# lr = 1e-3\n",
    "# model = ThermalNeuralModel().to(device=device)\n",
    "\n",
    "# ts_loss = torch.nn.CrossEntropyLoss()\n",
    "# tc_loss = torch.nn.CrossEntropyLoss()\n",
    "# ta_loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.09)\n",
    "\n",
    "\n",
    "# # writer.add_graph(model, example_data[\"features\"].to(device))\n",
    "\n",
    "# run = wandb.init(\n",
    "#     project=\"Thermal-with-Fully-Chinese\",\n",
    "#     config={\n",
    "#         'learning_rate': lr,\n",
    "#         \"epochs\": EPOCH,\n",
    "#     }\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    \"method\": \"random\",\n",
    "    \"name\": \"sweep\",\n",
    "    \"metric\": {\"goal\": \"minimize\", \"name\": \"total_vloss\"},\n",
    "    \"parameters\": {\n",
    "        \"batch_size\": {\"values\": [32, 64, 128]},\n",
    "        \"epochs\": {\"values\": [500, 1000, 1500, 2000, 2500, 3000, 3500, 4000, 4500, 5000, 6000, 7000 ]},\n",
    "        \"lr\": {\"max\": 0.01, \"min\": 0.00001},\n",
    "    },\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    wandb.init(\n",
    "        project=\"Thermal-with-Fully-Chinese\"\n",
    "    )\n",
    "    \n",
    "    lr = wandb.config.lr\n",
    "    BATCH_SIZE = wandb.config.batch_size\n",
    "    EPOCH = wandb.config.epochs\n",
    "    # total_step = len(train_dataloader)\n",
    "\n",
    "    model = ThermalNeuralModel().to(device=device)\n",
    "\n",
    "    ts_loss = torch.nn.CrossEntropyLoss()\n",
    "    tc_loss = torch.nn.CrossEntropyLoss()\n",
    "    ta_loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.09)\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_data, batch_size= BATCH_SIZE)\n",
    "    valid_dataloader = torch.utils.data.DataLoader(valid_data, batch_size= BATCH_SIZE)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_data, batch_size= BATCH_SIZE)\n",
    "\n",
    "    # writer.add_graph(model, example_data[\"features\"].to(device))\n",
    "\n",
    "    \n",
    "\n",
    "    # running_accept_loss = 0.0\n",
    "    # running_comfort_loss = 0.0\n",
    "    # running_sensation_loss = 0.0\n",
    "\n",
    "    # running_accept_correct = 0\n",
    "    # running_comfort_correct = 0\n",
    "    # running_sensation_correct = 0\n",
    "\n",
    "    for epoch in range(EPOCH):\n",
    "        model.train()\n",
    "        total_training_loss = 0\n",
    "        total_valid_loss = 0\n",
    "\n",
    "        #########################################\n",
    "        ############ Training process ###########\n",
    "        #########################################\n",
    "        running_accept_loss = 0.0\n",
    "        running_comfort_loss = 0.0\n",
    "        running_sensation_loss = 0.0\n",
    "\n",
    "        running_accept_correct = 0\n",
    "        running_comfort_correct = 0\n",
    "        running_sensation_correct = 0\n",
    "        for i, data in enumerate(train_dataloader):\n",
    "            inputs = data[\"features\"].to(device=device)\n",
    "\n",
    "            thermal_accept = data[\"thermal_accept\"].to(device=device)\n",
    "            thermal_comfort = data[\"thermal_comfort\"].to(device=device)\n",
    "            thermal_sensation = data[\"thermal_sensation\"].to(device=device)\n",
    "\n",
    "            thermal_accept_output, thermal_comfort_output, thermal_sensation_output  = model(inputs)\n",
    "\n",
    "            loss_ta = ta_loss(thermal_accept_output, thermal_accept)\n",
    "            loss_tc = tc_loss(thermal_comfort_output, thermal_comfort)\n",
    "            loss_ts = ts_loss(thermal_sensation_output, thermal_sensation)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            total_loss =  loss_ta + loss_tc + loss_ts\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_accept_loss += loss_ta.item()\n",
    "            running_comfort_loss += loss_tc.item()\n",
    "            running_sensation_loss += loss_ts.item()\n",
    "\n",
    "            _, thermal_accept_predicted = torch.max(thermal_accept_output,1)\n",
    "            running_accept_correct += (thermal_accept_predicted == thermal_accept).sum().item()\n",
    "\n",
    "            _, thermal_comfort_predicted = torch.max(thermal_comfort_output,1)\n",
    "            running_comfort_correct += (thermal_comfort_predicted == thermal_comfort).sum().item()\n",
    "\n",
    "            _, thermal_sensation_predicted = torch.max(thermal_sensation_output,1)\n",
    "            running_sensation_correct += (thermal_sensation_predicted == thermal_sensation).sum().item()\n",
    "\n",
    "            total_training_loss += total_loss\n",
    "\n",
    "\n",
    "        #########################################\n",
    "        ########## Validation process ###########\n",
    "        #########################################\n",
    "        valid_accept_loss = 0.0\n",
    "        valid_comfort_loss = 0.0\n",
    "        valid_sensation_loss = 0.0\n",
    "\n",
    "        valid_accept_correct = 0\n",
    "        valid_comfort_correct = 0\n",
    "        valid_sensation_correct = 0\n",
    "        # Set the model to evaluation mode, disabling dropout and using population statistics for batch normalization.\n",
    "        model.eval()\n",
    "        # Disable gradient computation and reduce memory consumption.\n",
    "        with torch.no_grad():\n",
    "            for i, vdata in enumerate(valid_dataloader):\n",
    "                inputs = vdata[\"features\"].to(device=device)\n",
    "\n",
    "                thermal_accept = vdata[\"thermal_accept\"].to(device=device)\n",
    "                thermal_comfort = vdata[\"thermal_comfort\"].to(device=device)\n",
    "                thermal_sensation = vdata[\"thermal_sensation\"].to(device=device)\n",
    "\n",
    "                thermal_accept_output, thermal_comfort_output, thermal_sensation_output  = model(inputs)\n",
    "\n",
    "                vloss_ta = ta_loss(thermal_accept_output, thermal_accept)\n",
    "                vloss_tc = tc_loss(thermal_comfort_output, thermal_comfort)\n",
    "                vloss_ts = ts_loss(thermal_sensation_output, thermal_sensation)\n",
    "\n",
    "                total_vloss =  vloss_ta + vloss_tc + vloss_ts\n",
    "\n",
    "                valid_accept_loss += vloss_ta.item()\n",
    "                valid_comfort_loss += vloss_tc.item()\n",
    "                valid_sensation_loss += vloss_ts.item()\n",
    "\n",
    "                _, thermal_accept_predicted = torch.max(thermal_accept_output,1)\n",
    "                valid_accept_correct += (thermal_accept_predicted == thermal_accept).sum().item()\n",
    "\n",
    "                _, thermal_comfort_predicted = torch.max(thermal_comfort_output,1)\n",
    "                valid_comfort_correct += (thermal_comfort_predicted == thermal_comfort).sum().item()\n",
    "\n",
    "                _, thermal_sensation_predicted = torch.max(thermal_sensation_output,1)\n",
    "                valid_sensation_correct += (thermal_sensation_predicted == thermal_sensation).sum().item()\n",
    "\n",
    "                total_valid_loss += total_vloss\n",
    "\n",
    "\n",
    "            # if(i+1)%10==0:\n",
    "        print(f\"epoch {epoch+1} / {EPOCH}, loss = {total_loss.item():.4f}, valid loss:  {total_vloss.item():.4f}\")\n",
    "\n",
    "        # writer.add_scalar(\"Thermal Accept Loss\", running_accept_loss / len(train_data), epoch)\n",
    "        # writer.add_scalar(\"Thermal Comfort Loss\", running_comfort_loss / len(train_data), epoch)\n",
    "        # writer.add_scalar(\"Thermal Sensation Loss\", running_sensation_loss / len(train_data), epoch)\n",
    "\n",
    "        # writer.add_scalar(\"Thermal Accept Accuracy\", running_accept_correct / len(train_data), epoch)\n",
    "        # writer.add_scalar(\"Thermal Comfort Accuracy\", running_comfort_correct / len(train_data), epoch)\n",
    "        # writer.add_scalar(\"Thermal Sensation Accuracy\", running_sensation_correct / len(train_data), epoch)\n",
    "\n",
    "        wandb.log({\"Thermal Accept Loss\": running_accept_loss/len(train_data), \n",
    "                    \"Thermal Comfort Loss\": running_comfort_loss/len(train_data), \n",
    "                    \"Thermal Sensation Loss\": running_sensation_loss/len(train_data),\n",
    "                \"Thermal Accept Accuracy\": running_accept_correct/len(train_data), \n",
    "                \"Thermal Comfort Accuracy\": running_comfort_correct/len(train_data), \n",
    "                \"Thermal Sensation Accuracy\": running_sensation_correct/len(train_data),\n",
    "\n",
    "                \"Thermal Accept Valid Loss\": valid_accept_loss/len(valid_data), \n",
    "                    \"Thermal Comfort Valid Loss\": valid_comfort_loss/len(valid_data), \n",
    "                    \"Thermal Sensation Valid Loss\": valid_sensation_loss/len(valid_data),\n",
    "                \"Thermal Accept Valid Accuracy\": valid_accept_correct/len(valid_data), \n",
    "                \"Thermal Comfort Valid Accuracy\": valid_comfort_correct/len(valid_data), \n",
    "                \"Thermal Sensation Valid Accuracy\": valid_sensation_correct/len(valid_data)})\n",
    "    # torch.save(model.state_dict(),\"runs/3_output_v0.4/model/howdoyoufeel.pt\")\n",
    "    # wandb.save(\"runs/3_output_v0.4/model/howdoyoufeel.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: yb81qiu6\n",
      "Sweep URL: https://wandb.ai/lhk/Thermal-with-Fully-Chinese/sweeps/yb81qiu6\n"
     ]
    }
   ],
   "source": [
    "sweep_id = wandb.sweep(sweep=sweep_config, project=\"Thermal-with-Fully-Chinese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 8at5n7mo with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 1000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.002785134533219285\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>e:\\LHK\\Study\\Lab\\SmartCity\\3.Others\\ThermalComfort\\wandb\\run-20240322_003308-8at5n7mo</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/lhk/Thermal-with-Fully-Chinese/runs/8at5n7mo' target=\"_blank\">deft-sweep-1</a></strong> to <a href='https://wandb.ai/lhk/Thermal-with-Fully-Chinese' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/lhk/Thermal-with-Fully-Chinese/sweeps/yb81qiu6' target=\"_blank\">https://wandb.ai/lhk/Thermal-with-Fully-Chinese/sweeps/yb81qiu6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/lhk/Thermal-with-Fully-Chinese' target=\"_blank\">https://wandb.ai/lhk/Thermal-with-Fully-Chinese</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/lhk/Thermal-with-Fully-Chinese/sweeps/yb81qiu6' target=\"_blank\">https://wandb.ai/lhk/Thermal-with-Fully-Chinese/sweeps/yb81qiu6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/lhk/Thermal-with-Fully-Chinese/runs/8at5n7mo' target=\"_blank\">https://wandb.ai/lhk/Thermal-with-Fully-Chinese/runs/8at5n7mo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 / 1000, loss = 4.7907, valid loss:  4.7917\n",
      "epoch 2 / 1000, loss = 4.6371, valid loss:  4.6315\n",
      "epoch 3 / 1000, loss = 4.4874, valid loss:  4.4693\n",
      "epoch 4 / 1000, loss = 4.3295, valid loss:  4.2940\n",
      "epoch 5 / 1000, loss = 4.1677, valid loss:  4.1121\n",
      "epoch 6 / 1000, loss = 4.0175, valid loss:  3.9414\n",
      "epoch 7 / 1000, loss = 3.8954, valid loss:  3.8035\n",
      "epoch 8 / 1000, loss = 3.8075, valid loss:  3.7058\n",
      "epoch 9 / 1000, loss = 3.7473, valid loss:  3.6417\n",
      "epoch 10 / 1000, loss = 3.7038, valid loss:  3.5999\n",
      "epoch 11 / 1000, loss = 3.6702, valid loss:  3.5714\n",
      "epoch 12 / 1000, loss = 3.6439, valid loss:  3.5510\n",
      "epoch 13 / 1000, loss = 3.6226, valid loss:  3.5356\n",
      "epoch 14 / 1000, loss = 3.6049, valid loss:  3.5234\n",
      "epoch 15 / 1000, loss = 3.5890, valid loss:  3.5132\n",
      "epoch 16 / 1000, loss = 3.5732, valid loss:  3.5037\n",
      "epoch 17 / 1000, loss = 3.5613, valid loss:  3.4960\n",
      "epoch 18 / 1000, loss = 3.5525, valid loss:  3.4902\n",
      "epoch 19 / 1000, loss = 3.5448, valid loss:  3.4850\n",
      "epoch 20 / 1000, loss = 3.5379, valid loss:  3.4804\n",
      "epoch 21 / 1000, loss = 3.5317, valid loss:  3.4763\n",
      "epoch 22 / 1000, loss = 3.5262, valid loss:  3.4724\n",
      "epoch 23 / 1000, loss = 3.5212, valid loss:  3.4689\n",
      "epoch 24 / 1000, loss = 3.5166, valid loss:  3.4657\n",
      "epoch 25 / 1000, loss = 3.5121, valid loss:  3.4628\n",
      "epoch 26 / 1000, loss = 3.5076, valid loss:  3.4600\n",
      "epoch 27 / 1000, loss = 3.5039, valid loss:  3.4574\n",
      "epoch 28 / 1000, loss = 3.5005, valid loss:  3.4549\n",
      "epoch 29 / 1000, loss = 3.4973, valid loss:  3.4525\n",
      "epoch 30 / 1000, loss = 3.4942, valid loss:  3.4502\n",
      "epoch 31 / 1000, loss = 3.4912, valid loss:  3.4480\n",
      "epoch 32 / 1000, loss = 3.4883, valid loss:  3.4460\n",
      "epoch 33 / 1000, loss = 3.4855, valid loss:  3.4439\n",
      "epoch 34 / 1000, loss = 3.4827, valid loss:  3.4418\n",
      "epoch 35 / 1000, loss = 3.4798, valid loss:  3.4397\n",
      "epoch 36 / 1000, loss = 3.4768, valid loss:  3.4377\n",
      "epoch 37 / 1000, loss = 3.4738, valid loss:  3.4357\n",
      "epoch 38 / 1000, loss = 3.4707, valid loss:  3.4338\n",
      "epoch 39 / 1000, loss = 3.4675, valid loss:  3.4319\n",
      "epoch 40 / 1000, loss = 3.4643, valid loss:  3.4299\n",
      "epoch 41 / 1000, loss = 3.4609, valid loss:  3.4280\n",
      "epoch 42 / 1000, loss = 3.4575, valid loss:  3.4262\n",
      "epoch 43 / 1000, loss = 3.4541, valid loss:  3.4243\n",
      "epoch 44 / 1000, loss = 3.4506, valid loss:  3.4225\n",
      "epoch 45 / 1000, loss = 3.4472, valid loss:  3.4207\n",
      "epoch 46 / 1000, loss = 3.4437, valid loss:  3.4189\n",
      "epoch 47 / 1000, loss = 3.4401, valid loss:  3.4171\n",
      "epoch 48 / 1000, loss = 3.4365, valid loss:  3.4153\n",
      "epoch 49 / 1000, loss = 3.4328, valid loss:  3.4134\n",
      "epoch 50 / 1000, loss = 3.4290, valid loss:  3.4117\n",
      "epoch 51 / 1000, loss = 3.4252, valid loss:  3.4100\n",
      "epoch 52 / 1000, loss = 3.4214, valid loss:  3.4084\n",
      "epoch 53 / 1000, loss = 3.4178, valid loss:  3.4066\n",
      "epoch 54 / 1000, loss = 3.4143, valid loss:  3.4049\n",
      "epoch 55 / 1000, loss = 3.4108, valid loss:  3.4031\n",
      "epoch 56 / 1000, loss = 3.4070, valid loss:  3.4013\n",
      "epoch 57 / 1000, loss = 3.4030, valid loss:  3.3995\n",
      "epoch 58 / 1000, loss = 3.3988, valid loss:  3.3977\n",
      "epoch 59 / 1000, loss = 3.3946, valid loss:  3.3958\n",
      "epoch 60 / 1000, loss = 3.3902, valid loss:  3.3939\n",
      "epoch 61 / 1000, loss = 3.3858, valid loss:  3.3920\n",
      "epoch 62 / 1000, loss = 3.3814, valid loss:  3.3901\n",
      "epoch 63 / 1000, loss = 3.3769, valid loss:  3.3883\n",
      "epoch 64 / 1000, loss = 3.3724, valid loss:  3.3865\n",
      "epoch 65 / 1000, loss = 3.3678, valid loss:  3.3850\n",
      "epoch 66 / 1000, loss = 3.3630, valid loss:  3.3841\n",
      "epoch 67 / 1000, loss = 3.3537, valid loss:  3.3813\n",
      "epoch 68 / 1000, loss = 3.3369, valid loss:  3.3755\n",
      "epoch 69 / 1000, loss = 3.3323, valid loss:  3.3735\n",
      "epoch 70 / 1000, loss = 3.3277, valid loss:  3.3717\n",
      "epoch 71 / 1000, loss = 3.3230, valid loss:  3.3700\n",
      "epoch 72 / 1000, loss = 3.3181, valid loss:  3.3683\n",
      "epoch 73 / 1000, loss = 3.3131, valid loss:  3.3666\n",
      "epoch 74 / 1000, loss = 3.3085, valid loss:  3.3642\n",
      "epoch 75 / 1000, loss = 3.3040, valid loss:  3.3615\n",
      "epoch 76 / 1000, loss = 3.2991, valid loss:  3.3594\n",
      "epoch 77 / 1000, loss = 3.2943, valid loss:  3.3578\n",
      "epoch 78 / 1000, loss = 3.2898, valid loss:  3.3564\n",
      "epoch 79 / 1000, loss = 3.2856, valid loss:  3.3550\n",
      "epoch 80 / 1000, loss = 3.2814, valid loss:  3.3535\n",
      "epoch 81 / 1000, loss = 3.2772, valid loss:  3.3520\n",
      "epoch 82 / 1000, loss = 3.2730, valid loss:  3.3506\n",
      "epoch 83 / 1000, loss = 3.2688, valid loss:  3.3492\n",
      "epoch 84 / 1000, loss = 3.2646, valid loss:  3.3479\n",
      "epoch 85 / 1000, loss = 3.2605, valid loss:  3.3467\n",
      "epoch 86 / 1000, loss = 3.2565, valid loss:  3.3456\n",
      "epoch 87 / 1000, loss = 3.2525, valid loss:  3.3446\n",
      "epoch 88 / 1000, loss = 3.2485, valid loss:  3.3436\n",
      "epoch 89 / 1000, loss = 3.2445, valid loss:  3.3425\n",
      "epoch 90 / 1000, loss = 3.2406, valid loss:  3.3411\n",
      "epoch 91 / 1000, loss = 3.2368, valid loss:  3.3394\n",
      "epoch 92 / 1000, loss = 3.2333, valid loss:  3.3376\n",
      "epoch 93 / 1000, loss = 3.2301, valid loss:  3.3355\n",
      "epoch 94 / 1000, loss = 3.2271, valid loss:  3.3334\n",
      "epoch 95 / 1000, loss = 3.2244, valid loss:  3.3312\n",
      "epoch 96 / 1000, loss = 3.2217, valid loss:  3.3291\n",
      "epoch 97 / 1000, loss = 3.2189, valid loss:  3.3272\n",
      "epoch 98 / 1000, loss = 3.2160, valid loss:  3.3255\n",
      "epoch 99 / 1000, loss = 3.2130, valid loss:  3.3239\n",
      "epoch 100 / 1000, loss = 3.2103, valid loss:  3.3224\n",
      "epoch 101 / 1000, loss = 3.2078, valid loss:  3.3212\n",
      "epoch 102 / 1000, loss = 3.2053, valid loss:  3.3201\n",
      "epoch 103 / 1000, loss = 3.2030, valid loss:  3.3193\n",
      "epoch 104 / 1000, loss = 3.2007, valid loss:  3.3185\n",
      "epoch 105 / 1000, loss = 3.1984, valid loss:  3.3177\n",
      "epoch 106 / 1000, loss = 3.1960, valid loss:  3.3168\n",
      "epoch 107 / 1000, loss = 3.1936, valid loss:  3.3159\n",
      "epoch 108 / 1000, loss = 3.1912, valid loss:  3.3149\n",
      "epoch 109 / 1000, loss = 3.1887, valid loss:  3.3140\n",
      "epoch 110 / 1000, loss = 3.1863, valid loss:  3.3130\n",
      "epoch 111 / 1000, loss = 3.1839, valid loss:  3.3121\n",
      "epoch 112 / 1000, loss = 3.1815, valid loss:  3.3112\n",
      "epoch 113 / 1000, loss = 3.1792, valid loss:  3.3103\n",
      "epoch 114 / 1000, loss = 3.1769, valid loss:  3.3094\n",
      "epoch 115 / 1000, loss = 3.1747, valid loss:  3.3085\n",
      "epoch 116 / 1000, loss = 3.1727, valid loss:  3.3075\n",
      "epoch 117 / 1000, loss = 3.1710, valid loss:  3.3062\n",
      "epoch 118 / 1000, loss = 3.1691, valid loss:  3.3052\n",
      "epoch 119 / 1000, loss = 3.1671, valid loss:  3.3043\n",
      "epoch 120 / 1000, loss = 3.1652, valid loss:  3.3033\n",
      "epoch 121 / 1000, loss = 3.1632, valid loss:  3.3023\n",
      "epoch 122 / 1000, loss = 3.1611, valid loss:  3.3013\n",
      "epoch 123 / 1000, loss = 3.1591, valid loss:  3.3003\n",
      "epoch 124 / 1000, loss = 3.1571, valid loss:  3.2993\n",
      "epoch 125 / 1000, loss = 3.1551, valid loss:  3.2984\n",
      "epoch 126 / 1000, loss = 3.1531, valid loss:  3.2974\n",
      "epoch 127 / 1000, loss = 3.1511, valid loss:  3.2964\n",
      "epoch 128 / 1000, loss = 3.1492, valid loss:  3.2954\n",
      "epoch 129 / 1000, loss = 3.1474, valid loss:  3.2944\n",
      "epoch 130 / 1000, loss = 3.1456, valid loss:  3.2933\n",
      "epoch 131 / 1000, loss = 3.1440, valid loss:  3.2921\n",
      "epoch 132 / 1000, loss = 3.1424, valid loss:  3.2908\n",
      "epoch 133 / 1000, loss = 3.1408, valid loss:  3.2895\n",
      "epoch 134 / 1000, loss = 3.1392, valid loss:  3.2883\n",
      "epoch 135 / 1000, loss = 3.1375, valid loss:  3.2870\n",
      "epoch 136 / 1000, loss = 3.1358, valid loss:  3.2857\n",
      "epoch 137 / 1000, loss = 3.1341, valid loss:  3.2844\n",
      "epoch 138 / 1000, loss = 3.1324, valid loss:  3.2831\n",
      "epoch 139 / 1000, loss = 3.1306, valid loss:  3.2818\n",
      "epoch 140 / 1000, loss = 3.1289, valid loss:  3.2806\n",
      "epoch 141 / 1000, loss = 3.1272, valid loss:  3.2793\n",
      "epoch 142 / 1000, loss = 3.1254, valid loss:  3.2780\n",
      "epoch 143 / 1000, loss = 3.1237, valid loss:  3.2768\n",
      "epoch 144 / 1000, loss = 3.1219, valid loss:  3.2756\n",
      "epoch 145 / 1000, loss = 3.1201, valid loss:  3.2744\n",
      "epoch 146 / 1000, loss = 3.1183, valid loss:  3.2732\n",
      "epoch 147 / 1000, loss = 3.1165, valid loss:  3.2720\n",
      "epoch 148 / 1000, loss = 3.1147, valid loss:  3.2709\n",
      "epoch 149 / 1000, loss = 3.1130, valid loss:  3.2698\n",
      "epoch 150 / 1000, loss = 3.1112, valid loss:  3.2687\n",
      "epoch 151 / 1000, loss = 3.1094, valid loss:  3.2676\n",
      "epoch 152 / 1000, loss = 3.1076, valid loss:  3.2666\n",
      "epoch 153 / 1000, loss = 3.1058, valid loss:  3.2655\n",
      "epoch 154 / 1000, loss = 3.1040, valid loss:  3.2645\n",
      "epoch 155 / 1000, loss = 3.1022, valid loss:  3.2634\n",
      "epoch 156 / 1000, loss = 3.1004, valid loss:  3.2624\n",
      "epoch 157 / 1000, loss = 3.0986, valid loss:  3.2614\n",
      "epoch 158 / 1000, loss = 3.0969, valid loss:  3.2604\n",
      "epoch 159 / 1000, loss = 3.0951, valid loss:  3.2594\n",
      "epoch 160 / 1000, loss = 3.0933, valid loss:  3.2584\n",
      "epoch 161 / 1000, loss = 3.0916, valid loss:  3.2575\n",
      "epoch 162 / 1000, loss = 3.0898, valid loss:  3.2565\n",
      "epoch 163 / 1000, loss = 3.0881, valid loss:  3.2555\n",
      "epoch 164 / 1000, loss = 3.0864, valid loss:  3.2546\n",
      "epoch 165 / 1000, loss = 3.0846, valid loss:  3.2536\n",
      "epoch 166 / 1000, loss = 3.0829, valid loss:  3.2527\n",
      "epoch 167 / 1000, loss = 3.0812, valid loss:  3.2518\n",
      "epoch 168 / 1000, loss = 3.0795, valid loss:  3.2509\n",
      "epoch 169 / 1000, loss = 3.0778, valid loss:  3.2499\n",
      "epoch 170 / 1000, loss = 3.0761, valid loss:  3.2490\n",
      "epoch 171 / 1000, loss = 3.0744, valid loss:  3.2481\n",
      "epoch 172 / 1000, loss = 3.0728, valid loss:  3.2472\n",
      "epoch 173 / 1000, loss = 3.0711, valid loss:  3.2464\n",
      "epoch 174 / 1000, loss = 3.0694, valid loss:  3.2455\n",
      "epoch 175 / 1000, loss = 3.0678, valid loss:  3.2446\n",
      "epoch 176 / 1000, loss = 3.0661, valid loss:  3.2438\n",
      "epoch 177 / 1000, loss = 3.0645, valid loss:  3.2429\n",
      "epoch 178 / 1000, loss = 3.0628, valid loss:  3.2421\n",
      "epoch 179 / 1000, loss = 3.0612, valid loss:  3.2413\n",
      "epoch 180 / 1000, loss = 3.0596, valid loss:  3.2405\n",
      "epoch 181 / 1000, loss = 3.0580, valid loss:  3.2397\n",
      "epoch 182 / 1000, loss = 3.0564, valid loss:  3.2389\n",
      "epoch 183 / 1000, loss = 3.0548, valid loss:  3.2381\n",
      "epoch 184 / 1000, loss = 3.0532, valid loss:  3.2374\n",
      "epoch 185 / 1000, loss = 3.0516, valid loss:  3.2366\n",
      "epoch 186 / 1000, loss = 3.0500, valid loss:  3.2359\n",
      "epoch 187 / 1000, loss = 3.0484, valid loss:  3.2351\n",
      "epoch 188 / 1000, loss = 3.0469, valid loss:  3.2344\n",
      "epoch 189 / 1000, loss = 3.0453, valid loss:  3.2337\n",
      "epoch 190 / 1000, loss = 3.0437, valid loss:  3.2330\n",
      "epoch 191 / 1000, loss = 3.0422, valid loss:  3.2323\n",
      "epoch 192 / 1000, loss = 3.0407, valid loss:  3.2315\n",
      "epoch 193 / 1000, loss = 3.0391, valid loss:  3.2309\n",
      "epoch 194 / 1000, loss = 3.0376, valid loss:  3.2302\n",
      "epoch 195 / 1000, loss = 3.0361, valid loss:  3.2295\n",
      "epoch 196 / 1000, loss = 3.0346, valid loss:  3.2288\n",
      "epoch 197 / 1000, loss = 3.0331, valid loss:  3.2282\n",
      "epoch 198 / 1000, loss = 3.0317, valid loss:  3.2275\n",
      "epoch 199 / 1000, loss = 3.0302, valid loss:  3.2269\n",
      "epoch 200 / 1000, loss = 3.0288, valid loss:  3.2263\n",
      "epoch 201 / 1000, loss = 3.0274, valid loss:  3.2256\n",
      "epoch 202 / 1000, loss = 3.0261, valid loss:  3.2250\n",
      "epoch 203 / 1000, loss = 3.0247, valid loss:  3.2244\n",
      "epoch 204 / 1000, loss = 3.0234, valid loss:  3.2238\n",
      "epoch 205 / 1000, loss = 3.0221, valid loss:  3.2232\n",
      "epoch 206 / 1000, loss = 3.0207, valid loss:  3.2226\n",
      "epoch 207 / 1000, loss = 3.0195, valid loss:  3.2221\n",
      "epoch 208 / 1000, loss = 3.0182, valid loss:  3.2215\n",
      "epoch 209 / 1000, loss = 3.0169, valid loss:  3.2209\n",
      "epoch 210 / 1000, loss = 3.0156, valid loss:  3.2204\n",
      "epoch 211 / 1000, loss = 3.0144, valid loss:  3.2198\n",
      "epoch 212 / 1000, loss = 3.0131, valid loss:  3.2193\n",
      "epoch 213 / 1000, loss = 3.0119, valid loss:  3.2188\n",
      "epoch 214 / 1000, loss = 3.0106, valid loss:  3.2183\n",
      "epoch 215 / 1000, loss = 3.0093, valid loss:  3.2177\n",
      "epoch 216 / 1000, loss = 3.0081, valid loss:  3.2172\n",
      "epoch 217 / 1000, loss = 3.0068, valid loss:  3.2167\n",
      "epoch 218 / 1000, loss = 3.0055, valid loss:  3.2162\n",
      "epoch 219 / 1000, loss = 3.0043, valid loss:  3.2157\n",
      "epoch 220 / 1000, loss = 3.0030, valid loss:  3.2153\n",
      "epoch 221 / 1000, loss = 3.0017, valid loss:  3.2148\n",
      "epoch 222 / 1000, loss = 3.0004, valid loss:  3.2143\n",
      "epoch 223 / 1000, loss = 2.9992, valid loss:  3.2138\n",
      "epoch 224 / 1000, loss = 2.9979, valid loss:  3.2133\n",
      "epoch 225 / 1000, loss = 2.9966, valid loss:  3.2128\n",
      "epoch 226 / 1000, loss = 2.9954, valid loss:  3.2124\n",
      "epoch 227 / 1000, loss = 2.9942, valid loss:  3.2119\n",
      "epoch 228 / 1000, loss = 2.9929, valid loss:  3.2114\n",
      "epoch 229 / 1000, loss = 2.9917, valid loss:  3.2109\n",
      "epoch 230 / 1000, loss = 2.9905, valid loss:  3.2104\n",
      "epoch 231 / 1000, loss = 2.9893, valid loss:  3.2100\n",
      "epoch 232 / 1000, loss = 2.9881, valid loss:  3.2095\n",
      "epoch 233 / 1000, loss = 2.9869, valid loss:  3.2090\n",
      "epoch 234 / 1000, loss = 2.9857, valid loss:  3.2086\n",
      "epoch 235 / 1000, loss = 2.9845, valid loss:  3.2081\n",
      "epoch 236 / 1000, loss = 2.9834, valid loss:  3.2077\n",
      "epoch 237 / 1000, loss = 2.9822, valid loss:  3.2073\n",
      "epoch 238 / 1000, loss = 2.9810, valid loss:  3.2069\n",
      "epoch 239 / 1000, loss = 2.9798, valid loss:  3.2066\n",
      "epoch 240 / 1000, loss = 2.9785, valid loss:  3.2063\n",
      "epoch 241 / 1000, loss = 2.9773, valid loss:  3.2061\n",
      "epoch 242 / 1000, loss = 2.9761, valid loss:  3.2059\n",
      "epoch 243 / 1000, loss = 2.9749, valid loss:  3.2057\n",
      "epoch 244 / 1000, loss = 2.9738, valid loss:  3.2054\n",
      "epoch 245 / 1000, loss = 2.9728, valid loss:  3.2051\n",
      "epoch 246 / 1000, loss = 2.9718, valid loss:  3.2048\n",
      "epoch 247 / 1000, loss = 2.9709, valid loss:  3.2045\n",
      "epoch 248 / 1000, loss = 2.9701, valid loss:  3.2043\n",
      "epoch 249 / 1000, loss = 2.9694, valid loss:  3.2040\n",
      "epoch 250 / 1000, loss = 2.9688, valid loss:  3.2037\n",
      "epoch 251 / 1000, loss = 2.9683, valid loss:  3.2033\n",
      "epoch 252 / 1000, loss = 2.9679, valid loss:  3.2029\n",
      "epoch 253 / 1000, loss = 2.9675, valid loss:  3.2025\n",
      "epoch 254 / 1000, loss = 2.9672, valid loss:  3.2021\n",
      "epoch 255 / 1000, loss = 2.9668, valid loss:  3.2017\n",
      "epoch 256 / 1000, loss = 2.9664, valid loss:  3.2012\n",
      "epoch 257 / 1000, loss = 2.9655, valid loss:  3.2007\n",
      "epoch 258 / 1000, loss = 2.9638, valid loss:  3.2001\n",
      "epoch 259 / 1000, loss = 2.9610, valid loss:  3.1993\n",
      "epoch 260 / 1000, loss = 2.9596, valid loss:  3.1987\n",
      "epoch 261 / 1000, loss = 2.9586, valid loss:  3.1983\n",
      "epoch 262 / 1000, loss = 2.9577, valid loss:  3.1979\n",
      "epoch 263 / 1000, loss = 2.9569, valid loss:  3.1974\n",
      "epoch 264 / 1000, loss = 2.9560, valid loss:  3.1970\n",
      "epoch 265 / 1000, loss = 2.9553, valid loss:  3.1965\n",
      "epoch 266 / 1000, loss = 2.9545, valid loss:  3.1960\n",
      "epoch 267 / 1000, loss = 2.9538, valid loss:  3.1955\n",
      "epoch 268 / 1000, loss = 2.9531, valid loss:  3.1950\n",
      "epoch 269 / 1000, loss = 2.9523, valid loss:  3.1944\n",
      "epoch 270 / 1000, loss = 2.9516, valid loss:  3.1937\n",
      "epoch 271 / 1000, loss = 2.9507, valid loss:  3.1930\n",
      "epoch 272 / 1000, loss = 2.9499, valid loss:  3.1921\n",
      "epoch 273 / 1000, loss = 2.9489, valid loss:  3.1912\n",
      "epoch 274 / 1000, loss = 2.9479, valid loss:  3.1902\n",
      "epoch 275 / 1000, loss = 2.9469, valid loss:  3.1891\n",
      "epoch 276 / 1000, loss = 2.9457, valid loss:  3.1880\n",
      "epoch 277 / 1000, loss = 2.9445, valid loss:  3.1869\n",
      "epoch 278 / 1000, loss = 2.9433, valid loss:  3.1858\n",
      "epoch 279 / 1000, loss = 2.9422, valid loss:  3.1848\n",
      "epoch 280 / 1000, loss = 2.9413, valid loss:  3.1837\n",
      "epoch 281 / 1000, loss = 2.9404, valid loss:  3.1826\n",
      "epoch 282 / 1000, loss = 2.9395, valid loss:  3.1815\n",
      "epoch 283 / 1000, loss = 2.9387, valid loss:  3.1804\n",
      "epoch 284 / 1000, loss = 2.9379, valid loss:  3.1793\n",
      "epoch 285 / 1000, loss = 2.9370, valid loss:  3.1783\n",
      "epoch 286 / 1000, loss = 2.9362, valid loss:  3.1774\n",
      "epoch 287 / 1000, loss = 2.9353, valid loss:  3.1764\n",
      "epoch 288 / 1000, loss = 2.9343, valid loss:  3.1755\n",
      "epoch 289 / 1000, loss = 2.9334, valid loss:  3.1746\n",
      "epoch 290 / 1000, loss = 2.9324, valid loss:  3.1738\n",
      "epoch 291 / 1000, loss = 2.9314, valid loss:  3.1729\n",
      "epoch 292 / 1000, loss = 2.9303, valid loss:  3.1721\n",
      "epoch 293 / 1000, loss = 2.9293, valid loss:  3.1713\n",
      "epoch 294 / 1000, loss = 2.9283, valid loss:  3.1704\n",
      "epoch 295 / 1000, loss = 2.9274, valid loss:  3.1696\n",
      "epoch 296 / 1000, loss = 2.9265, valid loss:  3.1686\n",
      "epoch 297 / 1000, loss = 2.9258, valid loss:  3.1677\n",
      "epoch 298 / 1000, loss = 2.9251, valid loss:  3.1668\n",
      "epoch 299 / 1000, loss = 2.9246, valid loss:  3.1660\n",
      "epoch 300 / 1000, loss = 2.9242, valid loss:  3.1654\n",
      "epoch 301 / 1000, loss = 2.9238, valid loss:  3.1649\n",
      "epoch 302 / 1000, loss = 2.9235, valid loss:  3.1646\n",
      "epoch 303 / 1000, loss = 2.9232, valid loss:  3.1644\n",
      "epoch 304 / 1000, loss = 2.9229, valid loss:  3.1643\n",
      "epoch 305 / 1000, loss = 2.9226, valid loss:  3.1642\n",
      "epoch 306 / 1000, loss = 2.9221, valid loss:  3.1639\n",
      "epoch 307 / 1000, loss = 2.9214, valid loss:  3.1633\n",
      "epoch 308 / 1000, loss = 2.9204, valid loss:  3.1621\n",
      "epoch 309 / 1000, loss = 2.9189, valid loss:  3.1603\n",
      "epoch 310 / 1000, loss = 2.9166, valid loss:  3.1580\n",
      "epoch 311 / 1000, loss = 2.9139, valid loss:  3.1572\n",
      "epoch 312 / 1000, loss = 2.9169, valid loss:  3.1591\n",
      "epoch 313 / 1000, loss = 2.9178, valid loss:  3.1592\n",
      "epoch 314 / 1000, loss = 2.9171, valid loss:  3.1573\n",
      "epoch 315 / 1000, loss = 2.9179, valid loss:  3.1583\n",
      "epoch 316 / 1000, loss = 2.9191, valid loss:  3.1606\n",
      "epoch 317 / 1000, loss = 2.9195, valid loss:  3.1612\n",
      "epoch 318 / 1000, loss = 2.9180, valid loss:  3.1575\n",
      "epoch 319 / 1000, loss = 2.9131, valid loss:  3.1486\n",
      "epoch 320 / 1000, loss = 2.9114, valid loss:  3.1467\n",
      "epoch 321 / 1000, loss = 2.9154, valid loss:  3.1528\n",
      "epoch 322 / 1000, loss = 2.9161, valid loss:  3.1546\n",
      "epoch 323 / 1000, loss = 2.9161, valid loss:  3.1547\n",
      "epoch 324 / 1000, loss = 2.9133, valid loss:  3.1484\n",
      "epoch 325 / 1000, loss = 2.9128, valid loss:  3.1490\n",
      "epoch 326 / 1000, loss = 2.9053, valid loss:  3.1395\n",
      "epoch 327 / 1000, loss = 2.9085, valid loss:  3.1409\n",
      "epoch 328 / 1000, loss = 2.9138, valid loss:  3.1558\n",
      "epoch 329 / 1000, loss = 2.9070, valid loss:  3.1411\n",
      "epoch 330 / 1000, loss = 2.9023, valid loss:  3.1379\n",
      "epoch 331 / 1000, loss = 2.8980, valid loss:  3.1358\n",
      "epoch 332 / 1000, loss = 2.9101, valid loss:  3.1594\n",
      "epoch 333 / 1000, loss = 2.9067, valid loss:  3.1457\n",
      "epoch 334 / 1000, loss = 2.9073, valid loss:  3.1537\n",
      "epoch 335 / 1000, loss = 2.9067, valid loss:  3.1535\n",
      "epoch 336 / 1000, loss = 2.9074, valid loss:  3.1639\n",
      "epoch 337 / 1000, loss = 2.9070, valid loss:  3.1659\n",
      "epoch 338 / 1000, loss = 2.9063, valid loss:  3.1651\n",
      "epoch 339 / 1000, loss = 2.9054, valid loss:  3.1624\n",
      "epoch 340 / 1000, loss = 2.9044, valid loss:  3.1590\n",
      "epoch 341 / 1000, loss = 2.9034, valid loss:  3.1547\n",
      "epoch 342 / 1000, loss = 2.9021, valid loss:  3.1488\n",
      "epoch 343 / 1000, loss = 2.9006, valid loss:  3.1418\n",
      "epoch 344 / 1000, loss = 2.8988, valid loss:  3.1363\n",
      "epoch 345 / 1000, loss = 2.8964, valid loss:  3.1327\n",
      "epoch 346 / 1000, loss = 2.8891, valid loss:  3.1304\n",
      "epoch 347 / 1000, loss = 2.8923, valid loss:  3.1328\n",
      "epoch 348 / 1000, loss = 2.9034, valid loss:  3.1446\n",
      "epoch 349 / 1000, loss = 2.9030, valid loss:  3.1416\n",
      "epoch 350 / 1000, loss = 2.9019, valid loss:  3.1395\n",
      "epoch 351 / 1000, loss = 2.9008, valid loss:  3.1384\n",
      "epoch 352 / 1000, loss = 2.9000, valid loss:  3.1386\n",
      "epoch 353 / 1000, loss = 2.8993, valid loss:  3.1404\n",
      "epoch 354 / 1000, loss = 2.8979, valid loss:  3.1434\n",
      "epoch 355 / 1000, loss = 2.8905, valid loss:  3.1352\n",
      "epoch 356 / 1000, loss = 2.8787, valid loss:  3.1271\n",
      "epoch 357 / 1000, loss = 2.8891, valid loss:  3.1517\n",
      "epoch 358 / 1000, loss = 2.8949, valid loss:  3.1429\n",
      "epoch 359 / 1000, loss = 2.8829, valid loss:  3.1241\n",
      "epoch 360 / 1000, loss = 2.8772, valid loss:  3.1194\n",
      "epoch 361 / 1000, loss = 2.8793, valid loss:  3.1350\n",
      "epoch 362 / 1000, loss = 2.8847, valid loss:  3.1230\n",
      "epoch 363 / 1000, loss = 2.8749, valid loss:  3.1200\n",
      "epoch 364 / 1000, loss = 2.8884, valid loss:  3.1353\n",
      "epoch 365 / 1000, loss = 2.8868, valid loss:  3.1415\n",
      "epoch 366 / 1000, loss = 2.8700, valid loss:  3.1403\n",
      "epoch 367 / 1000, loss = 2.8778, valid loss:  3.1252\n",
      "epoch 368 / 1000, loss = 2.8778, valid loss:  3.1301\n",
      "epoch 369 / 1000, loss = 2.8757, valid loss:  3.1288\n",
      "epoch 370 / 1000, loss = 2.8746, valid loss:  3.1258\n",
      "epoch 371 / 1000, loss = 2.8743, valid loss:  3.1275\n",
      "epoch 372 / 1000, loss = 2.8682, valid loss:  3.1280\n",
      "epoch 373 / 1000, loss = 2.8698, valid loss:  3.1277\n",
      "epoch 374 / 1000, loss = 2.8697, valid loss:  3.1228\n",
      "epoch 375 / 1000, loss = 2.8822, valid loss:  3.1222\n",
      "epoch 376 / 1000, loss = 2.8657, valid loss:  3.1255\n",
      "epoch 377 / 1000, loss = 2.8792, valid loss:  3.1227\n",
      "epoch 378 / 1000, loss = 2.8694, valid loss:  3.1241\n",
      "epoch 379 / 1000, loss = 2.8651, valid loss:  3.1198\n",
      "epoch 380 / 1000, loss = 2.8639, valid loss:  3.1170\n",
      "epoch 381 / 1000, loss = 2.8726, valid loss:  3.1197\n",
      "epoch 382 / 1000, loss = 2.8649, valid loss:  3.1145\n",
      "epoch 383 / 1000, loss = 2.8746, valid loss:  3.1290\n",
      "epoch 384 / 1000, loss = 2.8628, valid loss:  3.1143\n",
      "epoch 385 / 1000, loss = 2.8688, valid loss:  3.1150\n",
      "epoch 386 / 1000, loss = 2.8644, valid loss:  3.1165\n",
      "epoch 387 / 1000, loss = 2.8621, valid loss:  3.1081\n",
      "epoch 388 / 1000, loss = 2.8665, valid loss:  3.1363\n",
      "epoch 389 / 1000, loss = 2.8727, valid loss:  3.1177\n",
      "epoch 390 / 1000, loss = 2.8755, valid loss:  3.1175\n",
      "epoch 391 / 1000, loss = 2.8601, valid loss:  3.1108\n",
      "epoch 392 / 1000, loss = 2.8724, valid loss:  3.1167\n",
      "epoch 393 / 1000, loss = 2.8631, valid loss:  3.1176\n",
      "epoch 394 / 1000, loss = 2.8583, valid loss:  3.1105\n",
      "epoch 395 / 1000, loss = 2.8604, valid loss:  3.1080\n",
      "epoch 396 / 1000, loss = 2.8546, valid loss:  3.1106\n",
      "epoch 397 / 1000, loss = 2.8585, valid loss:  3.1119\n",
      "epoch 398 / 1000, loss = 2.8528, valid loss:  3.1074\n",
      "epoch 399 / 1000, loss = 2.8590, valid loss:  3.1064\n",
      "epoch 400 / 1000, loss = 2.8598, valid loss:  3.1132\n",
      "epoch 401 / 1000, loss = 2.8545, valid loss:  3.1048\n",
      "epoch 402 / 1000, loss = 2.8617, valid loss:  3.1154\n",
      "epoch 403 / 1000, loss = 2.8588, valid loss:  3.1045\n",
      "epoch 404 / 1000, loss = 2.8550, valid loss:  3.1074\n",
      "epoch 405 / 1000, loss = 2.8610, valid loss:  3.1155\n",
      "epoch 406 / 1000, loss = 2.8570, valid loss:  3.1087\n",
      "epoch 407 / 1000, loss = 2.8614, valid loss:  3.1131\n",
      "epoch 408 / 1000, loss = 2.8584, valid loss:  3.1104\n",
      "epoch 409 / 1000, loss = 2.8601, valid loss:  3.1121\n",
      "epoch 410 / 1000, loss = 2.8599, valid loss:  3.1132\n",
      "epoch 411 / 1000, loss = 2.8598, valid loss:  3.1126\n",
      "epoch 412 / 1000, loss = 2.8597, valid loss:  3.1120\n",
      "epoch 413 / 1000, loss = 2.8595, valid loss:  3.1112\n",
      "epoch 414 / 1000, loss = 2.8592, valid loss:  3.1104\n",
      "epoch 415 / 1000, loss = 2.8586, valid loss:  3.1096\n",
      "epoch 416 / 1000, loss = 2.8576, valid loss:  3.1090\n",
      "epoch 417 / 1000, loss = 2.8562, valid loss:  3.1089\n",
      "epoch 418 / 1000, loss = 2.8546, valid loss:  3.1089\n",
      "epoch 419 / 1000, loss = 2.8533, valid loss:  3.1091\n",
      "epoch 420 / 1000, loss = 2.8528, valid loss:  3.1101\n",
      "epoch 421 / 1000, loss = 2.8547, valid loss:  3.1141\n",
      "epoch 422 / 1000, loss = 2.8550, valid loss:  3.1137\n",
      "epoch 423 / 1000, loss = 2.8555, valid loss:  3.1127\n",
      "epoch 424 / 1000, loss = 2.8513, valid loss:  3.1080\n",
      "epoch 425 / 1000, loss = 2.8648, valid loss:  3.1149\n",
      "epoch 426 / 1000, loss = 2.8605, valid loss:  3.1088\n",
      "epoch 427 / 1000, loss = 2.8581, valid loss:  3.1088\n",
      "epoch 428 / 1000, loss = 2.8646, valid loss:  3.1112\n",
      "epoch 429 / 1000, loss = 2.8604, valid loss:  3.1071\n",
      "epoch 430 / 1000, loss = 2.8528, valid loss:  3.1177\n",
      "epoch 431 / 1000, loss = 2.8474, valid loss:  3.0956\n",
      "epoch 432 / 1000, loss = 2.8451, valid loss:  3.0907\n",
      "epoch 433 / 1000, loss = 2.8466, valid loss:  3.0899\n",
      "epoch 434 / 1000, loss = 2.8455, valid loss:  3.1092\n",
      "epoch 435 / 1000, loss = 2.8632, valid loss:  3.1136\n",
      "epoch 436 / 1000, loss = 2.8448, valid loss:  3.1039\n",
      "epoch 437 / 1000, loss = 2.8495, valid loss:  3.1104\n",
      "epoch 438 / 1000, loss = 2.8622, valid loss:  3.1066\n",
      "epoch 439 / 1000, loss = 2.8445, valid loss:  3.1025\n",
      "epoch 440 / 1000, loss = 2.8427, valid loss:  3.0899\n",
      "epoch 441 / 1000, loss = 2.8442, valid loss:  3.1044\n",
      "epoch 442 / 1000, loss = 2.8407, valid loss:  3.0906\n",
      "epoch 443 / 1000, loss = 2.8460, valid loss:  3.0866\n",
      "epoch 444 / 1000, loss = 2.8413, valid loss:  3.0985\n",
      "epoch 445 / 1000, loss = 2.8413, valid loss:  3.1029\n",
      "epoch 446 / 1000, loss = 2.8399, valid loss:  3.0990\n",
      "epoch 447 / 1000, loss = 2.8362, valid loss:  3.0871\n",
      "epoch 448 / 1000, loss = 2.8365, valid loss:  3.0904\n",
      "epoch 449 / 1000, loss = 2.8409, valid loss:  3.0901\n",
      "epoch 450 / 1000, loss = 2.8342, valid loss:  3.0861\n",
      "epoch 451 / 1000, loss = 2.8359, valid loss:  3.0838\n",
      "epoch 452 / 1000, loss = 2.8330, valid loss:  3.0820\n",
      "epoch 453 / 1000, loss = 2.8321, valid loss:  3.0818\n",
      "epoch 454 / 1000, loss = 2.8315, valid loss:  3.0813\n",
      "epoch 455 / 1000, loss = 2.8300, valid loss:  3.0812\n",
      "epoch 456 / 1000, loss = 2.8329, valid loss:  3.0870\n",
      "epoch 457 / 1000, loss = 2.8443, valid loss:  3.1003\n",
      "epoch 458 / 1000, loss = 2.8476, valid loss:  3.1084\n",
      "epoch 459 / 1000, loss = 2.8311, valid loss:  3.0815\n",
      "epoch 460 / 1000, loss = 2.8400, valid loss:  3.0912\n",
      "epoch 461 / 1000, loss = 2.8393, valid loss:  3.0913\n",
      "epoch 462 / 1000, loss = 2.8271, valid loss:  3.0794\n",
      "epoch 463 / 1000, loss = 2.8329, valid loss:  3.0817\n",
      "epoch 464 / 1000, loss = 2.8277, valid loss:  3.0781\n",
      "epoch 465 / 1000, loss = 2.8317, valid loss:  3.0963\n",
      "epoch 466 / 1000, loss = 2.8292, valid loss:  3.0772\n",
      "epoch 467 / 1000, loss = 2.8344, valid loss:  3.0804\n",
      "epoch 468 / 1000, loss = 2.8233, valid loss:  3.0784\n",
      "epoch 469 / 1000, loss = 2.8377, valid loss:  3.0925\n",
      "epoch 470 / 1000, loss = 2.8306, valid loss:  3.0772\n",
      "epoch 471 / 1000, loss = 2.8207, valid loss:  3.0781\n",
      "epoch 472 / 1000, loss = 2.8299, valid loss:  3.0821\n",
      "epoch 473 / 1000, loss = 2.8258, valid loss:  3.0834\n",
      "epoch 474 / 1000, loss = 2.8230, valid loss:  3.0834\n",
      "epoch 475 / 1000, loss = 2.8351, valid loss:  3.0937\n",
      "epoch 476 / 1000, loss = 2.8207, valid loss:  3.0799\n",
      "epoch 477 / 1000, loss = 2.8255, valid loss:  3.0787\n",
      "epoch 478 / 1000, loss = 2.8354, valid loss:  3.0865\n",
      "epoch 479 / 1000, loss = 2.8226, valid loss:  3.0734\n",
      "epoch 480 / 1000, loss = 2.8230, valid loss:  3.1094\n",
      "epoch 481 / 1000, loss = 2.8325, valid loss:  3.0900\n",
      "epoch 482 / 1000, loss = 2.8238, valid loss:  3.0735\n",
      "epoch 483 / 1000, loss = 2.8242, valid loss:  3.0840\n",
      "epoch 484 / 1000, loss = 2.8189, valid loss:  3.0798\n",
      "epoch 485 / 1000, loss = 2.8213, valid loss:  3.0779\n",
      "epoch 486 / 1000, loss = 2.8170, valid loss:  3.0805\n",
      "epoch 487 / 1000, loss = 2.8327, valid loss:  3.0911\n",
      "epoch 488 / 1000, loss = 2.8225, valid loss:  3.0774\n",
      "epoch 489 / 1000, loss = 2.8237, valid loss:  3.0743\n",
      "epoch 490 / 1000, loss = 2.8209, valid loss:  3.0716\n",
      "epoch 491 / 1000, loss = 2.8221, valid loss:  3.0812\n",
      "epoch 492 / 1000, loss = 2.8311, valid loss:  3.0914\n",
      "epoch 493 / 1000, loss = 2.8213, valid loss:  3.0772\n",
      "epoch 494 / 1000, loss = 2.8296, valid loss:  3.0989\n",
      "epoch 495 / 1000, loss = 2.8341, valid loss:  3.1001\n",
      "epoch 496 / 1000, loss = 2.8370, valid loss:  3.0980\n",
      "epoch 497 / 1000, loss = 2.8387, valid loss:  3.1008\n",
      "epoch 498 / 1000, loss = 2.8212, valid loss:  3.0845\n",
      "epoch 499 / 1000, loss = 2.8490, valid loss:  3.0985\n",
      "epoch 500 / 1000, loss = 2.8184, valid loss:  3.0892\n",
      "epoch 501 / 1000, loss = 2.8230, valid loss:  3.0877\n",
      "epoch 502 / 1000, loss = 2.8226, valid loss:  3.0870\n",
      "epoch 503 / 1000, loss = 2.8217, valid loss:  3.0861\n",
      "epoch 504 / 1000, loss = 2.8191, valid loss:  3.0850\n",
      "epoch 505 / 1000, loss = 2.8161, valid loss:  3.0854\n",
      "epoch 506 / 1000, loss = 2.8156, valid loss:  3.0850\n",
      "epoch 507 / 1000, loss = 2.8158, valid loss:  3.0838\n",
      "epoch 508 / 1000, loss = 2.8165, valid loss:  3.0826\n",
      "epoch 509 / 1000, loss = 2.8148, valid loss:  3.0833\n",
      "epoch 510 / 1000, loss = 2.8140, valid loss:  3.0803\n",
      "epoch 511 / 1000, loss = 2.8150, valid loss:  3.0748\n",
      "epoch 512 / 1000, loss = 2.8186, valid loss:  3.0733\n",
      "epoch 513 / 1000, loss = 2.8224, valid loss:  3.0773\n",
      "epoch 514 / 1000, loss = 2.8198, valid loss:  3.0744\n",
      "epoch 515 / 1000, loss = 2.8228, valid loss:  3.0811\n",
      "epoch 516 / 1000, loss = 2.8183, valid loss:  3.0742\n",
      "epoch 517 / 1000, loss = 2.8219, valid loss:  3.0820\n",
      "epoch 518 / 1000, loss = 2.8154, valid loss:  3.0782\n",
      "epoch 519 / 1000, loss = 2.8177, valid loss:  3.0807\n",
      "epoch 520 / 1000, loss = 2.8106, valid loss:  3.0716\n",
      "epoch 521 / 1000, loss = 2.8158, valid loss:  3.0794\n",
      "epoch 522 / 1000, loss = 2.8085, valid loss:  3.0771\n",
      "epoch 523 / 1000, loss = 2.8064, valid loss:  3.0722\n",
      "epoch 524 / 1000, loss = 2.8149, valid loss:  3.0794\n",
      "epoch 525 / 1000, loss = 2.7992, valid loss:  3.0855\n",
      "epoch 526 / 1000, loss = 2.8115, valid loss:  3.0734\n",
      "epoch 527 / 1000, loss = 2.8143, valid loss:  3.0781\n",
      "epoch 528 / 1000, loss = 2.7981, valid loss:  3.0831\n",
      "epoch 529 / 1000, loss = 2.8133, valid loss:  3.0806\n",
      "epoch 530 / 1000, loss = 2.8146, valid loss:  3.0825\n",
      "epoch 531 / 1000, loss = 2.8130, valid loss:  3.0789\n",
      "epoch 532 / 1000, loss = 2.8067, valid loss:  3.0696\n",
      "epoch 533 / 1000, loss = 2.8102, valid loss:  3.0752\n",
      "epoch 534 / 1000, loss = 2.7958, valid loss:  3.0792\n",
      "epoch 535 / 1000, loss = 2.7959, valid loss:  3.0792\n",
      "epoch 536 / 1000, loss = 2.8018, valid loss:  3.0811\n",
      "epoch 537 / 1000, loss = 2.8021, valid loss:  3.0823\n",
      "epoch 538 / 1000, loss = 2.7974, valid loss:  3.0705\n",
      "epoch 539 / 1000, loss = 2.8081, valid loss:  3.0864\n",
      "epoch 540 / 1000, loss = 2.7925, valid loss:  3.0925\n",
      "epoch 541 / 1000, loss = 2.8050, valid loss:  3.0712\n",
      "epoch 542 / 1000, loss = 2.7967, valid loss:  3.0766\n",
      "epoch 543 / 1000, loss = 2.8037, valid loss:  3.0834\n",
      "epoch 544 / 1000, loss = 2.8036, valid loss:  3.0690\n",
      "epoch 545 / 1000, loss = 2.7928, valid loss:  3.0740\n",
      "epoch 546 / 1000, loss = 2.7907, valid loss:  3.0744\n",
      "epoch 547 / 1000, loss = 2.7914, valid loss:  3.0731\n",
      "epoch 548 / 1000, loss = 2.8011, valid loss:  3.0818\n",
      "epoch 549 / 1000, loss = 2.7896, valid loss:  3.0683\n",
      "epoch 550 / 1000, loss = 2.8042, valid loss:  3.0719\n",
      "epoch 551 / 1000, loss = 2.8071, valid loss:  3.0782\n",
      "epoch 552 / 1000, loss = 2.8005, valid loss:  3.0744\n",
      "epoch 553 / 1000, loss = 2.8020, valid loss:  3.0698\n",
      "epoch 554 / 1000, loss = 2.7893, valid loss:  3.0751\n",
      "epoch 555 / 1000, loss = 2.7898, valid loss:  3.0741\n",
      "epoch 556 / 1000, loss = 2.7971, valid loss:  3.0787\n",
      "epoch 557 / 1000, loss = 2.7956, valid loss:  3.0791\n",
      "epoch 558 / 1000, loss = 2.7940, valid loss:  3.0789\n",
      "epoch 559 / 1000, loss = 2.7812, valid loss:  3.0734\n",
      "epoch 560 / 1000, loss = 2.7819, valid loss:  3.0697\n",
      "epoch 561 / 1000, loss = 2.7841, valid loss:  3.0640\n",
      "epoch 562 / 1000, loss = 2.7942, valid loss:  3.0644\n",
      "epoch 563 / 1000, loss = 2.7797, valid loss:  3.0761\n",
      "epoch 564 / 1000, loss = 2.7791, valid loss:  3.0787\n",
      "epoch 565 / 1000, loss = 2.7819, valid loss:  3.0839\n",
      "epoch 566 / 1000, loss = 2.7833, valid loss:  3.0685\n",
      "epoch 567 / 1000, loss = 2.7763, valid loss:  3.0763\n",
      "epoch 568 / 1000, loss = 2.7839, valid loss:  3.0771\n",
      "epoch 569 / 1000, loss = 2.8004, valid loss:  3.0738\n",
      "epoch 570 / 1000, loss = 2.7804, valid loss:  3.0690\n",
      "epoch 571 / 1000, loss = 2.7750, valid loss:  3.0799\n",
      "epoch 572 / 1000, loss = 2.7747, valid loss:  3.0746\n",
      "epoch 573 / 1000, loss = 2.7753, valid loss:  3.0780\n",
      "epoch 574 / 1000, loss = 2.7968, valid loss:  3.0708\n",
      "epoch 575 / 1000, loss = 2.7745, valid loss:  3.0708\n",
      "epoch 576 / 1000, loss = 2.7730, valid loss:  3.0752\n",
      "epoch 577 / 1000, loss = 2.7768, valid loss:  3.0619\n",
      "epoch 578 / 1000, loss = 2.7725, valid loss:  3.0716\n",
      "epoch 579 / 1000, loss = 2.7968, valid loss:  3.0718\n",
      "epoch 580 / 1000, loss = 2.7767, valid loss:  3.0667\n",
      "epoch 581 / 1000, loss = 2.7732, valid loss:  3.0799\n",
      "epoch 582 / 1000, loss = 2.7731, valid loss:  3.0656\n",
      "epoch 583 / 1000, loss = 2.7822, valid loss:  3.0742\n",
      "epoch 584 / 1000, loss = 2.7715, valid loss:  3.0718\n",
      "epoch 585 / 1000, loss = 2.7977, valid loss:  3.0679\n",
      "epoch 586 / 1000, loss = 2.7851, valid loss:  3.0627\n",
      "epoch 587 / 1000, loss = 2.7907, valid loss:  3.0611\n",
      "epoch 588 / 1000, loss = 2.7709, valid loss:  3.0657\n",
      "epoch 589 / 1000, loss = 2.7883, valid loss:  3.0734\n",
      "epoch 590 / 1000, loss = 2.7705, valid loss:  3.0730\n",
      "epoch 591 / 1000, loss = 2.7871, valid loss:  3.0574\n",
      "epoch 592 / 1000, loss = 2.7955, valid loss:  3.0662\n",
      "epoch 593 / 1000, loss = 2.7823, valid loss:  3.0587\n",
      "epoch 594 / 1000, loss = 2.7700, valid loss:  3.0707\n",
      "epoch 595 / 1000, loss = 2.7889, valid loss:  3.0575\n",
      "epoch 596 / 1000, loss = 2.7710, valid loss:  3.0606\n",
      "epoch 597 / 1000, loss = 2.7794, valid loss:  3.0546\n",
      "epoch 598 / 1000, loss = 2.7847, valid loss:  3.0542\n",
      "epoch 599 / 1000, loss = 2.7874, valid loss:  3.0589\n",
      "epoch 600 / 1000, loss = 2.7705, valid loss:  3.0724\n",
      "epoch 601 / 1000, loss = 2.7720, valid loss:  3.0559\n",
      "epoch 602 / 1000, loss = 2.7675, valid loss:  3.0692\n",
      "epoch 603 / 1000, loss = 2.7934, valid loss:  3.0975\n",
      "epoch 604 / 1000, loss = 2.7832, valid loss:  3.0698\n",
      "epoch 605 / 1000, loss = 2.7819, valid loss:  3.0549\n",
      "epoch 606 / 1000, loss = 2.7663, valid loss:  3.0682\n",
      "epoch 607 / 1000, loss = 2.7840, valid loss:  3.0692\n",
      "epoch 608 / 1000, loss = 2.7825, valid loss:  3.0548\n",
      "epoch 609 / 1000, loss = 2.7911, valid loss:  3.1032\n",
      "epoch 610 / 1000, loss = 2.7780, valid loss:  3.0694\n",
      "epoch 611 / 1000, loss = 2.7704, valid loss:  3.0603\n",
      "epoch 612 / 1000, loss = 2.7860, valid loss:  3.0730\n",
      "epoch 613 / 1000, loss = 2.7675, valid loss:  3.0596\n",
      "epoch 614 / 1000, loss = 2.7817, valid loss:  3.0686\n",
      "epoch 615 / 1000, loss = 2.7885, valid loss:  3.0510\n",
      "epoch 616 / 1000, loss = 2.7814, valid loss:  3.0703\n",
      "epoch 617 / 1000, loss = 2.7827, valid loss:  3.0802\n",
      "epoch 618 / 1000, loss = 2.7882, valid loss:  3.0484\n",
      "epoch 619 / 1000, loss = 2.7673, valid loss:  3.0600\n",
      "epoch 620 / 1000, loss = 2.7960, valid loss:  3.0503\n",
      "epoch 621 / 1000, loss = 2.7739, valid loss:  3.0713\n",
      "epoch 622 / 1000, loss = 2.7827, valid loss:  3.0593\n",
      "epoch 623 / 1000, loss = 2.7827, valid loss:  3.0600\n",
      "epoch 624 / 1000, loss = 2.7827, valid loss:  3.0619\n",
      "epoch 625 / 1000, loss = 2.7817, valid loss:  3.0592\n",
      "epoch 626 / 1000, loss = 2.7681, valid loss:  3.0527\n",
      "epoch 627 / 1000, loss = 2.7756, valid loss:  3.0479\n",
      "epoch 628 / 1000, loss = 2.7895, valid loss:  3.0407\n",
      "epoch 629 / 1000, loss = 2.7719, valid loss:  3.0472\n",
      "epoch 630 / 1000, loss = 2.7887, valid loss:  3.0417\n",
      "epoch 631 / 1000, loss = 2.7856, valid loss:  3.0770\n",
      "epoch 632 / 1000, loss = 2.7707, valid loss:  3.0443\n",
      "epoch 633 / 1000, loss = 2.7851, valid loss:  3.0628\n",
      "epoch 634 / 1000, loss = 2.7698, valid loss:  3.0441\n",
      "epoch 635 / 1000, loss = 2.7831, valid loss:  3.0535\n",
      "epoch 636 / 1000, loss = 2.7702, valid loss:  3.0551\n",
      "epoch 637 / 1000, loss = 2.7932, valid loss:  3.0403\n",
      "epoch 638 / 1000, loss = 2.7712, valid loss:  3.0390\n",
      "epoch 639 / 1000, loss = 2.7680, valid loss:  3.0549\n",
      "epoch 640 / 1000, loss = 2.7681, valid loss:  3.0497\n",
      "epoch 641 / 1000, loss = 2.7887, valid loss:  3.0354\n",
      "epoch 642 / 1000, loss = 2.7874, valid loss:  3.0351\n",
      "epoch 643 / 1000, loss = 2.7694, valid loss:  3.0417\n",
      "epoch 644 / 1000, loss = 2.7748, valid loss:  3.0344\n",
      "epoch 645 / 1000, loss = 2.7705, valid loss:  3.0375\n",
      "epoch 646 / 1000, loss = 2.7694, valid loss:  3.0487\n",
      "epoch 647 / 1000, loss = 2.7686, valid loss:  3.0505\n",
      "epoch 648 / 1000, loss = 2.7683, valid loss:  3.0454\n",
      "epoch 649 / 1000, loss = 2.7759, valid loss:  3.0326\n",
      "epoch 650 / 1000, loss = 2.7906, valid loss:  3.0291\n",
      "epoch 651 / 1000, loss = 2.7742, valid loss:  3.0350\n",
      "epoch 652 / 1000, loss = 2.7856, valid loss:  3.0287\n",
      "epoch 653 / 1000, loss = 2.7697, valid loss:  3.0438\n",
      "epoch 654 / 1000, loss = 2.7947, valid loss:  3.0284\n",
      "epoch 655 / 1000, loss = 2.7711, valid loss:  3.0443\n",
      "epoch 656 / 1000, loss = 2.7806, valid loss:  3.0371\n",
      "epoch 657 / 1000, loss = 2.7694, valid loss:  3.0465\n",
      "epoch 658 / 1000, loss = 2.7650, valid loss:  3.0385\n",
      "epoch 659 / 1000, loss = 2.7680, valid loss:  3.0334\n",
      "epoch 660 / 1000, loss = 2.7863, valid loss:  3.0269\n",
      "epoch 661 / 1000, loss = 2.7814, valid loss:  3.0260\n",
      "epoch 662 / 1000, loss = 2.8545, valid loss:  3.1311\n",
      "epoch 663 / 1000, loss = 2.8435, valid loss:  3.1167\n",
      "epoch 664 / 1000, loss = 2.8399, valid loss:  3.1065\n",
      "epoch 665 / 1000, loss = 2.8296, valid loss:  3.1051\n",
      "epoch 666 / 1000, loss = 2.8265, valid loss:  3.1024\n",
      "epoch 667 / 1000, loss = 2.8233, valid loss:  3.0989\n",
      "epoch 668 / 1000, loss = 2.8197, valid loss:  3.0970\n",
      "epoch 669 / 1000, loss = 2.8155, valid loss:  3.0956\n",
      "epoch 670 / 1000, loss = 2.8117, valid loss:  3.0921\n",
      "epoch 671 / 1000, loss = 2.8089, valid loss:  3.0850\n",
      "epoch 672 / 1000, loss = 2.8122, valid loss:  3.0871\n",
      "epoch 673 / 1000, loss = 2.8175, valid loss:  3.0863\n",
      "epoch 674 / 1000, loss = 2.8087, valid loss:  3.0863\n",
      "epoch 675 / 1000, loss = 2.8007, valid loss:  3.0900\n",
      "epoch 676 / 1000, loss = 2.8132, valid loss:  3.0728\n",
      "epoch 677 / 1000, loss = 2.8253, valid loss:  3.0827\n",
      "epoch 678 / 1000, loss = 2.8152, valid loss:  3.0793\n",
      "epoch 679 / 1000, loss = 2.8180, valid loss:  3.0824\n",
      "epoch 680 / 1000, loss = 2.8130, valid loss:  3.0733\n",
      "epoch 681 / 1000, loss = 2.8205, valid loss:  3.0884\n",
      "epoch 682 / 1000, loss = 2.8229, valid loss:  3.0839\n",
      "epoch 683 / 1000, loss = 2.8240, valid loss:  3.0743\n",
      "epoch 684 / 1000, loss = 2.8172, valid loss:  3.0718\n",
      "epoch 685 / 1000, loss = 2.8163, valid loss:  3.0669\n",
      "epoch 686 / 1000, loss = 2.8169, valid loss:  3.0654\n",
      "epoch 687 / 1000, loss = 2.8155, valid loss:  3.0691\n",
      "epoch 688 / 1000, loss = 2.8656, valid loss:  3.0880\n",
      "epoch 689 / 1000, loss = 2.8132, valid loss:  3.0637\n",
      "epoch 690 / 1000, loss = 2.8141, valid loss:  3.0632\n",
      "epoch 691 / 1000, loss = 2.8378, valid loss:  3.0860\n",
      "epoch 692 / 1000, loss = 2.8412, valid loss:  3.0775\n",
      "epoch 693 / 1000, loss = 2.8490, valid loss:  3.0669\n",
      "epoch 694 / 1000, loss = 2.8255, valid loss:  3.0952\n",
      "epoch 695 / 1000, loss = 2.8366, valid loss:  3.0835\n",
      "epoch 696 / 1000, loss = 2.8103, valid loss:  3.0857\n",
      "epoch 697 / 1000, loss = 2.8183, valid loss:  3.0606\n",
      "epoch 698 / 1000, loss = 2.8166, valid loss:  3.0892\n",
      "epoch 699 / 1000, loss = 2.8532, valid loss:  3.0658\n",
      "epoch 700 / 1000, loss = 2.8214, valid loss:  3.0917\n",
      "epoch 701 / 1000, loss = 2.8147, valid loss:  3.0902\n",
      "epoch 702 / 1000, loss = 2.8424, valid loss:  3.0738\n",
      "epoch 703 / 1000, loss = 2.8181, valid loss:  3.0858\n",
      "epoch 704 / 1000, loss = 2.8290, valid loss:  3.0871\n",
      "epoch 705 / 1000, loss = 2.8590, valid loss:  3.0603\n",
      "epoch 706 / 1000, loss = 2.8446, valid loss:  3.0763\n",
      "epoch 707 / 1000, loss = 2.8309, valid loss:  3.0806\n",
      "epoch 708 / 1000, loss = 2.8113, valid loss:  3.0811\n",
      "epoch 709 / 1000, loss = 2.8287, valid loss:  3.0836\n",
      "epoch 710 / 1000, loss = 2.8091, valid loss:  3.0869\n",
      "epoch 711 / 1000, loss = 2.8396, valid loss:  3.0706\n",
      "epoch 712 / 1000, loss = 2.8144, valid loss:  3.0828\n",
      "epoch 713 / 1000, loss = 2.8419, valid loss:  3.0724\n",
      "epoch 714 / 1000, loss = 2.8154, valid loss:  3.0768\n",
      "epoch 715 / 1000, loss = 2.8406, valid loss:  3.0644\n",
      "epoch 716 / 1000, loss = 2.8458, valid loss:  3.0639\n",
      "epoch 717 / 1000, loss = 2.8276, valid loss:  3.0716\n",
      "epoch 718 / 1000, loss = 2.8385, valid loss:  3.0627\n",
      "epoch 719 / 1000, loss = 2.8095, valid loss:  3.0702\n",
      "epoch 720 / 1000, loss = 2.8486, valid loss:  3.0589\n",
      "epoch 721 / 1000, loss = 2.8365, valid loss:  3.0698\n",
      "epoch 722 / 1000, loss = 2.8477, valid loss:  3.0582\n",
      "epoch 723 / 1000, loss = 2.8308, valid loss:  3.0702\n",
      "epoch 724 / 1000, loss = 2.8154, valid loss:  3.0819\n",
      "epoch 725 / 1000, loss = 2.8575, valid loss:  3.0466\n",
      "epoch 726 / 1000, loss = 2.8116, valid loss:  3.0576\n",
      "epoch 727 / 1000, loss = 2.7963, valid loss:  3.0592\n",
      "epoch 728 / 1000, loss = 2.8245, valid loss:  3.0721\n",
      "epoch 729 / 1000, loss = 2.8387, valid loss:  3.0720\n",
      "epoch 730 / 1000, loss = 2.8430, valid loss:  3.0559\n",
      "epoch 731 / 1000, loss = 2.8247, valid loss:  3.0627\n",
      "epoch 732 / 1000, loss = 2.8297, valid loss:  3.0642\n",
      "epoch 733 / 1000, loss = 2.8110, valid loss:  3.0800\n",
      "epoch 734 / 1000, loss = 2.8635, valid loss:  3.0395\n",
      "epoch 735 / 1000, loss = 2.8099, valid loss:  3.0499\n",
      "epoch 736 / 1000, loss = 2.8561, valid loss:  3.0543\n",
      "epoch 737 / 1000, loss = 2.8008, valid loss:  3.0437\n",
      "epoch 738 / 1000, loss = 2.8258, valid loss:  3.0430\n",
      "epoch 739 / 1000, loss = 2.8130, valid loss:  3.0555\n",
      "epoch 740 / 1000, loss = 2.8117, valid loss:  3.0395\n",
      "epoch 741 / 1000, loss = 2.7991, valid loss:  3.0443\n",
      "epoch 742 / 1000, loss = 2.8626, valid loss:  3.0461\n",
      "epoch 743 / 1000, loss = 2.7922, valid loss:  3.0306\n",
      "epoch 744 / 1000, loss = 2.8365, valid loss:  3.0531\n",
      "epoch 745 / 1000, loss = 2.8065, valid loss:  3.0476\n",
      "epoch 746 / 1000, loss = 2.8606, valid loss:  3.0529\n",
      "epoch 747 / 1000, loss = 2.8276, valid loss:  3.0731\n",
      "epoch 748 / 1000, loss = 2.8583, valid loss:  3.0679\n",
      "epoch 749 / 1000, loss = 2.7879, valid loss:  3.0411\n",
      "epoch 750 / 1000, loss = 2.8272, valid loss:  3.0977\n",
      "epoch 751 / 1000, loss = 2.7874, valid loss:  3.0451\n",
      "epoch 752 / 1000, loss = 2.7995, valid loss:  3.0396\n",
      "epoch 753 / 1000, loss = 2.8267, valid loss:  3.0439\n",
      "epoch 754 / 1000, loss = 2.8603, valid loss:  3.0486\n",
      "epoch 755 / 1000, loss = 2.7945, valid loss:  3.0542\n",
      "epoch 756 / 1000, loss = 2.8576, valid loss:  3.0408\n",
      "epoch 757 / 1000, loss = 2.8590, valid loss:  3.0390\n",
      "epoch 758 / 1000, loss = 2.7803, valid loss:  3.0276\n",
      "epoch 759 / 1000, loss = 2.8278, valid loss:  3.0345\n",
      "epoch 760 / 1000, loss = 2.8079, valid loss:  3.0566\n",
      "epoch 761 / 1000, loss = 2.8278, valid loss:  3.0387\n",
      "epoch 762 / 1000, loss = 2.8588, valid loss:  3.0417\n",
      "epoch 763 / 1000, loss = 2.7859, valid loss:  3.0227\n",
      "epoch 764 / 1000, loss = 2.8053, valid loss:  3.0320\n",
      "epoch 765 / 1000, loss = 2.8537, valid loss:  3.0426\n",
      "epoch 766 / 1000, loss = 2.8064, valid loss:  3.0473\n",
      "epoch 767 / 1000, loss = 2.7861, valid loss:  3.0259\n",
      "epoch 768 / 1000, loss = 2.8094, valid loss:  3.1002\n",
      "epoch 769 / 1000, loss = 2.8589, valid loss:  3.0554\n",
      "epoch 770 / 1000, loss = 2.7681, valid loss:  3.0305\n",
      "epoch 771 / 1000, loss = 2.8776, valid loss:  3.0937\n",
      "epoch 772 / 1000, loss = 2.8312, valid loss:  3.0303\n",
      "epoch 773 / 1000, loss = 2.8199, valid loss:  3.0624\n",
      "epoch 774 / 1000, loss = 2.8674, valid loss:  3.0660\n",
      "epoch 775 / 1000, loss = 2.7701, valid loss:  3.0229\n",
      "epoch 776 / 1000, loss = 2.7841, valid loss:  3.0466\n",
      "epoch 777 / 1000, loss = 2.8329, valid loss:  3.0360\n",
      "epoch 778 / 1000, loss = 2.7695, valid loss:  3.0194\n",
      "epoch 779 / 1000, loss = 2.8672, valid loss:  3.0791\n",
      "epoch 780 / 1000, loss = 2.7742, valid loss:  3.0203\n",
      "epoch 781 / 1000, loss = 2.7863, valid loss:  3.0336\n",
      "epoch 782 / 1000, loss = 2.7755, valid loss:  3.0259\n",
      "epoch 783 / 1000, loss = 2.8631, valid loss:  3.0728\n",
      "epoch 784 / 1000, loss = 2.8329, valid loss:  3.0372\n",
      "epoch 785 / 1000, loss = 2.8274, valid loss:  3.0430\n",
      "epoch 786 / 1000, loss = 2.7847, valid loss:  3.0546\n",
      "epoch 787 / 1000, loss = 2.7744, valid loss:  3.0148\n",
      "epoch 788 / 1000, loss = 2.8608, valid loss:  3.0717\n",
      "epoch 789 / 1000, loss = 2.8550, valid loss:  3.0667\n",
      "epoch 790 / 1000, loss = 2.8293, valid loss:  3.0341\n",
      "epoch 791 / 1000, loss = 2.7748, valid loss:  3.0154\n",
      "epoch 792 / 1000, loss = 2.7788, valid loss:  3.0159\n",
      "epoch 793 / 1000, loss = 2.7936, valid loss:  3.1190\n",
      "epoch 794 / 1000, loss = 2.8599, valid loss:  3.0710\n",
      "epoch 795 / 1000, loss = 2.7920, valid loss:  3.0216\n",
      "epoch 796 / 1000, loss = 2.8142, valid loss:  3.0287\n",
      "epoch 797 / 1000, loss = 2.8420, valid loss:  3.0305\n",
      "epoch 798 / 1000, loss = 2.8190, valid loss:  3.0458\n",
      "epoch 799 / 1000, loss = 2.7953, valid loss:  3.0767\n",
      "epoch 800 / 1000, loss = 2.8344, valid loss:  3.0107\n",
      "epoch 801 / 1000, loss = 2.7866, valid loss:  3.0253\n",
      "epoch 802 / 1000, loss = 2.8563, valid loss:  3.0717\n",
      "epoch 803 / 1000, loss = 2.7918, valid loss:  3.0343\n",
      "epoch 804 / 1000, loss = 2.7766, valid loss:  3.0189\n",
      "epoch 805 / 1000, loss = 2.8337, valid loss:  3.0121\n",
      "epoch 806 / 1000, loss = 2.7749, valid loss:  3.0403\n",
      "epoch 807 / 1000, loss = 2.8162, valid loss:  3.0089\n",
      "epoch 808 / 1000, loss = 2.8483, valid loss:  3.0378\n",
      "epoch 809 / 1000, loss = 2.7657, valid loss:  3.0067\n",
      "epoch 810 / 1000, loss = 2.7850, valid loss:  3.0604\n",
      "epoch 811 / 1000, loss = 2.7578, valid loss:  3.0123\n",
      "epoch 812 / 1000, loss = 2.7645, valid loss:  3.0266\n",
      "epoch 813 / 1000, loss = 2.7963, valid loss:  3.0307\n",
      "epoch 814 / 1000, loss = 2.7999, valid loss:  3.0011\n",
      "epoch 815 / 1000, loss = 2.7858, valid loss:  3.0024\n",
      "epoch 816 / 1000, loss = 2.7844, valid loss:  3.0106\n",
      "epoch 817 / 1000, loss = 2.7748, valid loss:  3.0017\n",
      "epoch 818 / 1000, loss = 2.7640, valid loss:  3.0334\n",
      "epoch 819 / 1000, loss = 2.7760, valid loss:  3.0099\n",
      "epoch 820 / 1000, loss = 2.7835, valid loss:  3.0049\n",
      "epoch 821 / 1000, loss = 2.7776, valid loss:  3.0607\n",
      "epoch 822 / 1000, loss = 2.7933, valid loss:  3.0034\n",
      "epoch 823 / 1000, loss = 2.8274, valid loss:  3.0321\n",
      "epoch 824 / 1000, loss = 2.7688, valid loss:  2.9944\n",
      "epoch 825 / 1000, loss = 2.7909, valid loss:  3.0279\n",
      "epoch 826 / 1000, loss = 2.7694, valid loss:  3.0088\n",
      "epoch 827 / 1000, loss = 2.7799, valid loss:  3.0046\n",
      "epoch 828 / 1000, loss = 2.8234, valid loss:  3.0190\n",
      "epoch 829 / 1000, loss = 2.7636, valid loss:  3.0021\n",
      "epoch 830 / 1000, loss = 2.7622, valid loss:  3.0512\n",
      "epoch 831 / 1000, loss = 2.7723, valid loss:  3.0141\n",
      "epoch 832 / 1000, loss = 2.7605, valid loss:  3.0578\n",
      "epoch 833 / 1000, loss = 2.7729, valid loss:  3.0189\n",
      "epoch 834 / 1000, loss = 2.7534, valid loss:  3.0000\n",
      "epoch 835 / 1000, loss = 2.7569, valid loss:  3.0524\n",
      "epoch 836 / 1000, loss = 2.8351, valid loss:  3.0179\n",
      "epoch 837 / 1000, loss = 2.8019, valid loss:  3.0069\n",
      "epoch 838 / 1000, loss = 2.7546, valid loss:  3.0073\n",
      "epoch 839 / 1000, loss = 2.7485, valid loss:  3.0097\n",
      "epoch 840 / 1000, loss = 2.7713, valid loss:  3.0056\n",
      "epoch 841 / 1000, loss = 2.8028, valid loss:  3.0067\n",
      "epoch 842 / 1000, loss = 2.7724, valid loss:  3.0064\n",
      "epoch 843 / 1000, loss = 2.7613, valid loss:  3.0173\n",
      "epoch 844 / 1000, loss = 2.7509, valid loss:  3.0106\n",
      "epoch 845 / 1000, loss = 2.7747, valid loss:  3.0029\n",
      "epoch 846 / 1000, loss = 2.7838, valid loss:  3.0011\n",
      "epoch 847 / 1000, loss = 2.7511, valid loss:  3.0101\n",
      "epoch 848 / 1000, loss = 2.7482, valid loss:  3.0071\n",
      "epoch 849 / 1000, loss = 2.7473, valid loss:  3.0051\n",
      "epoch 850 / 1000, loss = 2.7433, valid loss:  3.0084\n",
      "epoch 851 / 1000, loss = 2.7735, valid loss:  3.0148\n",
      "epoch 852 / 1000, loss = 2.7823, valid loss:  3.0003\n",
      "epoch 853 / 1000, loss = 2.7418, valid loss:  3.0105\n",
      "epoch 854 / 1000, loss = 2.7467, valid loss:  3.0074\n",
      "epoch 855 / 1000, loss = 2.8211, valid loss:  3.0376\n",
      "epoch 856 / 1000, loss = 2.7559, valid loss:  2.9943\n",
      "epoch 857 / 1000, loss = 2.7750, valid loss:  3.0183\n",
      "epoch 858 / 1000, loss = 2.7784, valid loss:  3.0000\n",
      "epoch 859 / 1000, loss = 2.7545, valid loss:  2.9940\n",
      "epoch 860 / 1000, loss = 2.7497, valid loss:  3.0121\n",
      "epoch 861 / 1000, loss = 2.7478, valid loss:  3.0329\n",
      "epoch 862 / 1000, loss = 2.7592, valid loss:  3.0621\n",
      "epoch 863 / 1000, loss = 2.7485, valid loss:  3.0004\n",
      "epoch 864 / 1000, loss = 2.7840, valid loss:  2.9994\n",
      "epoch 865 / 1000, loss = 2.7475, valid loss:  3.0118\n",
      "epoch 866 / 1000, loss = 2.7683, valid loss:  3.0077\n",
      "epoch 867 / 1000, loss = 2.8317, valid loss:  3.0496\n",
      "epoch 868 / 1000, loss = 2.7567, valid loss:  3.0177\n",
      "epoch 869 / 1000, loss = 2.7393, valid loss:  3.0023\n",
      "epoch 870 / 1000, loss = 2.7713, valid loss:  3.0077\n",
      "epoch 871 / 1000, loss = 2.7531, valid loss:  2.9970\n",
      "epoch 872 / 1000, loss = 2.8307, valid loss:  3.0464\n",
      "epoch 873 / 1000, loss = 2.7705, valid loss:  3.0035\n",
      "epoch 874 / 1000, loss = 2.7426, valid loss:  3.0010\n",
      "epoch 875 / 1000, loss = 2.7592, valid loss:  3.0168\n",
      "epoch 876 / 1000, loss = 2.7518, valid loss:  3.0132\n",
      "epoch 877 / 1000, loss = 2.7444, valid loss:  3.0025\n",
      "epoch 878 / 1000, loss = 2.7382, valid loss:  3.0179\n",
      "epoch 879 / 1000, loss = 2.7626, valid loss:  3.0188\n",
      "epoch 880 / 1000, loss = 2.7396, valid loss:  3.0037\n",
      "epoch 881 / 1000, loss = 2.7700, valid loss:  3.0034\n",
      "epoch 882 / 1000, loss = 2.7580, valid loss:  3.0092\n",
      "epoch 883 / 1000, loss = 2.8087, valid loss:  3.0138\n",
      "epoch 884 / 1000, loss = 2.7422, valid loss:  2.9846\n",
      "epoch 885 / 1000, loss = 2.7355, valid loss:  3.0068\n",
      "epoch 886 / 1000, loss = 2.7561, valid loss:  3.0614\n",
      "epoch 887 / 1000, loss = 2.7393, valid loss:  3.0406\n",
      "epoch 888 / 1000, loss = 2.7395, valid loss:  3.0442\n",
      "epoch 889 / 1000, loss = 2.7620, valid loss:  3.0083\n",
      "epoch 890 / 1000, loss = 2.7434, valid loss:  3.0338\n",
      "epoch 891 / 1000, loss = 2.7483, valid loss:  3.0055\n",
      "epoch 892 / 1000, loss = 2.7747, valid loss:  2.9953\n",
      "epoch 893 / 1000, loss = 2.7350, valid loss:  2.9959\n",
      "epoch 894 / 1000, loss = 2.7927, valid loss:  3.0237\n",
      "epoch 895 / 1000, loss = 2.7514, valid loss:  3.0108\n",
      "epoch 896 / 1000, loss = 2.7370, valid loss:  2.9984\n",
      "epoch 897 / 1000, loss = 2.7480, valid loss:  3.0118\n",
      "epoch 898 / 1000, loss = 2.7908, valid loss:  3.0289\n",
      "epoch 899 / 1000, loss = 2.7590, valid loss:  3.0084\n",
      "epoch 900 / 1000, loss = 2.7507, valid loss:  3.0096\n",
      "epoch 901 / 1000, loss = 2.7349, valid loss:  3.0003\n",
      "epoch 902 / 1000, loss = 2.7599, valid loss:  3.0222\n",
      "epoch 903 / 1000, loss = 2.7332, valid loss:  3.0088\n",
      "epoch 904 / 1000, loss = 2.7315, valid loss:  3.0373\n",
      "epoch 905 / 1000, loss = 2.7823, valid loss:  2.9983\n",
      "epoch 906 / 1000, loss = 2.7430, valid loss:  3.0020\n",
      "epoch 907 / 1000, loss = 2.7324, valid loss:  3.0028\n",
      "epoch 908 / 1000, loss = 2.7767, valid loss:  3.0660\n",
      "epoch 909 / 1000, loss = 2.7422, valid loss:  2.9937\n",
      "epoch 910 / 1000, loss = 2.7359, valid loss:  3.0398\n",
      "epoch 911 / 1000, loss = 2.7528, valid loss:  3.0053\n",
      "epoch 912 / 1000, loss = 2.7569, valid loss:  3.0006\n",
      "epoch 913 / 1000, loss = 2.7484, valid loss:  3.0085\n",
      "epoch 914 / 1000, loss = 2.7397, valid loss:  3.0029\n",
      "epoch 915 / 1000, loss = 2.7334, valid loss:  3.0093\n",
      "epoch 916 / 1000, loss = 2.7411, valid loss:  3.0483\n",
      "epoch 917 / 1000, loss = 2.7345, valid loss:  3.0227\n",
      "epoch 918 / 1000, loss = 2.7344, valid loss:  3.0054\n",
      "epoch 919 / 1000, loss = 2.7680, valid loss:  3.0057\n",
      "epoch 920 / 1000, loss = 2.7510, valid loss:  3.0019\n",
      "epoch 921 / 1000, loss = 2.7546, valid loss:  3.0031\n",
      "epoch 922 / 1000, loss = 2.7541, valid loss:  3.0157\n",
      "epoch 923 / 1000, loss = 2.7316, valid loss:  2.9987\n",
      "epoch 924 / 1000, loss = 2.7791, valid loss:  3.0007\n",
      "epoch 925 / 1000, loss = 2.7328, valid loss:  3.0074\n",
      "epoch 926 / 1000, loss = 2.7460, valid loss:  2.9940\n",
      "epoch 927 / 1000, loss = 2.7395, valid loss:  2.9976\n",
      "epoch 928 / 1000, loss = 2.7570, valid loss:  2.9964\n",
      "epoch 929 / 1000, loss = 2.7327, valid loss:  3.0198\n",
      "epoch 930 / 1000, loss = 2.7581, valid loss:  3.0750\n",
      "epoch 931 / 1000, loss = 2.7448, valid loss:  3.0043\n",
      "epoch 932 / 1000, loss = 2.7340, valid loss:  2.9900\n",
      "epoch 933 / 1000, loss = 2.7331, valid loss:  2.9997\n",
      "epoch 934 / 1000, loss = 2.8039, valid loss:  3.0260\n",
      "epoch 935 / 1000, loss = 2.7342, valid loss:  2.9996\n",
      "epoch 936 / 1000, loss = 2.7405, valid loss:  2.9892\n",
      "epoch 937 / 1000, loss = 2.7332, valid loss:  2.9995\n",
      "epoch 938 / 1000, loss = 2.7276, valid loss:  2.9878\n",
      "epoch 939 / 1000, loss = 2.7374, valid loss:  2.9931\n",
      "epoch 940 / 1000, loss = 2.7288, valid loss:  2.9952\n",
      "epoch 941 / 1000, loss = 2.7400, valid loss:  3.0481\n",
      "epoch 942 / 1000, loss = 2.7455, valid loss:  3.0398\n",
      "epoch 943 / 1000, loss = 2.8243, valid loss:  3.0051\n",
      "epoch 944 / 1000, loss = 2.7250, valid loss:  3.0308\n",
      "epoch 945 / 1000, loss = 2.8131, valid loss:  3.0128\n",
      "epoch 946 / 1000, loss = 2.7393, valid loss:  2.9908\n",
      "epoch 947 / 1000, loss = 2.7274, valid loss:  2.9918\n",
      "epoch 948 / 1000, loss = 2.7587, valid loss:  3.0640\n",
      "epoch 949 / 1000, loss = 2.7485, valid loss:  3.0654\n",
      "epoch 950 / 1000, loss = 2.7563, valid loss:  3.0013\n",
      "epoch 951 / 1000, loss = 2.7333, valid loss:  3.0496\n",
      "epoch 952 / 1000, loss = 2.7239, valid loss:  3.0000\n",
      "epoch 953 / 1000, loss = 2.7261, valid loss:  3.0185\n",
      "epoch 954 / 1000, loss = 2.8057, valid loss:  3.0099\n",
      "epoch 955 / 1000, loss = 2.7333, valid loss:  2.9859\n",
      "epoch 956 / 1000, loss = 2.7349, valid loss:  3.0465\n",
      "epoch 957 / 1000, loss = 2.7646, valid loss:  3.0317\n",
      "epoch 958 / 1000, loss = 2.7231, valid loss:  2.9956\n",
      "epoch 959 / 1000, loss = 2.7269, valid loss:  2.9907\n",
      "epoch 960 / 1000, loss = 2.7214, valid loss:  2.9916\n",
      "epoch 961 / 1000, loss = 2.7256, valid loss:  2.9906\n",
      "epoch 962 / 1000, loss = 2.7704, valid loss:  3.0407\n",
      "epoch 963 / 1000, loss = 2.7199, valid loss:  3.0202\n",
      "epoch 964 / 1000, loss = 2.7233, valid loss:  3.0068\n",
      "epoch 965 / 1000, loss = 2.7260, valid loss:  3.0210\n",
      "epoch 966 / 1000, loss = 2.7253, valid loss:  3.0133\n",
      "epoch 967 / 1000, loss = 2.7517, valid loss:  3.0669\n",
      "epoch 968 / 1000, loss = 2.7452, valid loss:  3.0715\n",
      "epoch 969 / 1000, loss = 2.7357, valid loss:  3.0514\n",
      "epoch 970 / 1000, loss = 2.7378, valid loss:  3.0601\n",
      "epoch 971 / 1000, loss = 2.7245, valid loss:  2.9865\n",
      "epoch 972 / 1000, loss = 2.7462, valid loss:  2.9960\n",
      "epoch 973 / 1000, loss = 2.7249, valid loss:  3.0334\n",
      "epoch 974 / 1000, loss = 2.7223, valid loss:  2.9857\n",
      "epoch 975 / 1000, loss = 2.7397, valid loss:  3.0452\n",
      "epoch 976 / 1000, loss = 2.7493, valid loss:  3.0596\n",
      "epoch 977 / 1000, loss = 2.7420, valid loss:  3.0704\n",
      "epoch 978 / 1000, loss = 2.7245, valid loss:  3.0468\n",
      "epoch 979 / 1000, loss = 2.7325, valid loss:  2.9917\n",
      "epoch 980 / 1000, loss = 2.7337, valid loss:  3.0484\n",
      "epoch 981 / 1000, loss = 2.7332, valid loss:  3.0411\n",
      "epoch 982 / 1000, loss = 2.7275, valid loss:  3.0544\n",
      "epoch 983 / 1000, loss = 2.7183, valid loss:  3.0072\n",
      "epoch 984 / 1000, loss = 2.7171, valid loss:  3.0386\n",
      "epoch 985 / 1000, loss = 2.7432, valid loss:  3.0269\n",
      "epoch 986 / 1000, loss = 2.7208, valid loss:  3.0447\n",
      "epoch 987 / 1000, loss = 2.7295, valid loss:  2.9941\n",
      "epoch 988 / 1000, loss = 2.7234, valid loss:  3.0463\n",
      "epoch 989 / 1000, loss = 2.7308, valid loss:  3.0580\n",
      "epoch 990 / 1000, loss = 2.7276, valid loss:  3.0528\n",
      "epoch 991 / 1000, loss = 2.7267, valid loss:  2.9919\n",
      "epoch 992 / 1000, loss = 2.7323, valid loss:  3.0411\n",
      "epoch 993 / 1000, loss = 2.7307, valid loss:  3.0623\n",
      "epoch 994 / 1000, loss = 2.7297, valid loss:  3.0506\n",
      "epoch 995 / 1000, loss = 2.7127, valid loss:  2.9836\n",
      "epoch 996 / 1000, loss = 2.7323, valid loss:  3.0371\n",
      "epoch 997 / 1000, loss = 2.7349, valid loss:  3.0348\n",
      "epoch 998 / 1000, loss = 2.7165, valid loss:  2.9802\n",
      "epoch 999 / 1000, loss = 2.7341, valid loss:  3.0375\n",
      "epoch 1000 / 1000, loss = 2.7218, valid loss:  3.0027\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Thermal Accept Accuracy</td><td>▁▁▃▃▃▃▄▄▄▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▆▆▇▇▇▇▇▇▇█▇█▇█</td></tr><tr><td>Thermal Accept Loss</td><td>█▆▆▆▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▂▁</td></tr><tr><td>Thermal Accept Valid Accuracy</td><td>▁▃▃▂▃▃▃▃▅▅▅▆▇▆▇▆▆█▇▇▇▇█▇▇█▇▆██▆█▇▇▆▇▇▆▆▆</td></tr><tr><td>Thermal Accept Valid Loss</td><td>█▆▆▅▅▅▄▄▄▃▃▃▂▂▂▂▂▁▁▁▁▁▁▂▂▁▂▂▂▂▂▁▁▁▂▁▁▁▂▂</td></tr><tr><td>Thermal Comfort Accuracy</td><td>▅▅▅▃▁▂▁▁▁▂▃▅▅▆▆▆▆▅▆▅▆▆▆▆▇▇▇▆▇▇▇▇▆▇▇▇▇█▇█</td></tr><tr><td>Thermal Comfort Loss</td><td>█▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Thermal Comfort Valid Accuracy</td><td>▆▆▆▂▁▁▁▁▁▁▂▆▆▆▆▅▆▆▅▆▆▆▇█▇▇▆▆▆▇▇▇▇▆▇▇▇▇▇▇</td></tr><tr><td>Thermal Comfort Valid Loss</td><td>█▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▂▂▁▂▂▁▁▂▁▁▁▁▁▁▁▂▁</td></tr><tr><td>Thermal Sensation Accuracy</td><td>▂▂▂▂▂▃▂▁▁▂▁▂▃▂▂▄▄▅▅▆▆▄▃▄▄▆▆█▅▄▅▄▃▆▄▅▆▅▅▇</td></tr><tr><td>Thermal Sensation Loss</td><td>█▅▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Thermal Sensation Valid Accuracy</td><td>▄▄▄▄▅███████▇███▇▆▅▄▄▄▄▄▄▄▄▄▅▄▄▄▄▅▅▅▄▃▁▁</td></tr><tr><td>Thermal Sensation Valid Loss</td><td>█▄▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▁▂▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Thermal Accept Accuracy</td><td>0.62437</td></tr><tr><td>Thermal Accept Loss</td><td>0.00652</td></tr><tr><td>Thermal Accept Valid Accuracy</td><td>0.60743</td></tr><tr><td>Thermal Accept Valid Loss</td><td>0.00636</td></tr><tr><td>Thermal Comfort Accuracy</td><td>0.59424</td></tr><tr><td>Thermal Comfort Loss</td><td>0.00746</td></tr><tr><td>Thermal Comfort Valid Accuracy</td><td>0.5994</td></tr><tr><td>Thermal Comfort Valid Loss</td><td>0.00748</td></tr><tr><td>Thermal Sensation Accuracy</td><td>0.47606</td></tr><tr><td>Thermal Sensation Loss</td><td>0.01058</td></tr><tr><td>Thermal Sensation Valid Accuracy</td><td>0.44478</td></tr><tr><td>Thermal Sensation Valid Loss</td><td>0.01098</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">deft-sweep-1</strong> at: <a href='https://wandb.ai/lhk/Thermal-with-Fully-Chinese/runs/8at5n7mo' target=\"_blank\">https://wandb.ai/lhk/Thermal-with-Fully-Chinese/runs/8at5n7mo</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240322_003308-8at5n7mo\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: cvttr8jr with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 1000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.001784570990624434\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>e:\\LHK\\Study\\Lab\\SmartCity\\3.Others\\ThermalComfort\\wandb\\run-20240322_004615-cvttr8jr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/lhk/Thermal-with-Fully-Chinese/runs/cvttr8jr' target=\"_blank\">stellar-sweep-2</a></strong> to <a href='https://wandb.ai/lhk/Thermal-with-Fully-Chinese' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/lhk/Thermal-with-Fully-Chinese/sweeps/yb81qiu6' target=\"_blank\">https://wandb.ai/lhk/Thermal-with-Fully-Chinese/sweeps/yb81qiu6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/lhk/Thermal-with-Fully-Chinese' target=\"_blank\">https://wandb.ai/lhk/Thermal-with-Fully-Chinese</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/lhk/Thermal-with-Fully-Chinese/sweeps/yb81qiu6' target=\"_blank\">https://wandb.ai/lhk/Thermal-with-Fully-Chinese/sweeps/yb81qiu6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/lhk/Thermal-with-Fully-Chinese/runs/cvttr8jr' target=\"_blank\">https://wandb.ai/lhk/Thermal-with-Fully-Chinese/runs/cvttr8jr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 / 1000, loss = 4.6966, valid loss:  4.7096\n",
      "epoch 2 / 1000, loss = 4.4191, valid loss:  4.4012\n",
      "epoch 3 / 1000, loss = 4.1545, valid loss:  4.1009\n",
      "epoch 4 / 1000, loss = 3.9391, valid loss:  3.8577\n",
      "epoch 5 / 1000, loss = 3.8106, valid loss:  3.7121\n",
      "epoch 6 / 1000, loss = 3.7342, valid loss:  3.6340\n",
      "epoch 7 / 1000, loss = 3.6817, valid loss:  3.5914\n",
      "epoch 8 / 1000, loss = 3.6426, valid loss:  3.5655\n",
      "epoch 9 / 1000, loss = 3.6139, valid loss:  3.5492\n",
      "epoch 10 / 1000, loss = 3.5915, valid loss:  3.5385\n",
      "epoch 11 / 1000, loss = 3.5666, valid loss:  3.5298\n",
      "epoch 12 / 1000, loss = 3.5494, valid loss:  3.5248\n",
      "epoch 13 / 1000, loss = 3.5342, valid loss:  3.5206\n",
      "epoch 14 / 1000, loss = 3.5253, valid loss:  3.5182\n",
      "epoch 15 / 1000, loss = 3.5164, valid loss:  3.5161\n",
      "epoch 16 / 1000, loss = 3.5030, valid loss:  3.5131\n",
      "epoch 17 / 1000, loss = 3.4935, valid loss:  3.5098\n",
      "epoch 18 / 1000, loss = 3.4845, valid loss:  3.5090\n",
      "epoch 19 / 1000, loss = 3.4787, valid loss:  3.5079\n",
      "epoch 20 / 1000, loss = 3.4717, valid loss:  3.5057\n",
      "epoch 21 / 1000, loss = 3.4568, valid loss:  3.5020\n",
      "epoch 22 / 1000, loss = 3.4503, valid loss:  3.5020\n",
      "epoch 23 / 1000, loss = 3.4448, valid loss:  3.5010\n",
      "epoch 24 / 1000, loss = 3.4393, valid loss:  3.4998\n",
      "epoch 25 / 1000, loss = 3.4338, valid loss:  3.4987\n",
      "epoch 26 / 1000, loss = 3.4283, valid loss:  3.4978\n",
      "epoch 27 / 1000, loss = 3.4229, valid loss:  3.4974\n",
      "epoch 28 / 1000, loss = 3.4175, valid loss:  3.4978\n",
      "epoch 29 / 1000, loss = 3.4121, valid loss:  3.4969\n",
      "epoch 30 / 1000, loss = 3.4059, valid loss:  3.4948\n",
      "epoch 31 / 1000, loss = 3.3988, valid loss:  3.4923\n",
      "epoch 32 / 1000, loss = 3.3932, valid loss:  3.4909\n",
      "epoch 33 / 1000, loss = 3.3877, valid loss:  3.4894\n",
      "epoch 34 / 1000, loss = 3.3822, valid loss:  3.4880\n",
      "epoch 35 / 1000, loss = 3.3767, valid loss:  3.4864\n",
      "epoch 36 / 1000, loss = 3.3712, valid loss:  3.4846\n",
      "epoch 37 / 1000, loss = 3.3657, valid loss:  3.4826\n",
      "epoch 38 / 1000, loss = 3.3602, valid loss:  3.4807\n",
      "epoch 39 / 1000, loss = 3.3549, valid loss:  3.4790\n",
      "epoch 40 / 1000, loss = 3.3496, valid loss:  3.4773\n",
      "epoch 41 / 1000, loss = 3.3444, valid loss:  3.4756\n",
      "epoch 42 / 1000, loss = 3.3393, valid loss:  3.4739\n",
      "epoch 43 / 1000, loss = 3.3343, valid loss:  3.4722\n",
      "epoch 44 / 1000, loss = 3.3294, valid loss:  3.4705\n",
      "epoch 45 / 1000, loss = 3.3246, valid loss:  3.4688\n",
      "epoch 46 / 1000, loss = 3.3199, valid loss:  3.4671\n",
      "epoch 47 / 1000, loss = 3.3152, valid loss:  3.4654\n",
      "epoch 48 / 1000, loss = 3.3106, valid loss:  3.4637\n",
      "epoch 49 / 1000, loss = 3.3061, valid loss:  3.4620\n",
      "epoch 50 / 1000, loss = 3.3017, valid loss:  3.4603\n",
      "epoch 51 / 1000, loss = 3.2973, valid loss:  3.4586\n",
      "epoch 52 / 1000, loss = 3.2929, valid loss:  3.4569\n",
      "epoch 53 / 1000, loss = 3.2886, valid loss:  3.4552\n",
      "epoch 54 / 1000, loss = 3.2845, valid loss:  3.4535\n",
      "epoch 55 / 1000, loss = 3.2803, valid loss:  3.4518\n",
      "epoch 56 / 1000, loss = 3.2763, valid loss:  3.4500\n",
      "epoch 57 / 1000, loss = 3.2725, valid loss:  3.4483\n",
      "epoch 58 / 1000, loss = 3.2687, valid loss:  3.4464\n",
      "epoch 59 / 1000, loss = 3.2649, valid loss:  3.4446\n",
      "epoch 60 / 1000, loss = 3.2613, valid loss:  3.4427\n",
      "epoch 61 / 1000, loss = 3.2577, valid loss:  3.4408\n",
      "epoch 62 / 1000, loss = 3.2541, valid loss:  3.4388\n",
      "epoch 63 / 1000, loss = 3.2507, valid loss:  3.4369\n",
      "epoch 64 / 1000, loss = 3.2472, valid loss:  3.4350\n",
      "epoch 65 / 1000, loss = 3.2439, valid loss:  3.4330\n",
      "epoch 66 / 1000, loss = 3.2405, valid loss:  3.4311\n",
      "epoch 67 / 1000, loss = 3.2372, valid loss:  3.4292\n",
      "epoch 68 / 1000, loss = 3.2339, valid loss:  3.4272\n",
      "epoch 69 / 1000, loss = 3.2307, valid loss:  3.4253\n",
      "epoch 70 / 1000, loss = 3.2274, valid loss:  3.4234\n",
      "epoch 71 / 1000, loss = 3.2241, valid loss:  3.4215\n",
      "epoch 72 / 1000, loss = 3.2208, valid loss:  3.4196\n",
      "epoch 73 / 1000, loss = 3.2176, valid loss:  3.4177\n",
      "epoch 74 / 1000, loss = 3.2142, valid loss:  3.4157\n",
      "epoch 75 / 1000, loss = 3.2108, valid loss:  3.4137\n",
      "epoch 76 / 1000, loss = 3.2076, valid loss:  3.4117\n",
      "epoch 77 / 1000, loss = 3.2045, valid loss:  3.4095\n",
      "epoch 78 / 1000, loss = 3.2017, valid loss:  3.4074\n",
      "epoch 79 / 1000, loss = 3.1991, valid loss:  3.4053\n",
      "epoch 80 / 1000, loss = 3.1966, valid loss:  3.4032\n",
      "epoch 81 / 1000, loss = 3.1942, valid loss:  3.4011\n",
      "epoch 82 / 1000, loss = 3.1919, valid loss:  3.3991\n",
      "epoch 83 / 1000, loss = 3.1897, valid loss:  3.3971\n",
      "epoch 84 / 1000, loss = 3.1875, valid loss:  3.3951\n",
      "epoch 85 / 1000, loss = 3.1853, valid loss:  3.3931\n",
      "epoch 86 / 1000, loss = 3.1832, valid loss:  3.3912\n",
      "epoch 87 / 1000, loss = 3.1811, valid loss:  3.3893\n",
      "epoch 88 / 1000, loss = 3.1791, valid loss:  3.3874\n",
      "epoch 89 / 1000, loss = 3.1770, valid loss:  3.3855\n",
      "epoch 90 / 1000, loss = 3.1750, valid loss:  3.3836\n",
      "epoch 91 / 1000, loss = 3.1730, valid loss:  3.3818\n",
      "epoch 92 / 1000, loss = 3.1711, valid loss:  3.3800\n",
      "epoch 93 / 1000, loss = 3.1692, valid loss:  3.3782\n",
      "epoch 94 / 1000, loss = 3.1673, valid loss:  3.3764\n",
      "epoch 95 / 1000, loss = 3.1654, valid loss:  3.3747\n",
      "epoch 96 / 1000, loss = 3.1636, valid loss:  3.3729\n",
      "epoch 97 / 1000, loss = 3.1619, valid loss:  3.3712\n",
      "epoch 98 / 1000, loss = 3.1601, valid loss:  3.3696\n",
      "epoch 99 / 1000, loss = 3.1584, valid loss:  3.3679\n",
      "epoch 100 / 1000, loss = 3.1568, valid loss:  3.3663\n",
      "epoch 101 / 1000, loss = 3.1552, valid loss:  3.3647\n",
      "epoch 102 / 1000, loss = 3.1536, valid loss:  3.3632\n",
      "epoch 103 / 1000, loss = 3.1520, valid loss:  3.3617\n",
      "epoch 104 / 1000, loss = 3.1505, valid loss:  3.3602\n",
      "epoch 105 / 1000, loss = 3.1490, valid loss:  3.3588\n",
      "epoch 106 / 1000, loss = 3.1475, valid loss:  3.3574\n",
      "epoch 107 / 1000, loss = 3.1461, valid loss:  3.3560\n",
      "epoch 108 / 1000, loss = 3.1447, valid loss:  3.3548\n",
      "epoch 109 / 1000, loss = 3.1434, valid loss:  3.3535\n",
      "epoch 110 / 1000, loss = 3.1421, valid loss:  3.3523\n",
      "epoch 111 / 1000, loss = 3.1409, valid loss:  3.3512\n",
      "epoch 112 / 1000, loss = 3.1397, valid loss:  3.3501\n",
      "epoch 113 / 1000, loss = 3.1387, valid loss:  3.3490\n",
      "epoch 114 / 1000, loss = 3.1377, valid loss:  3.3479\n",
      "epoch 115 / 1000, loss = 3.1368, valid loss:  3.3468\n",
      "epoch 116 / 1000, loss = 3.1386, valid loss:  3.3462\n",
      "epoch 117 / 1000, loss = 3.1381, valid loss:  3.3433\n",
      "epoch 118 / 1000, loss = 3.1370, valid loss:  3.3407\n",
      "epoch 119 / 1000, loss = 3.1354, valid loss:  3.3389\n",
      "epoch 120 / 1000, loss = 3.1337, valid loss:  3.3372\n",
      "epoch 121 / 1000, loss = 3.1319, valid loss:  3.3353\n",
      "epoch 122 / 1000, loss = 3.1301, valid loss:  3.3335\n",
      "epoch 123 / 1000, loss = 3.1284, valid loss:  3.3318\n",
      "epoch 124 / 1000, loss = 3.1267, valid loss:  3.3303\n",
      "epoch 125 / 1000, loss = 3.1250, valid loss:  3.3288\n",
      "epoch 126 / 1000, loss = 3.1235, valid loss:  3.3275\n",
      "epoch 127 / 1000, loss = 3.1220, valid loss:  3.3263\n",
      "epoch 128 / 1000, loss = 3.1206, valid loss:  3.3251\n",
      "epoch 129 / 1000, loss = 3.1193, valid loss:  3.3240\n",
      "epoch 130 / 1000, loss = 3.1180, valid loss:  3.3230\n",
      "epoch 131 / 1000, loss = 3.1169, valid loss:  3.3220\n",
      "epoch 132 / 1000, loss = 3.1159, valid loss:  3.3212\n",
      "epoch 133 / 1000, loss = 3.1150, valid loss:  3.3204\n",
      "epoch 134 / 1000, loss = 3.1143, valid loss:  3.3197\n",
      "epoch 135 / 1000, loss = 3.1138, valid loss:  3.3192\n",
      "epoch 136 / 1000, loss = 3.1135, valid loss:  3.3189\n",
      "epoch 137 / 1000, loss = 3.1133, valid loss:  3.3188\n",
      "epoch 138 / 1000, loss = 3.1129, valid loss:  3.3187\n",
      "epoch 139 / 1000, loss = 3.1122, valid loss:  3.3182\n",
      "epoch 140 / 1000, loss = 3.1113, valid loss:  3.3176\n",
      "epoch 141 / 1000, loss = 3.1103, valid loss:  3.3168\n",
      "epoch 142 / 1000, loss = 3.1092, valid loss:  3.3160\n",
      "epoch 143 / 1000, loss = 3.1080, valid loss:  3.3151\n",
      "epoch 144 / 1000, loss = 3.1068, valid loss:  3.3143\n",
      "epoch 145 / 1000, loss = 3.1056, valid loss:  3.3134\n",
      "epoch 146 / 1000, loss = 3.1044, valid loss:  3.3126\n",
      "epoch 147 / 1000, loss = 3.1032, valid loss:  3.3117\n",
      "epoch 148 / 1000, loss = 3.1020, valid loss:  3.3108\n",
      "epoch 149 / 1000, loss = 3.1008, valid loss:  3.3098\n",
      "epoch 150 / 1000, loss = 3.0996, valid loss:  3.3089\n",
      "epoch 151 / 1000, loss = 3.0984, valid loss:  3.3079\n",
      "epoch 152 / 1000, loss = 3.0972, valid loss:  3.3070\n",
      "epoch 153 / 1000, loss = 3.0960, valid loss:  3.3060\n",
      "epoch 154 / 1000, loss = 3.0948, valid loss:  3.3050\n",
      "epoch 155 / 1000, loss = 3.0936, valid loss:  3.3039\n",
      "epoch 156 / 1000, loss = 3.0924, valid loss:  3.3029\n",
      "epoch 157 / 1000, loss = 3.0912, valid loss:  3.3018\n",
      "epoch 158 / 1000, loss = 3.0900, valid loss:  3.3006\n",
      "epoch 159 / 1000, loss = 3.0888, valid loss:  3.2994\n",
      "epoch 160 / 1000, loss = 3.0876, valid loss:  3.2982\n",
      "epoch 161 / 1000, loss = 3.0865, valid loss:  3.2968\n",
      "epoch 162 / 1000, loss = 3.0853, valid loss:  3.2955\n",
      "epoch 163 / 1000, loss = 3.0841, valid loss:  3.2940\n",
      "epoch 164 / 1000, loss = 3.0829, valid loss:  3.2925\n",
      "epoch 165 / 1000, loss = 3.0818, valid loss:  3.2911\n",
      "epoch 166 / 1000, loss = 3.0806, valid loss:  3.2898\n",
      "epoch 167 / 1000, loss = 3.0794, valid loss:  3.2881\n",
      "epoch 168 / 1000, loss = 3.0783, valid loss:  3.2859\n",
      "epoch 169 / 1000, loss = 3.0774, valid loss:  3.2834\n",
      "epoch 170 / 1000, loss = 3.0770, valid loss:  3.2814\n",
      "epoch 171 / 1000, loss = 3.0765, valid loss:  3.2798\n",
      "epoch 172 / 1000, loss = 3.0755, valid loss:  3.2782\n",
      "epoch 173 / 1000, loss = 3.0742, valid loss:  3.2768\n",
      "epoch 174 / 1000, loss = 3.0729, valid loss:  3.2754\n",
      "epoch 175 / 1000, loss = 3.0716, valid loss:  3.2740\n",
      "epoch 176 / 1000, loss = 3.0702, valid loss:  3.2727\n",
      "epoch 177 / 1000, loss = 3.0689, valid loss:  3.2714\n",
      "epoch 178 / 1000, loss = 3.0675, valid loss:  3.2702\n",
      "epoch 179 / 1000, loss = 3.0661, valid loss:  3.2689\n",
      "epoch 180 / 1000, loss = 3.0647, valid loss:  3.2676\n",
      "epoch 181 / 1000, loss = 3.0633, valid loss:  3.2663\n",
      "epoch 182 / 1000, loss = 3.0619, valid loss:  3.2651\n",
      "epoch 183 / 1000, loss = 3.0604, valid loss:  3.2638\n",
      "epoch 184 / 1000, loss = 3.0590, valid loss:  3.2627\n",
      "epoch 185 / 1000, loss = 3.0576, valid loss:  3.2617\n",
      "epoch 186 / 1000, loss = 3.0562, valid loss:  3.2609\n",
      "epoch 187 / 1000, loss = 3.0549, valid loss:  3.2601\n",
      "epoch 188 / 1000, loss = 3.0535, valid loss:  3.2594\n",
      "epoch 189 / 1000, loss = 3.0521, valid loss:  3.2588\n",
      "epoch 190 / 1000, loss = 3.0507, valid loss:  3.2583\n",
      "epoch 191 / 1000, loss = 3.0493, valid loss:  3.2578\n",
      "epoch 192 / 1000, loss = 3.0479, valid loss:  3.2574\n",
      "epoch 193 / 1000, loss = 3.0465, valid loss:  3.2570\n",
      "epoch 194 / 1000, loss = 3.0451, valid loss:  3.2565\n",
      "epoch 195 / 1000, loss = 3.0437, valid loss:  3.2560\n",
      "epoch 196 / 1000, loss = 3.0423, valid loss:  3.2555\n",
      "epoch 197 / 1000, loss = 3.0409, valid loss:  3.2549\n",
      "epoch 198 / 1000, loss = 3.0395, valid loss:  3.2543\n",
      "epoch 199 / 1000, loss = 3.0381, valid loss:  3.2536\n",
      "epoch 200 / 1000, loss = 3.0366, valid loss:  3.2530\n",
      "epoch 201 / 1000, loss = 3.0352, valid loss:  3.2523\n",
      "epoch 202 / 1000, loss = 3.0338, valid loss:  3.2516\n",
      "epoch 203 / 1000, loss = 3.0323, valid loss:  3.2509\n",
      "epoch 204 / 1000, loss = 3.0309, valid loss:  3.2502\n",
      "epoch 205 / 1000, loss = 3.0295, valid loss:  3.2495\n",
      "epoch 206 / 1000, loss = 3.0281, valid loss:  3.2487\n",
      "epoch 207 / 1000, loss = 3.0267, valid loss:  3.2480\n",
      "epoch 208 / 1000, loss = 3.0253, valid loss:  3.2473\n",
      "epoch 209 / 1000, loss = 3.0239, valid loss:  3.2466\n",
      "epoch 210 / 1000, loss = 3.0225, valid loss:  3.2459\n",
      "epoch 211 / 1000, loss = 3.0211, valid loss:  3.2452\n",
      "epoch 212 / 1000, loss = 3.0198, valid loss:  3.2445\n",
      "epoch 213 / 1000, loss = 3.0184, valid loss:  3.2439\n",
      "epoch 214 / 1000, loss = 3.0171, valid loss:  3.2432\n",
      "epoch 215 / 1000, loss = 3.0157, valid loss:  3.2425\n",
      "epoch 216 / 1000, loss = 3.0144, valid loss:  3.2419\n",
      "epoch 217 / 1000, loss = 3.0131, valid loss:  3.2413\n",
      "epoch 218 / 1000, loss = 3.0118, valid loss:  3.2407\n",
      "epoch 219 / 1000, loss = 3.0105, valid loss:  3.2401\n",
      "epoch 220 / 1000, loss = 3.0092, valid loss:  3.2395\n",
      "epoch 221 / 1000, loss = 3.0079, valid loss:  3.2390\n",
      "epoch 222 / 1000, loss = 3.0066, valid loss:  3.2385\n",
      "epoch 223 / 1000, loss = 3.0053, valid loss:  3.2380\n",
      "epoch 224 / 1000, loss = 3.0040, valid loss:  3.2375\n",
      "epoch 225 / 1000, loss = 3.0028, valid loss:  3.2370\n",
      "epoch 226 / 1000, loss = 3.0016, valid loss:  3.2366\n",
      "epoch 227 / 1000, loss = 3.0005, valid loss:  3.2361\n",
      "epoch 228 / 1000, loss = 2.9994, valid loss:  3.2357\n",
      "epoch 229 / 1000, loss = 2.9985, valid loss:  3.2353\n",
      "epoch 230 / 1000, loss = 2.9976, valid loss:  3.2349\n",
      "epoch 231 / 1000, loss = 2.9969, valid loss:  3.2345\n",
      "epoch 232 / 1000, loss = 2.9963, valid loss:  3.2342\n",
      "epoch 233 / 1000, loss = 2.9957, valid loss:  3.2340\n",
      "epoch 234 / 1000, loss = 2.9952, valid loss:  3.2337\n",
      "epoch 235 / 1000, loss = 2.9945, valid loss:  3.2336\n",
      "epoch 236 / 1000, loss = 2.9938, valid loss:  3.2334\n",
      "epoch 237 / 1000, loss = 2.9931, valid loss:  3.2333\n",
      "epoch 238 / 1000, loss = 2.9922, valid loss:  3.2333\n",
      "epoch 239 / 1000, loss = 2.9914, valid loss:  3.2332\n",
      "epoch 240 / 1000, loss = 2.9905, valid loss:  3.2332\n",
      "epoch 241 / 1000, loss = 2.9897, valid loss:  3.2333\n",
      "epoch 242 / 1000, loss = 2.9889, valid loss:  3.2333\n",
      "epoch 243 / 1000, loss = 2.9881, valid loss:  3.2334\n",
      "epoch 244 / 1000, loss = 2.9873, valid loss:  3.2335\n",
      "epoch 245 / 1000, loss = 2.9866, valid loss:  3.2335\n",
      "epoch 246 / 1000, loss = 2.9858, valid loss:  3.2336\n",
      "epoch 247 / 1000, loss = 2.9851, valid loss:  3.2336\n",
      "epoch 248 / 1000, loss = 2.9842, valid loss:  3.2336\n",
      "epoch 249 / 1000, loss = 2.9833, valid loss:  3.2335\n",
      "epoch 250 / 1000, loss = 2.9822, valid loss:  3.2335\n",
      "epoch 251 / 1000, loss = 2.9810, valid loss:  3.2334\n",
      "epoch 252 / 1000, loss = 2.9796, valid loss:  3.2334\n",
      "epoch 253 / 1000, loss = 2.9780, valid loss:  3.2334\n",
      "epoch 254 / 1000, loss = 2.9763, valid loss:  3.2335\n",
      "epoch 255 / 1000, loss = 2.9749, valid loss:  3.2336\n",
      "epoch 256 / 1000, loss = 2.9741, valid loss:  3.2337\n",
      "epoch 257 / 1000, loss = 2.9743, valid loss:  3.2335\n",
      "epoch 258 / 1000, loss = 2.9757, valid loss:  3.2333\n",
      "epoch 259 / 1000, loss = 2.9772, valid loss:  3.2333\n",
      "epoch 260 / 1000, loss = 2.9775, valid loss:  3.2334\n",
      "epoch 261 / 1000, loss = 2.9768, valid loss:  3.2338\n",
      "epoch 262 / 1000, loss = 2.9764, valid loss:  3.2345\n",
      "epoch 263 / 1000, loss = 2.9766, valid loss:  3.2351\n",
      "epoch 264 / 1000, loss = 2.9767, valid loss:  3.2357\n",
      "epoch 265 / 1000, loss = 2.9766, valid loss:  3.2360\n",
      "epoch 266 / 1000, loss = 2.9763, valid loss:  3.2363\n",
      "epoch 267 / 1000, loss = 2.9759, valid loss:  3.2366\n",
      "epoch 268 / 1000, loss = 2.9754, valid loss:  3.2368\n",
      "epoch 269 / 1000, loss = 2.9749, valid loss:  3.2370\n",
      "epoch 270 / 1000, loss = 2.9744, valid loss:  3.2372\n",
      "epoch 271 / 1000, loss = 2.9739, valid loss:  3.2375\n",
      "epoch 272 / 1000, loss = 2.9734, valid loss:  3.2377\n",
      "epoch 273 / 1000, loss = 2.9730, valid loss:  3.2379\n",
      "epoch 274 / 1000, loss = 2.9725, valid loss:  3.2381\n",
      "epoch 275 / 1000, loss = 2.9722, valid loss:  3.2383\n",
      "epoch 276 / 1000, loss = 2.9718, valid loss:  3.2385\n",
      "epoch 277 / 1000, loss = 2.9715, valid loss:  3.2387\n",
      "epoch 278 / 1000, loss = 2.9713, valid loss:  3.2389\n",
      "epoch 279 / 1000, loss = 2.9710, valid loss:  3.2391\n",
      "epoch 280 / 1000, loss = 2.9708, valid loss:  3.2392\n",
      "epoch 281 / 1000, loss = 2.9707, valid loss:  3.2394\n",
      "epoch 282 / 1000, loss = 2.9706, valid loss:  3.2396\n",
      "epoch 283 / 1000, loss = 2.9705, valid loss:  3.2397\n",
      "epoch 284 / 1000, loss = 2.9704, valid loss:  3.2398\n",
      "epoch 285 / 1000, loss = 2.9703, valid loss:  3.2398\n",
      "epoch 286 / 1000, loss = 2.9703, valid loss:  3.2397\n",
      "epoch 287 / 1000, loss = 2.9704, valid loss:  3.2397\n",
      "epoch 288 / 1000, loss = 2.9704, valid loss:  3.2397\n",
      "epoch 289 / 1000, loss = 2.9705, valid loss:  3.2398\n",
      "epoch 290 / 1000, loss = 2.9706, valid loss:  3.2400\n",
      "epoch 291 / 1000, loss = 2.9707, valid loss:  3.2401\n",
      "epoch 292 / 1000, loss = 2.9707, valid loss:  3.2403\n",
      "epoch 293 / 1000, loss = 2.9707, valid loss:  3.2404\n",
      "epoch 294 / 1000, loss = 2.9706, valid loss:  3.2406\n",
      "epoch 295 / 1000, loss = 2.9706, valid loss:  3.2408\n",
      "epoch 296 / 1000, loss = 2.9705, valid loss:  3.2410\n",
      "epoch 297 / 1000, loss = 2.9705, valid loss:  3.2412\n",
      "epoch 298 / 1000, loss = 2.9704, valid loss:  3.2414\n",
      "epoch 299 / 1000, loss = 2.9703, valid loss:  3.2415\n",
      "epoch 300 / 1000, loss = 2.9703, valid loss:  3.2416\n",
      "epoch 301 / 1000, loss = 2.9703, valid loss:  3.2416\n",
      "epoch 302 / 1000, loss = 2.9703, valid loss:  3.2417\n",
      "epoch 303 / 1000, loss = 2.9704, valid loss:  3.2419\n",
      "epoch 304 / 1000, loss = 2.9704, valid loss:  3.2422\n",
      "epoch 305 / 1000, loss = 2.9701, valid loss:  3.2426\n",
      "epoch 306 / 1000, loss = 2.9696, valid loss:  3.2429\n",
      "epoch 307 / 1000, loss = 2.9693, valid loss:  3.2433\n",
      "epoch 308 / 1000, loss = 2.9691, valid loss:  3.2438\n",
      "epoch 309 / 1000, loss = 2.9690, valid loss:  3.2443\n",
      "epoch 310 / 1000, loss = 2.9689, valid loss:  3.2449\n",
      "epoch 311 / 1000, loss = 2.9687, valid loss:  3.2456\n",
      "epoch 312 / 1000, loss = 2.9685, valid loss:  3.2463\n",
      "epoch 313 / 1000, loss = 2.9682, valid loss:  3.2469\n",
      "epoch 314 / 1000, loss = 2.9679, valid loss:  3.2475\n",
      "epoch 315 / 1000, loss = 2.9679, valid loss:  3.2480\n",
      "epoch 316 / 1000, loss = 2.9682, valid loss:  3.2483\n",
      "epoch 317 / 1000, loss = 2.9695, valid loss:  3.2485\n",
      "epoch 318 / 1000, loss = 2.9704, valid loss:  3.2487\n",
      "epoch 319 / 1000, loss = 2.9706, valid loss:  3.2484\n",
      "epoch 320 / 1000, loss = 2.9685, valid loss:  3.2494\n",
      "epoch 321 / 1000, loss = 2.9642, valid loss:  3.2483\n",
      "epoch 322 / 1000, loss = 2.9625, valid loss:  3.2480\n",
      "epoch 323 / 1000, loss = 2.9649, valid loss:  3.2473\n",
      "epoch 324 / 1000, loss = 2.9655, valid loss:  3.2480\n",
      "epoch 325 / 1000, loss = 2.9660, valid loss:  3.2472\n",
      "epoch 326 / 1000, loss = 2.9648, valid loss:  3.2466\n",
      "epoch 327 / 1000, loss = 2.9639, valid loss:  3.2452\n",
      "epoch 328 / 1000, loss = 2.9623, valid loss:  3.2448\n",
      "epoch 329 / 1000, loss = 2.9599, valid loss:  3.2436\n",
      "epoch 330 / 1000, loss = 2.9582, valid loss:  3.2428\n",
      "epoch 331 / 1000, loss = 2.9568, valid loss:  3.2424\n",
      "epoch 332 / 1000, loss = 2.9571, valid loss:  3.2417\n",
      "epoch 333 / 1000, loss = 2.9578, valid loss:  3.2412\n",
      "epoch 334 / 1000, loss = 2.9575, valid loss:  3.2407\n",
      "epoch 335 / 1000, loss = 2.9671, valid loss:  3.2297\n",
      "epoch 336 / 1000, loss = 2.9616, valid loss:  3.2387\n",
      "epoch 337 / 1000, loss = 2.9532, valid loss:  3.2428\n",
      "epoch 338 / 1000, loss = 2.9672, valid loss:  3.2318\n",
      "epoch 339 / 1000, loss = 2.9602, valid loss:  3.2439\n",
      "epoch 340 / 1000, loss = 2.9536, valid loss:  3.2414\n",
      "epoch 341 / 1000, loss = 2.9595, valid loss:  3.2419\n",
      "epoch 342 / 1000, loss = 2.9559, valid loss:  3.2438\n",
      "epoch 343 / 1000, loss = 2.9561, valid loss:  3.2466\n",
      "epoch 344 / 1000, loss = 2.9551, valid loss:  3.2448\n",
      "epoch 345 / 1000, loss = 2.9558, valid loss:  3.2456\n",
      "epoch 346 / 1000, loss = 2.9572, valid loss:  3.2471\n",
      "epoch 347 / 1000, loss = 2.9551, valid loss:  3.2461\n",
      "epoch 348 / 1000, loss = 2.9534, valid loss:  3.2450\n",
      "epoch 349 / 1000, loss = 2.9581, valid loss:  3.2475\n",
      "epoch 350 / 1000, loss = 2.9530, valid loss:  3.2448\n",
      "epoch 351 / 1000, loss = 2.9547, valid loss:  3.2438\n",
      "epoch 352 / 1000, loss = 2.9556, valid loss:  3.2422\n",
      "epoch 353 / 1000, loss = 2.9573, valid loss:  3.2412\n",
      "epoch 354 / 1000, loss = 2.9592, valid loss:  3.2404\n",
      "epoch 355 / 1000, loss = 2.9598, valid loss:  3.2390\n",
      "epoch 356 / 1000, loss = 2.9600, valid loss:  3.2376\n",
      "epoch 357 / 1000, loss = 2.9573, valid loss:  3.2317\n",
      "epoch 358 / 1000, loss = 2.9598, valid loss:  3.2280\n",
      "epoch 359 / 1000, loss = 2.9658, valid loss:  3.2270\n",
      "epoch 360 / 1000, loss = 2.9637, valid loss:  3.2169\n",
      "epoch 361 / 1000, loss = 2.9613, valid loss:  3.2272\n",
      "epoch 362 / 1000, loss = 2.9596, valid loss:  3.2238\n",
      "epoch 363 / 1000, loss = 2.9589, valid loss:  3.2210\n",
      "epoch 364 / 1000, loss = 2.9570, valid loss:  3.2176\n",
      "epoch 365 / 1000, loss = 2.9564, valid loss:  3.2161\n",
      "epoch 366 / 1000, loss = 2.9561, valid loss:  3.2161\n",
      "epoch 367 / 1000, loss = 2.9555, valid loss:  3.2197\n",
      "epoch 368 / 1000, loss = 2.9559, valid loss:  3.2183\n",
      "epoch 369 / 1000, loss = 2.9577, valid loss:  3.2167\n",
      "epoch 370 / 1000, loss = 2.9565, valid loss:  3.2138\n",
      "epoch 371 / 1000, loss = 2.9565, valid loss:  3.2100\n",
      "epoch 372 / 1000, loss = 2.9571, valid loss:  3.2101\n",
      "epoch 373 / 1000, loss = 2.9573, valid loss:  3.2067\n",
      "epoch 374 / 1000, loss = 2.9560, valid loss:  3.2100\n",
      "epoch 375 / 1000, loss = 2.9560, valid loss:  3.2118\n",
      "epoch 376 / 1000, loss = 2.9552, valid loss:  3.2142\n",
      "epoch 377 / 1000, loss = 2.9545, valid loss:  3.2149\n",
      "epoch 378 / 1000, loss = 2.9541, valid loss:  3.2153\n",
      "epoch 379 / 1000, loss = 2.9538, valid loss:  3.2154\n",
      "epoch 380 / 1000, loss = 2.9537, valid loss:  3.2151\n",
      "epoch 381 / 1000, loss = 2.9537, valid loss:  3.2149\n",
      "epoch 382 / 1000, loss = 2.9536, valid loss:  3.2145\n",
      "epoch 383 / 1000, loss = 2.9534, valid loss:  3.2143\n",
      "epoch 384 / 1000, loss = 2.9556, valid loss:  3.2098\n",
      "epoch 385 / 1000, loss = 2.9578, valid loss:  3.2030\n",
      "epoch 386 / 1000, loss = 2.9534, valid loss:  3.2168\n",
      "epoch 387 / 1000, loss = 2.9568, valid loss:  3.2130\n",
      "epoch 388 / 1000, loss = 2.9555, valid loss:  3.2109\n",
      "epoch 389 / 1000, loss = 2.9588, valid loss:  3.2018\n",
      "epoch 390 / 1000, loss = 2.9588, valid loss:  3.2024\n",
      "epoch 391 / 1000, loss = 2.9590, valid loss:  3.2024\n",
      "epoch 392 / 1000, loss = 2.9589, valid loss:  3.2025\n",
      "epoch 393 / 1000, loss = 2.9582, valid loss:  3.2032\n",
      "epoch 394 / 1000, loss = 2.9578, valid loss:  3.2028\n",
      "epoch 395 / 1000, loss = 2.9577, valid loss:  3.2016\n",
      "epoch 396 / 1000, loss = 2.9579, valid loss:  3.2002\n",
      "epoch 397 / 1000, loss = 2.9581, valid loss:  3.1988\n",
      "epoch 398 / 1000, loss = 2.9582, valid loss:  3.1977\n",
      "epoch 399 / 1000, loss = 2.9581, valid loss:  3.1971\n",
      "epoch 400 / 1000, loss = 2.9580, valid loss:  3.1966\n",
      "epoch 401 / 1000, loss = 2.9579, valid loss:  3.1962\n",
      "epoch 402 / 1000, loss = 2.9578, valid loss:  3.1958\n",
      "epoch 403 / 1000, loss = 2.9576, valid loss:  3.1955\n",
      "epoch 404 / 1000, loss = 2.9575, valid loss:  3.1951\n",
      "epoch 405 / 1000, loss = 2.9573, valid loss:  3.1947\n",
      "epoch 406 / 1000, loss = 2.9572, valid loss:  3.1943\n",
      "epoch 407 / 1000, loss = 2.9569, valid loss:  3.1939\n",
      "epoch 408 / 1000, loss = 2.9566, valid loss:  3.1933\n",
      "epoch 409 / 1000, loss = 2.9559, valid loss:  3.1928\n",
      "epoch 410 / 1000, loss = 2.9560, valid loss:  3.1922\n",
      "epoch 411 / 1000, loss = 2.9574, valid loss:  3.1925\n",
      "epoch 412 / 1000, loss = 2.9547, valid loss:  3.1913\n",
      "epoch 413 / 1000, loss = 2.9542, valid loss:  3.1908\n",
      "epoch 414 / 1000, loss = 2.9530, valid loss:  3.1899\n",
      "epoch 415 / 1000, loss = 2.9508, valid loss:  3.1894\n",
      "epoch 416 / 1000, loss = 2.9574, valid loss:  3.1901\n",
      "epoch 417 / 1000, loss = 2.9502, valid loss:  3.1873\n",
      "epoch 418 / 1000, loss = 2.9497, valid loss:  3.1867\n",
      "epoch 419 / 1000, loss = 2.9502, valid loss:  3.1865\n",
      "epoch 420 / 1000, loss = 2.9486, valid loss:  3.1857\n",
      "epoch 421 / 1000, loss = 2.9457, valid loss:  3.1846\n",
      "epoch 422 / 1000, loss = 2.9446, valid loss:  3.1844\n",
      "epoch 423 / 1000, loss = 2.9444, valid loss:  3.1852\n",
      "epoch 424 / 1000, loss = 2.9467, valid loss:  3.1850\n",
      "epoch 425 / 1000, loss = 2.9466, valid loss:  3.1852\n",
      "epoch 426 / 1000, loss = 2.9452, valid loss:  3.1873\n",
      "epoch 427 / 1000, loss = 2.9539, valid loss:  3.1888\n",
      "epoch 428 / 1000, loss = 2.9466, valid loss:  3.1878\n",
      "epoch 429 / 1000, loss = 2.9456, valid loss:  3.1850\n",
      "epoch 430 / 1000, loss = 2.9543, valid loss:  3.1857\n",
      "epoch 431 / 1000, loss = 2.9524, valid loss:  3.1837\n",
      "epoch 432 / 1000, loss = 2.9535, valid loss:  3.1845\n",
      "epoch 433 / 1000, loss = 2.9531, valid loss:  3.1816\n",
      "epoch 434 / 1000, loss = 2.9424, valid loss:  3.1778\n",
      "epoch 435 / 1000, loss = 2.9513, valid loss:  3.1818\n",
      "epoch 436 / 1000, loss = 2.9520, valid loss:  3.1802\n",
      "epoch 437 / 1000, loss = 2.9517, valid loss:  3.1794\n",
      "epoch 438 / 1000, loss = 2.9513, valid loss:  3.1791\n",
      "epoch 439 / 1000, loss = 2.9509, valid loss:  3.1789\n",
      "epoch 440 / 1000, loss = 2.9457, valid loss:  3.1725\n",
      "epoch 441 / 1000, loss = 2.9504, valid loss:  3.1774\n",
      "epoch 442 / 1000, loss = 2.9493, valid loss:  3.1753\n",
      "epoch 443 / 1000, loss = 2.9489, valid loss:  3.1783\n",
      "epoch 444 / 1000, loss = 2.9446, valid loss:  3.1708\n",
      "epoch 445 / 1000, loss = 2.9483, valid loss:  3.1733\n",
      "epoch 446 / 1000, loss = 2.9480, valid loss:  3.1705\n",
      "epoch 447 / 1000, loss = 2.9481, valid loss:  3.1714\n",
      "epoch 448 / 1000, loss = 2.9494, valid loss:  3.1727\n",
      "epoch 449 / 1000, loss = 2.9495, valid loss:  3.1731\n",
      "epoch 450 / 1000, loss = 2.9500, valid loss:  3.1733\n",
      "epoch 451 / 1000, loss = 2.9492, valid loss:  3.1732\n",
      "epoch 452 / 1000, loss = 2.9500, valid loss:  3.1734\n",
      "epoch 453 / 1000, loss = 2.9492, valid loss:  3.1714\n",
      "epoch 454 / 1000, loss = 2.9493, valid loss:  3.1708\n",
      "epoch 455 / 1000, loss = 2.9394, valid loss:  3.1598\n",
      "epoch 456 / 1000, loss = 2.9424, valid loss:  3.1713\n",
      "epoch 457 / 1000, loss = 3.0005, valid loss:  3.1488\n",
      "epoch 458 / 1000, loss = 2.9891, valid loss:  3.1544\n",
      "epoch 459 / 1000, loss = 2.9843, valid loss:  3.1595\n",
      "epoch 460 / 1000, loss = 2.9776, valid loss:  3.1627\n",
      "epoch 461 / 1000, loss = 2.9717, valid loss:  3.1665\n",
      "epoch 462 / 1000, loss = 2.9669, valid loss:  3.1678\n",
      "epoch 463 / 1000, loss = 2.9630, valid loss:  3.1684\n",
      "epoch 464 / 1000, loss = 2.9610, valid loss:  3.1697\n",
      "epoch 465 / 1000, loss = 2.9598, valid loss:  3.1711\n",
      "epoch 466 / 1000, loss = 2.9588, valid loss:  3.1721\n",
      "epoch 467 / 1000, loss = 2.9578, valid loss:  3.1730\n",
      "epoch 468 / 1000, loss = 2.9568, valid loss:  3.1738\n",
      "epoch 469 / 1000, loss = 2.9556, valid loss:  3.1745\n",
      "epoch 470 / 1000, loss = 2.9544, valid loss:  3.1753\n",
      "epoch 471 / 1000, loss = 2.9534, valid loss:  3.1760\n",
      "epoch 472 / 1000, loss = 2.9528, valid loss:  3.1766\n",
      "epoch 473 / 1000, loss = 2.9520, valid loss:  3.1773\n",
      "epoch 474 / 1000, loss = 2.9510, valid loss:  3.1779\n",
      "epoch 475 / 1000, loss = 2.9497, valid loss:  3.1785\n",
      "epoch 476 / 1000, loss = 2.9484, valid loss:  3.1792\n",
      "epoch 477 / 1000, loss = 2.9468, valid loss:  3.1800\n",
      "epoch 478 / 1000, loss = 2.9455, valid loss:  3.1807\n",
      "epoch 479 / 1000, loss = 2.9441, valid loss:  3.1813\n",
      "epoch 480 / 1000, loss = 2.9426, valid loss:  3.1820\n",
      "epoch 481 / 1000, loss = 2.9413, valid loss:  3.1825\n",
      "epoch 482 / 1000, loss = 2.9404, valid loss:  3.1827\n",
      "epoch 483 / 1000, loss = 2.9399, valid loss:  3.1828\n",
      "epoch 484 / 1000, loss = 2.9393, valid loss:  3.1833\n",
      "epoch 485 / 1000, loss = 2.9405, valid loss:  3.1831\n",
      "epoch 486 / 1000, loss = 2.9392, valid loss:  3.1830\n",
      "epoch 487 / 1000, loss = 2.9430, valid loss:  3.1820\n",
      "epoch 488 / 1000, loss = 2.9450, valid loss:  3.1850\n",
      "epoch 489 / 1000, loss = 2.9465, valid loss:  3.1853\n",
      "epoch 490 / 1000, loss = 2.9465, valid loss:  3.1877\n",
      "epoch 491 / 1000, loss = 2.9493, valid loss:  3.1848\n",
      "epoch 492 / 1000, loss = 2.9491, valid loss:  3.1852\n",
      "epoch 493 / 1000, loss = 2.9486, valid loss:  3.1853\n",
      "epoch 494 / 1000, loss = 2.9477, valid loss:  3.1858\n",
      "epoch 495 / 1000, loss = 2.9463, valid loss:  3.1867\n",
      "epoch 496 / 1000, loss = 2.9449, valid loss:  3.1874\n",
      "epoch 497 / 1000, loss = 2.9441, valid loss:  3.1875\n",
      "epoch 498 / 1000, loss = 2.9441, valid loss:  3.1870\n",
      "epoch 499 / 1000, loss = 2.9475, valid loss:  3.1791\n",
      "epoch 500 / 1000, loss = 2.9394, valid loss:  3.1851\n",
      "epoch 501 / 1000, loss = 2.9496, valid loss:  3.1775\n",
      "epoch 502 / 1000, loss = 2.9408, valid loss:  3.1850\n",
      "epoch 503 / 1000, loss = 2.9461, valid loss:  3.1774\n",
      "epoch 504 / 1000, loss = 2.9419, valid loss:  3.1826\n",
      "epoch 505 / 1000, loss = 2.9401, valid loss:  3.1835\n",
      "epoch 506 / 1000, loss = 2.9504, valid loss:  3.1729\n",
      "epoch 507 / 1000, loss = 2.9389, valid loss:  3.1814\n",
      "epoch 508 / 1000, loss = 2.9396, valid loss:  3.1810\n",
      "epoch 509 / 1000, loss = 2.9459, valid loss:  3.1729\n",
      "epoch 510 / 1000, loss = 2.9398, valid loss:  3.1731\n",
      "epoch 511 / 1000, loss = 2.9447, valid loss:  3.1747\n",
      "epoch 512 / 1000, loss = 2.9459, valid loss:  3.1727\n",
      "epoch 513 / 1000, loss = 2.9455, valid loss:  3.1726\n",
      "epoch 514 / 1000, loss = 2.9471, valid loss:  3.1722\n",
      "epoch 515 / 1000, loss = 2.9355, valid loss:  3.1766\n",
      "epoch 516 / 1000, loss = 2.9360, valid loss:  3.1783\n",
      "epoch 517 / 1000, loss = 2.9421, valid loss:  3.1745\n",
      "epoch 518 / 1000, loss = 2.9408, valid loss:  3.1758\n",
      "epoch 519 / 1000, loss = 2.9402, valid loss:  3.1760\n",
      "epoch 520 / 1000, loss = 2.9366, valid loss:  3.1782\n",
      "epoch 521 / 1000, loss = 2.9365, valid loss:  3.1770\n",
      "epoch 522 / 1000, loss = 2.9350, valid loss:  3.1768\n",
      "epoch 523 / 1000, loss = 2.9333, valid loss:  3.1762\n",
      "epoch 524 / 1000, loss = 2.9323, valid loss:  3.1753\n",
      "epoch 525 / 1000, loss = 2.9317, valid loss:  3.1745\n",
      "epoch 526 / 1000, loss = 2.9312, valid loss:  3.1741\n",
      "epoch 527 / 1000, loss = 2.9309, valid loss:  3.1737\n",
      "epoch 528 / 1000, loss = 2.9299, valid loss:  3.1731\n",
      "epoch 529 / 1000, loss = 2.9288, valid loss:  3.1728\n",
      "epoch 530 / 1000, loss = 2.9288, valid loss:  3.1723\n",
      "epoch 531 / 1000, loss = 2.9302, valid loss:  3.1716\n",
      "epoch 532 / 1000, loss = 2.9341, valid loss:  3.1696\n",
      "epoch 533 / 1000, loss = 2.9369, valid loss:  3.1650\n",
      "epoch 534 / 1000, loss = 2.9367, valid loss:  3.1622\n",
      "epoch 535 / 1000, loss = 2.9362, valid loss:  3.1608\n",
      "epoch 536 / 1000, loss = 2.9354, valid loss:  3.1599\n",
      "epoch 537 / 1000, loss = 2.9347, valid loss:  3.1596\n",
      "epoch 538 / 1000, loss = 2.9361, valid loss:  3.1565\n",
      "epoch 539 / 1000, loss = 2.9362, valid loss:  3.1562\n",
      "epoch 540 / 1000, loss = 2.9334, valid loss:  3.1584\n",
      "epoch 541 / 1000, loss = 2.9337, valid loss:  3.1569\n",
      "epoch 542 / 1000, loss = 2.9285, valid loss:  3.1620\n",
      "epoch 543 / 1000, loss = 2.9337, valid loss:  3.1612\n",
      "epoch 544 / 1000, loss = 2.9315, valid loss:  3.1581\n",
      "epoch 545 / 1000, loss = 2.9302, valid loss:  3.1566\n",
      "epoch 546 / 1000, loss = 2.9296, valid loss:  3.1563\n",
      "epoch 547 / 1000, loss = 2.9259, valid loss:  3.1586\n",
      "epoch 548 / 1000, loss = 2.9299, valid loss:  3.1621\n",
      "epoch 549 / 1000, loss = 2.9304, valid loss:  3.1608\n",
      "epoch 550 / 1000, loss = 2.9299, valid loss:  3.1604\n",
      "epoch 551 / 1000, loss = 2.9299, valid loss:  3.1606\n",
      "epoch 552 / 1000, loss = 2.9305, valid loss:  3.1569\n",
      "epoch 553 / 1000, loss = 2.9307, valid loss:  3.1557\n",
      "epoch 554 / 1000, loss = 2.9297, valid loss:  3.1550\n",
      "epoch 555 / 1000, loss = 2.9295, valid loss:  3.1533\n",
      "epoch 556 / 1000, loss = 2.9295, valid loss:  3.1511\n",
      "epoch 557 / 1000, loss = 2.9290, valid loss:  3.1497\n",
      "epoch 558 / 1000, loss = 2.9279, valid loss:  3.1500\n",
      "epoch 559 / 1000, loss = 2.9271, valid loss:  3.1501\n",
      "epoch 560 / 1000, loss = 2.9267, valid loss:  3.1505\n",
      "epoch 561 / 1000, loss = 2.9262, valid loss:  3.1509\n",
      "epoch 562 / 1000, loss = 2.9251, valid loss:  3.1514\n",
      "epoch 563 / 1000, loss = 2.9243, valid loss:  3.1517\n",
      "epoch 564 / 1000, loss = 2.9241, valid loss:  3.1519\n",
      "epoch 565 / 1000, loss = 2.9245, valid loss:  3.1526\n",
      "epoch 566 / 1000, loss = 2.9239, valid loss:  3.1544\n",
      "epoch 567 / 1000, loss = 2.9224, valid loss:  3.1556\n",
      "epoch 568 / 1000, loss = 2.9266, valid loss:  3.1564\n",
      "epoch 569 / 1000, loss = 2.9255, valid loss:  3.1565\n",
      "epoch 570 / 1000, loss = 2.9253, valid loss:  3.1556\n",
      "epoch 571 / 1000, loss = 2.9283, valid loss:  3.1542\n",
      "epoch 572 / 1000, loss = 2.9284, valid loss:  3.1543\n",
      "epoch 573 / 1000, loss = 2.9268, valid loss:  3.1549\n",
      "epoch 574 / 1000, loss = 2.9257, valid loss:  3.1566\n",
      "epoch 575 / 1000, loss = 2.9254, valid loss:  3.1573\n",
      "epoch 576 / 1000, loss = 2.9251, valid loss:  3.1581\n",
      "epoch 577 / 1000, loss = 2.9242, valid loss:  3.1583\n",
      "epoch 578 / 1000, loss = 2.9225, valid loss:  3.1603\n",
      "epoch 579 / 1000, loss = 2.9231, valid loss:  3.1612\n",
      "epoch 580 / 1000, loss = 2.9228, valid loss:  3.1616\n",
      "epoch 581 / 1000, loss = 2.9228, valid loss:  3.1620\n",
      "epoch 582 / 1000, loss = 2.9232, valid loss:  3.1624\n",
      "epoch 583 / 1000, loss = 2.9239, valid loss:  3.1629\n",
      "epoch 584 / 1000, loss = 2.9233, valid loss:  3.1633\n",
      "epoch 585 / 1000, loss = 2.9243, valid loss:  3.1644\n",
      "epoch 586 / 1000, loss = 2.9240, valid loss:  3.1635\n",
      "epoch 587 / 1000, loss = 2.9236, valid loss:  3.1651\n",
      "epoch 588 / 1000, loss = 2.9224, valid loss:  3.1659\n",
      "epoch 589 / 1000, loss = 2.9224, valid loss:  3.1671\n",
      "epoch 590 / 1000, loss = 2.9201, valid loss:  3.1676\n",
      "epoch 591 / 1000, loss = 2.9204, valid loss:  3.1686\n",
      "epoch 592 / 1000, loss = 2.9211, valid loss:  3.1690\n",
      "epoch 593 / 1000, loss = 2.9206, valid loss:  3.1705\n",
      "epoch 594 / 1000, loss = 2.9224, valid loss:  3.1680\n",
      "epoch 595 / 1000, loss = 2.9209, valid loss:  3.1715\n",
      "epoch 596 / 1000, loss = 2.9211, valid loss:  3.1723\n",
      "epoch 597 / 1000, loss = 2.9207, valid loss:  3.1724\n",
      "epoch 598 / 1000, loss = 2.9195, valid loss:  3.1725\n",
      "epoch 599 / 1000, loss = 2.9183, valid loss:  3.1727\n",
      "epoch 600 / 1000, loss = 2.9172, valid loss:  3.1730\n",
      "epoch 601 / 1000, loss = 2.9142, valid loss:  3.1718\n",
      "epoch 602 / 1000, loss = 2.9113, valid loss:  3.1675\n",
      "epoch 603 / 1000, loss = 2.9127, valid loss:  3.1587\n",
      "epoch 604 / 1000, loss = 2.9121, valid loss:  3.1587\n",
      "epoch 605 / 1000, loss = 2.9098, valid loss:  3.1611\n",
      "epoch 606 / 1000, loss = 2.9090, valid loss:  3.1622\n",
      "epoch 607 / 1000, loss = 2.9075, valid loss:  3.1645\n",
      "epoch 608 / 1000, loss = 2.9111, valid loss:  3.1620\n",
      "epoch 609 / 1000, loss = 2.9082, valid loss:  3.1664\n",
      "epoch 610 / 1000, loss = 2.9119, valid loss:  3.1655\n",
      "epoch 611 / 1000, loss = 2.9044, valid loss:  3.1696\n",
      "epoch 612 / 1000, loss = 2.9083, valid loss:  3.1712\n",
      "epoch 613 / 1000, loss = 2.9084, valid loss:  3.1650\n",
      "epoch 614 / 1000, loss = 2.9074, valid loss:  3.1684\n",
      "epoch 615 / 1000, loss = 2.9076, valid loss:  3.1700\n",
      "epoch 616 / 1000, loss = 2.9068, valid loss:  3.1708\n",
      "epoch 617 / 1000, loss = 2.9056, valid loss:  3.1719\n",
      "epoch 618 / 1000, loss = 2.9056, valid loss:  3.1703\n",
      "epoch 619 / 1000, loss = 2.9055, valid loss:  3.1741\n",
      "epoch 620 / 1000, loss = 2.9050, valid loss:  3.1731\n",
      "epoch 621 / 1000, loss = 2.9049, valid loss:  3.1700\n",
      "epoch 622 / 1000, loss = 2.9043, valid loss:  3.1755\n",
      "epoch 623 / 1000, loss = 2.9031, valid loss:  3.1725\n",
      "epoch 624 / 1000, loss = 2.9034, valid loss:  3.1708\n",
      "epoch 625 / 1000, loss = 2.9038, valid loss:  3.1752\n",
      "epoch 626 / 1000, loss = 2.9024, valid loss:  3.1731\n",
      "epoch 627 / 1000, loss = 2.9006, valid loss:  3.1700\n",
      "epoch 628 / 1000, loss = 2.9006, valid loss:  3.1719\n",
      "epoch 629 / 1000, loss = 2.9020, valid loss:  3.1730\n",
      "epoch 630 / 1000, loss = 2.9010, valid loss:  3.1744\n",
      "epoch 631 / 1000, loss = 2.8987, valid loss:  3.1781\n",
      "epoch 632 / 1000, loss = 2.8953, valid loss:  3.1792\n",
      "epoch 633 / 1000, loss = 2.8947, valid loss:  3.1778\n",
      "epoch 634 / 1000, loss = 2.8945, valid loss:  3.1804\n",
      "epoch 635 / 1000, loss = 2.8931, valid loss:  3.1811\n",
      "epoch 636 / 1000, loss = 2.8928, valid loss:  3.1812\n",
      "epoch 637 / 1000, loss = 2.8931, valid loss:  3.1844\n",
      "epoch 638 / 1000, loss = 2.8938, valid loss:  3.1867\n",
      "epoch 639 / 1000, loss = 2.8934, valid loss:  3.1864\n",
      "epoch 640 / 1000, loss = 2.8905, valid loss:  3.1833\n",
      "epoch 641 / 1000, loss = 2.8909, valid loss:  3.1827\n",
      "epoch 642 / 1000, loss = 2.8924, valid loss:  3.1878\n",
      "epoch 643 / 1000, loss = 2.8932, valid loss:  3.1897\n",
      "epoch 644 / 1000, loss = 2.8938, valid loss:  3.1908\n",
      "epoch 645 / 1000, loss = 2.8940, valid loss:  3.1920\n",
      "epoch 646 / 1000, loss = 2.8928, valid loss:  3.1914\n",
      "epoch 647 / 1000, loss = 2.8895, valid loss:  3.1862\n",
      "epoch 648 / 1000, loss = 2.8891, valid loss:  3.1873\n",
      "epoch 649 / 1000, loss = 2.8906, valid loss:  3.1900\n",
      "epoch 650 / 1000, loss = 2.8913, valid loss:  3.1906\n",
      "epoch 651 / 1000, loss = 2.8924, valid loss:  3.1918\n",
      "epoch 652 / 1000, loss = 2.8929, valid loss:  3.1935\n",
      "epoch 653 / 1000, loss = 2.8936, valid loss:  3.1950\n",
      "epoch 654 / 1000, loss = 2.8937, valid loss:  3.1950\n",
      "epoch 655 / 1000, loss = 2.8943, valid loss:  3.1957\n",
      "epoch 656 / 1000, loss = 2.8950, valid loss:  3.1973\n",
      "epoch 657 / 1000, loss = 2.8959, valid loss:  3.1953\n",
      "epoch 658 / 1000, loss = 2.8962, valid loss:  3.1956\n",
      "epoch 659 / 1000, loss = 2.8962, valid loss:  3.1954\n",
      "epoch 660 / 1000, loss = 2.8950, valid loss:  3.1982\n",
      "epoch 661 / 1000, loss = 2.8964, valid loss:  3.1992\n",
      "epoch 662 / 1000, loss = 2.8975, valid loss:  3.1969\n",
      "epoch 663 / 1000, loss = 2.8968, valid loss:  3.1973\n",
      "epoch 664 / 1000, loss = 2.8957, valid loss:  3.1976\n",
      "epoch 665 / 1000, loss = 2.8972, valid loss:  3.1983\n",
      "epoch 666 / 1000, loss = 2.8959, valid loss:  3.1975\n",
      "epoch 667 / 1000, loss = 2.8963, valid loss:  3.1988\n",
      "epoch 668 / 1000, loss = 2.8984, valid loss:  3.1990\n",
      "epoch 669 / 1000, loss = 2.8976, valid loss:  3.1989\n",
      "epoch 670 / 1000, loss = 2.8962, valid loss:  3.1987\n",
      "epoch 671 / 1000, loss = 2.9003, valid loss:  3.1986\n",
      "epoch 672 / 1000, loss = 2.9008, valid loss:  3.1981\n",
      "epoch 673 / 1000, loss = 2.9020, valid loss:  3.1982\n",
      "epoch 674 / 1000, loss = 2.9024, valid loss:  3.1969\n",
      "epoch 675 / 1000, loss = 2.9013, valid loss:  3.1965\n",
      "epoch 676 / 1000, loss = 2.9049, valid loss:  3.1961\n",
      "epoch 677 / 1000, loss = 2.9056, valid loss:  3.1947\n",
      "epoch 678 / 1000, loss = 2.9052, valid loss:  3.1940\n",
      "epoch 679 / 1000, loss = 2.9033, valid loss:  3.1927\n",
      "epoch 680 / 1000, loss = 2.9026, valid loss:  3.1920\n",
      "epoch 681 / 1000, loss = 2.9042, valid loss:  3.1929\n",
      "epoch 682 / 1000, loss = 2.9056, valid loss:  3.1946\n",
      "epoch 683 / 1000, loss = 2.9075, valid loss:  3.1946\n",
      "epoch 684 / 1000, loss = 2.9091, valid loss:  3.1935\n",
      "epoch 685 / 1000, loss = 2.9089, valid loss:  3.1936\n",
      "epoch 686 / 1000, loss = 2.9090, valid loss:  3.1949\n",
      "epoch 687 / 1000, loss = 2.9090, valid loss:  3.1938\n",
      "epoch 688 / 1000, loss = 2.9100, valid loss:  3.1948\n",
      "epoch 689 / 1000, loss = 2.9099, valid loss:  3.1925\n",
      "epoch 690 / 1000, loss = 2.9097, valid loss:  3.1904\n",
      "epoch 691 / 1000, loss = 2.9098, valid loss:  3.1894\n",
      "epoch 692 / 1000, loss = 2.9098, valid loss:  3.1860\n",
      "epoch 693 / 1000, loss = 2.9098, valid loss:  3.1828\n",
      "epoch 694 / 1000, loss = 2.9101, valid loss:  3.1784\n",
      "epoch 695 / 1000, loss = 2.9088, valid loss:  3.1732\n",
      "epoch 696 / 1000, loss = 2.9059, valid loss:  3.1677\n",
      "epoch 697 / 1000, loss = 2.9033, valid loss:  3.1626\n",
      "epoch 698 / 1000, loss = 2.9030, valid loss:  3.1612\n",
      "epoch 699 / 1000, loss = 2.9015, valid loss:  3.1568\n",
      "epoch 700 / 1000, loss = 2.9021, valid loss:  3.1577\n",
      "epoch 701 / 1000, loss = 2.9001, valid loss:  3.1540\n",
      "epoch 702 / 1000, loss = 2.9003, valid loss:  3.1548\n",
      "epoch 703 / 1000, loss = 2.9011, valid loss:  3.1556\n",
      "epoch 704 / 1000, loss = 2.9006, valid loss:  3.1545\n",
      "epoch 705 / 1000, loss = 2.9005, valid loss:  3.1538\n",
      "epoch 706 / 1000, loss = 2.9013, valid loss:  3.1540\n",
      "epoch 707 / 1000, loss = 2.9009, valid loss:  3.1533\n",
      "epoch 708 / 1000, loss = 2.8999, valid loss:  3.1523\n",
      "epoch 709 / 1000, loss = 2.8990, valid loss:  3.1517\n",
      "epoch 710 / 1000, loss = 2.8983, valid loss:  3.1511\n",
      "epoch 711 / 1000, loss = 2.8976, valid loss:  3.1505\n",
      "epoch 712 / 1000, loss = 2.8972, valid loss:  3.1498\n",
      "epoch 713 / 1000, loss = 2.8971, valid loss:  3.1489\n",
      "epoch 714 / 1000, loss = 2.8974, valid loss:  3.1473\n",
      "epoch 715 / 1000, loss = 2.8973, valid loss:  3.1457\n",
      "epoch 716 / 1000, loss = 2.8959, valid loss:  3.1448\n",
      "epoch 717 / 1000, loss = 2.8949, valid loss:  3.1442\n",
      "epoch 718 / 1000, loss = 2.8940, valid loss:  3.1438\n",
      "epoch 719 / 1000, loss = 2.8932, valid loss:  3.1435\n",
      "epoch 720 / 1000, loss = 2.8926, valid loss:  3.1431\n",
      "epoch 721 / 1000, loss = 2.8922, valid loss:  3.1427\n",
      "epoch 722 / 1000, loss = 2.8920, valid loss:  3.1422\n",
      "epoch 723 / 1000, loss = 2.8921, valid loss:  3.1417\n",
      "epoch 724 / 1000, loss = 2.8923, valid loss:  3.1415\n",
      "epoch 725 / 1000, loss = 2.8933, valid loss:  3.1416\n",
      "epoch 726 / 1000, loss = 2.8941, valid loss:  3.1425\n",
      "epoch 727 / 1000, loss = 2.8938, valid loss:  3.1433\n",
      "epoch 728 / 1000, loss = 2.8937, valid loss:  3.1437\n",
      "epoch 729 / 1000, loss = 2.8935, valid loss:  3.1437\n",
      "epoch 730 / 1000, loss = 2.8936, valid loss:  3.1438\n",
      "epoch 731 / 1000, loss = 2.8935, valid loss:  3.1438\n",
      "epoch 732 / 1000, loss = 2.8937, valid loss:  3.1440\n",
      "epoch 733 / 1000, loss = 2.8937, valid loss:  3.1439\n",
      "epoch 734 / 1000, loss = 2.8938, valid loss:  3.1441\n",
      "epoch 735 / 1000, loss = 2.8940, valid loss:  3.1442\n",
      "epoch 736 / 1000, loss = 2.8941, valid loss:  3.1438\n",
      "epoch 737 / 1000, loss = 2.8938, valid loss:  3.1435\n",
      "epoch 738 / 1000, loss = 2.8934, valid loss:  3.1434\n",
      "epoch 739 / 1000, loss = 2.8925, valid loss:  3.1433\n",
      "epoch 740 / 1000, loss = 2.8929, valid loss:  3.1417\n",
      "epoch 741 / 1000, loss = 2.8929, valid loss:  3.1400\n",
      "epoch 742 / 1000, loss = 2.8929, valid loss:  3.1385\n",
      "epoch 743 / 1000, loss = 2.8928, valid loss:  3.1373\n",
      "epoch 744 / 1000, loss = 2.8927, valid loss:  3.1363\n",
      "epoch 745 / 1000, loss = 2.8926, valid loss:  3.1353\n",
      "epoch 746 / 1000, loss = 2.8925, valid loss:  3.1342\n",
      "epoch 747 / 1000, loss = 2.8931, valid loss:  3.1340\n",
      "epoch 748 / 1000, loss = 2.8938, valid loss:  3.1334\n",
      "epoch 749 / 1000, loss = 2.8943, valid loss:  3.1333\n",
      "epoch 750 / 1000, loss = 2.8946, valid loss:  3.1334\n",
      "epoch 751 / 1000, loss = 2.8946, valid loss:  3.1334\n",
      "epoch 752 / 1000, loss = 2.8949, valid loss:  3.1323\n",
      "epoch 753 / 1000, loss = 2.8955, valid loss:  3.1331\n",
      "epoch 754 / 1000, loss = 2.8941, valid loss:  3.1332\n",
      "epoch 755 / 1000, loss = 2.8927, valid loss:  3.1327\n",
      "epoch 756 / 1000, loss = 2.8922, valid loss:  3.1333\n",
      "epoch 757 / 1000, loss = 2.8903, valid loss:  3.1322\n",
      "epoch 758 / 1000, loss = 2.8887, valid loss:  3.1315\n",
      "epoch 759 / 1000, loss = 2.8888, valid loss:  3.1336\n",
      "epoch 760 / 1000, loss = 2.8883, valid loss:  3.1323\n",
      "epoch 761 / 1000, loss = 2.8868, valid loss:  3.1320\n",
      "epoch 762 / 1000, loss = 2.8856, valid loss:  3.1315\n",
      "epoch 763 / 1000, loss = 2.8859, valid loss:  3.1304\n",
      "epoch 764 / 1000, loss = 2.8866, valid loss:  3.1299\n",
      "epoch 765 / 1000, loss = 2.8873, valid loss:  3.1302\n",
      "epoch 766 / 1000, loss = 2.8884, valid loss:  3.1306\n",
      "epoch 767 / 1000, loss = 2.8895, valid loss:  3.1311\n",
      "epoch 768 / 1000, loss = 2.8898, valid loss:  3.1311\n",
      "epoch 769 / 1000, loss = 2.8896, valid loss:  3.1306\n",
      "epoch 770 / 1000, loss = 2.8907, valid loss:  3.1310\n",
      "epoch 771 / 1000, loss = 2.8909, valid loss:  3.1311\n",
      "epoch 772 / 1000, loss = 2.8908, valid loss:  3.1311\n",
      "epoch 773 / 1000, loss = 2.8914, valid loss:  3.1321\n",
      "epoch 774 / 1000, loss = 2.8931, valid loss:  3.1336\n",
      "epoch 775 / 1000, loss = 2.8916, valid loss:  3.1317\n",
      "epoch 776 / 1000, loss = 2.8930, valid loss:  3.1325\n",
      "epoch 777 / 1000, loss = 2.8908, valid loss:  3.1303\n",
      "epoch 778 / 1000, loss = 2.8891, valid loss:  3.1292\n",
      "epoch 779 / 1000, loss = 2.8883, valid loss:  3.1286\n",
      "epoch 780 / 1000, loss = 2.8866, valid loss:  3.1281\n",
      "epoch 781 / 1000, loss = 2.8853, valid loss:  3.1277\n",
      "epoch 782 / 1000, loss = 2.8837, valid loss:  3.1276\n",
      "epoch 783 / 1000, loss = 2.8821, valid loss:  3.1275\n",
      "epoch 784 / 1000, loss = 2.8812, valid loss:  3.1271\n",
      "epoch 785 / 1000, loss = 2.8800, valid loss:  3.1271\n",
      "epoch 786 / 1000, loss = 2.8793, valid loss:  3.1266\n",
      "epoch 787 / 1000, loss = 2.8778, valid loss:  3.1264\n",
      "epoch 788 / 1000, loss = 2.8764, valid loss:  3.1262\n",
      "epoch 789 / 1000, loss = 2.8755, valid loss:  3.1261\n",
      "epoch 790 / 1000, loss = 2.8747, valid loss:  3.1260\n",
      "epoch 791 / 1000, loss = 2.8738, valid loss:  3.1255\n",
      "epoch 792 / 1000, loss = 2.8725, valid loss:  3.1262\n",
      "epoch 793 / 1000, loss = 2.8714, valid loss:  3.1272\n",
      "epoch 794 / 1000, loss = 2.8701, valid loss:  3.1290\n",
      "epoch 795 / 1000, loss = 2.8689, valid loss:  3.1303\n",
      "epoch 796 / 1000, loss = 2.8705, valid loss:  3.1329\n",
      "epoch 797 / 1000, loss = 2.8703, valid loss:  3.1327\n",
      "epoch 798 / 1000, loss = 2.8689, valid loss:  3.1319\n",
      "epoch 799 / 1000, loss = 2.8657, valid loss:  3.1337\n",
      "epoch 800 / 1000, loss = 2.8646, valid loss:  3.1325\n",
      "epoch 801 / 1000, loss = 2.8636, valid loss:  3.1327\n",
      "epoch 802 / 1000, loss = 2.8624, valid loss:  3.1320\n",
      "epoch 803 / 1000, loss = 2.8615, valid loss:  3.1313\n",
      "epoch 804 / 1000, loss = 2.8618, valid loss:  3.1301\n",
      "epoch 805 / 1000, loss = 2.8606, valid loss:  3.1292\n",
      "epoch 806 / 1000, loss = 2.8604, valid loss:  3.1289\n",
      "epoch 807 / 1000, loss = 2.8597, valid loss:  3.1290\n",
      "epoch 808 / 1000, loss = 2.8598, valid loss:  3.1287\n",
      "epoch 809 / 1000, loss = 2.8599, valid loss:  3.1277\n",
      "epoch 810 / 1000, loss = 2.8600, valid loss:  3.1272\n",
      "epoch 811 / 1000, loss = 2.8591, valid loss:  3.1265\n",
      "epoch 812 / 1000, loss = 2.8582, valid loss:  3.1262\n",
      "epoch 813 / 1000, loss = 2.8564, valid loss:  3.1258\n",
      "epoch 814 / 1000, loss = 2.8555, valid loss:  3.1257\n",
      "epoch 815 / 1000, loss = 2.8546, valid loss:  3.1255\n",
      "epoch 816 / 1000, loss = 2.8538, valid loss:  3.1256\n",
      "epoch 817 / 1000, loss = 2.8534, valid loss:  3.1260\n",
      "epoch 818 / 1000, loss = 2.8529, valid loss:  3.1268\n",
      "epoch 819 / 1000, loss = 2.8521, valid loss:  3.1276\n",
      "epoch 820 / 1000, loss = 2.8514, valid loss:  3.1282\n",
      "epoch 821 / 1000, loss = 2.8509, valid loss:  3.1285\n",
      "epoch 822 / 1000, loss = 2.8508, valid loss:  3.1284\n",
      "epoch 823 / 1000, loss = 2.8511, valid loss:  3.1281\n",
      "epoch 824 / 1000, loss = 2.8516, valid loss:  3.1283\n",
      "epoch 825 / 1000, loss = 2.8520, valid loss:  3.1283\n",
      "epoch 826 / 1000, loss = 2.8525, valid loss:  3.1275\n",
      "epoch 827 / 1000, loss = 2.8535, valid loss:  3.1278\n",
      "epoch 828 / 1000, loss = 2.8539, valid loss:  3.1260\n",
      "epoch 829 / 1000, loss = 2.8548, valid loss:  3.1253\n",
      "epoch 830 / 1000, loss = 2.8557, valid loss:  3.1253\n",
      "epoch 831 / 1000, loss = 2.8576, valid loss:  3.1251\n",
      "epoch 832 / 1000, loss = 2.8573, valid loss:  3.1266\n",
      "epoch 833 / 1000, loss = 2.8572, valid loss:  3.1249\n",
      "epoch 834 / 1000, loss = 2.8572, valid loss:  3.1253\n",
      "epoch 835 / 1000, loss = 2.8569, valid loss:  3.1308\n",
      "epoch 836 / 1000, loss = 2.8577, valid loss:  3.1346\n",
      "epoch 837 / 1000, loss = 2.8575, valid loss:  3.1316\n",
      "epoch 838 / 1000, loss = 2.8568, valid loss:  3.1300\n",
      "epoch 839 / 1000, loss = 2.8562, valid loss:  3.1303\n",
      "epoch 840 / 1000, loss = 2.8564, valid loss:  3.1320\n",
      "epoch 841 / 1000, loss = 2.8578, valid loss:  3.1339\n",
      "epoch 842 / 1000, loss = 2.8594, valid loss:  3.1353\n",
      "epoch 843 / 1000, loss = 2.8607, valid loss:  3.1366\n",
      "epoch 844 / 1000, loss = 2.8615, valid loss:  3.1373\n",
      "epoch 845 / 1000, loss = 2.8618, valid loss:  3.1372\n",
      "epoch 846 / 1000, loss = 2.8615, valid loss:  3.1369\n",
      "epoch 847 / 1000, loss = 2.8607, valid loss:  3.1371\n",
      "epoch 848 / 1000, loss = 2.8595, valid loss:  3.1386\n",
      "epoch 849 / 1000, loss = 2.8577, valid loss:  3.1411\n",
      "epoch 850 / 1000, loss = 2.8556, valid loss:  3.1427\n",
      "epoch 851 / 1000, loss = 2.8531, valid loss:  3.1426\n",
      "epoch 852 / 1000, loss = 2.8503, valid loss:  3.1417\n",
      "epoch 853 / 1000, loss = 2.8469, valid loss:  3.1394\n",
      "epoch 854 / 1000, loss = 2.8442, valid loss:  3.1360\n",
      "epoch 855 / 1000, loss = 2.8424, valid loss:  3.1335\n",
      "epoch 856 / 1000, loss = 2.8407, valid loss:  3.1319\n",
      "epoch 857 / 1000, loss = 2.8392, valid loss:  3.1308\n",
      "epoch 858 / 1000, loss = 2.8378, valid loss:  3.1302\n",
      "epoch 859 / 1000, loss = 2.8366, valid loss:  3.1303\n",
      "epoch 860 / 1000, loss = 2.8353, valid loss:  3.1314\n",
      "epoch 861 / 1000, loss = 2.8339, valid loss:  3.1331\n",
      "epoch 862 / 1000, loss = 2.8331, valid loss:  3.1358\n",
      "epoch 863 / 1000, loss = 2.8329, valid loss:  3.1394\n",
      "epoch 864 / 1000, loss = 2.8331, valid loss:  3.1435\n",
      "epoch 865 / 1000, loss = 2.8334, valid loss:  3.1471\n",
      "epoch 866 / 1000, loss = 2.8336, valid loss:  3.1497\n",
      "epoch 867 / 1000, loss = 2.8338, valid loss:  3.1521\n",
      "epoch 868 / 1000, loss = 2.8341, valid loss:  3.1545\n",
      "epoch 869 / 1000, loss = 2.8347, valid loss:  3.1568\n",
      "epoch 870 / 1000, loss = 2.8350, valid loss:  3.1598\n",
      "epoch 871 / 1000, loss = 2.8353, valid loss:  3.1624\n",
      "epoch 872 / 1000, loss = 2.8356, valid loss:  3.1648\n",
      "epoch 873 / 1000, loss = 2.8358, valid loss:  3.1669\n",
      "epoch 874 / 1000, loss = 2.8359, valid loss:  3.1682\n",
      "epoch 875 / 1000, loss = 2.8360, valid loss:  3.1692\n",
      "epoch 876 / 1000, loss = 2.8351, valid loss:  3.1503\n",
      "epoch 877 / 1000, loss = 2.8363, valid loss:  3.1726\n",
      "epoch 878 / 1000, loss = 2.8370, valid loss:  3.1707\n",
      "epoch 879 / 1000, loss = 2.8366, valid loss:  3.1690\n",
      "epoch 880 / 1000, loss = 2.8364, valid loss:  3.1696\n",
      "epoch 881 / 1000, loss = 2.8344, valid loss:  3.1555\n",
      "epoch 882 / 1000, loss = 2.8358, valid loss:  3.1714\n",
      "epoch 883 / 1000, loss = 2.8361, valid loss:  3.1734\n",
      "epoch 884 / 1000, loss = 2.8355, valid loss:  3.1731\n",
      "epoch 885 / 1000, loss = 2.8340, valid loss:  3.1643\n",
      "epoch 886 / 1000, loss = 2.8336, valid loss:  3.1654\n",
      "epoch 887 / 1000, loss = 2.8331, valid loss:  3.1616\n",
      "epoch 888 / 1000, loss = 2.8323, valid loss:  3.1525\n",
      "epoch 889 / 1000, loss = 2.8321, valid loss:  3.1441\n",
      "epoch 890 / 1000, loss = 2.8313, valid loss:  3.1373\n",
      "epoch 891 / 1000, loss = 2.8330, valid loss:  3.1484\n",
      "epoch 892 / 1000, loss = 2.8493, valid loss:  3.1552\n",
      "epoch 893 / 1000, loss = 2.8367, valid loss:  3.1703\n",
      "epoch 894 / 1000, loss = 2.8684, valid loss:  3.1683\n",
      "epoch 895 / 1000, loss = 2.8298, valid loss:  3.1409\n",
      "epoch 896 / 1000, loss = 2.8355, valid loss:  3.1527\n",
      "epoch 897 / 1000, loss = 2.8524, valid loss:  3.1555\n",
      "epoch 898 / 1000, loss = 2.8596, valid loss:  3.1558\n",
      "epoch 899 / 1000, loss = 2.8555, valid loss:  3.1563\n",
      "epoch 900 / 1000, loss = 2.8510, valid loss:  3.1528\n",
      "epoch 901 / 1000, loss = 2.8473, valid loss:  3.1503\n",
      "epoch 902 / 1000, loss = 2.8603, valid loss:  3.1546\n",
      "epoch 903 / 1000, loss = 2.8503, valid loss:  3.1511\n",
      "epoch 904 / 1000, loss = 2.8413, valid loss:  3.1821\n",
      "epoch 905 / 1000, loss = 2.8425, valid loss:  3.1753\n",
      "epoch 906 / 1000, loss = 2.8436, valid loss:  3.1782\n",
      "epoch 907 / 1000, loss = 2.8428, valid loss:  3.1734\n",
      "epoch 908 / 1000, loss = 2.8418, valid loss:  3.1700\n",
      "epoch 909 / 1000, loss = 2.8403, valid loss:  3.1669\n",
      "epoch 910 / 1000, loss = 2.8388, valid loss:  3.1640\n",
      "epoch 911 / 1000, loss = 2.8380, valid loss:  3.1623\n",
      "epoch 912 / 1000, loss = 2.8374, valid loss:  3.1609\n",
      "epoch 913 / 1000, loss = 2.8367, valid loss:  3.1593\n",
      "epoch 914 / 1000, loss = 2.8361, valid loss:  3.1578\n",
      "epoch 915 / 1000, loss = 2.8355, valid loss:  3.1563\n",
      "epoch 916 / 1000, loss = 2.8349, valid loss:  3.1550\n",
      "epoch 917 / 1000, loss = 2.8344, valid loss:  3.1537\n",
      "epoch 918 / 1000, loss = 2.8338, valid loss:  3.1524\n",
      "epoch 919 / 1000, loss = 2.8331, valid loss:  3.1510\n",
      "epoch 920 / 1000, loss = 2.8324, valid loss:  3.1496\n",
      "epoch 921 / 1000, loss = 2.8317, valid loss:  3.1482\n",
      "epoch 922 / 1000, loss = 2.8311, valid loss:  3.1469\n",
      "epoch 923 / 1000, loss = 2.8306, valid loss:  3.1457\n",
      "epoch 924 / 1000, loss = 2.8302, valid loss:  3.1446\n",
      "epoch 925 / 1000, loss = 2.8302, valid loss:  3.1444\n",
      "epoch 926 / 1000, loss = 2.8364, valid loss:  3.1571\n",
      "epoch 927 / 1000, loss = 2.8329, valid loss:  3.1453\n",
      "epoch 928 / 1000, loss = 2.8398, valid loss:  3.1682\n",
      "epoch 929 / 1000, loss = 2.8367, valid loss:  3.1450\n",
      "epoch 930 / 1000, loss = 2.8382, valid loss:  3.1456\n",
      "epoch 931 / 1000, loss = 2.8424, valid loss:  3.1596\n",
      "epoch 932 / 1000, loss = 2.8446, valid loss:  3.1649\n",
      "epoch 933 / 1000, loss = 2.8396, valid loss:  3.1419\n",
      "epoch 934 / 1000, loss = 2.8443, valid loss:  3.1589\n",
      "epoch 935 / 1000, loss = 2.8410, valid loss:  3.1501\n",
      "epoch 936 / 1000, loss = 2.8420, valid loss:  3.1535\n",
      "epoch 937 / 1000, loss = 2.8425, valid loss:  3.1558\n",
      "epoch 938 / 1000, loss = 2.8390, valid loss:  3.1517\n",
      "epoch 939 / 1000, loss = 2.8416, valid loss:  3.1558\n",
      "epoch 940 / 1000, loss = 2.8431, valid loss:  3.1590\n",
      "epoch 941 / 1000, loss = 2.8432, valid loss:  3.1559\n",
      "epoch 942 / 1000, loss = 2.8450, valid loss:  3.1552\n",
      "epoch 943 / 1000, loss = 2.8466, valid loss:  3.1581\n",
      "epoch 944 / 1000, loss = 2.8473, valid loss:  3.1567\n",
      "epoch 945 / 1000, loss = 2.8472, valid loss:  3.1646\n",
      "epoch 946 / 1000, loss = 2.8475, valid loss:  3.1665\n",
      "epoch 947 / 1000, loss = 2.8478, valid loss:  3.1700\n",
      "epoch 948 / 1000, loss = 2.8482, valid loss:  3.1728\n",
      "epoch 949 / 1000, loss = 2.8488, valid loss:  3.1781\n",
      "epoch 950 / 1000, loss = 2.8475, valid loss:  3.1737\n",
      "epoch 951 / 1000, loss = 2.8464, valid loss:  3.1578\n",
      "epoch 952 / 1000, loss = 2.8473, valid loss:  3.1765\n",
      "epoch 953 / 1000, loss = 2.8474, valid loss:  3.1839\n",
      "epoch 954 / 1000, loss = 2.8359, valid loss:  3.1550\n",
      "epoch 955 / 1000, loss = 2.8374, valid loss:  3.1594\n",
      "epoch 956 / 1000, loss = 2.8401, valid loss:  3.1815\n",
      "epoch 957 / 1000, loss = 2.8378, valid loss:  3.1893\n",
      "epoch 958 / 1000, loss = 2.8366, valid loss:  3.1973\n",
      "epoch 959 / 1000, loss = 2.8387, valid loss:  3.1993\n",
      "epoch 960 / 1000, loss = 2.8401, valid loss:  3.1999\n",
      "epoch 961 / 1000, loss = 2.8325, valid loss:  3.1716\n",
      "epoch 962 / 1000, loss = 2.8332, valid loss:  3.1652\n",
      "epoch 963 / 1000, loss = 2.8310, valid loss:  3.1448\n",
      "epoch 964 / 1000, loss = 2.8316, valid loss:  3.1508\n",
      "epoch 965 / 1000, loss = 2.8400, valid loss:  3.1936\n",
      "epoch 966 / 1000, loss = 2.8409, valid loss:  3.1961\n",
      "epoch 967 / 1000, loss = 2.8414, valid loss:  3.1963\n",
      "epoch 968 / 1000, loss = 2.8421, valid loss:  3.1945\n",
      "epoch 969 / 1000, loss = 2.8413, valid loss:  3.1914\n",
      "epoch 970 / 1000, loss = 2.8328, valid loss:  3.1483\n",
      "epoch 971 / 1000, loss = 2.8332, valid loss:  3.1498\n",
      "epoch 972 / 1000, loss = 2.8309, valid loss:  3.1401\n",
      "epoch 973 / 1000, loss = 2.8307, valid loss:  3.1436\n",
      "epoch 974 / 1000, loss = 2.8363, valid loss:  3.1625\n",
      "epoch 975 / 1000, loss = 2.8312, valid loss:  3.1420\n",
      "epoch 976 / 1000, loss = 2.8372, valid loss:  3.1565\n",
      "epoch 977 / 1000, loss = 2.8335, valid loss:  3.1499\n",
      "epoch 978 / 1000, loss = 2.8360, valid loss:  3.1528\n",
      "epoch 979 / 1000, loss = 2.8343, valid loss:  3.1461\n",
      "epoch 980 / 1000, loss = 2.8354, valid loss:  3.1466\n",
      "epoch 981 / 1000, loss = 2.8348, valid loss:  3.1432\n",
      "epoch 982 / 1000, loss = 2.8355, valid loss:  3.1434\n",
      "epoch 983 / 1000, loss = 2.8356, valid loss:  3.1420\n",
      "epoch 984 / 1000, loss = 2.8360, valid loss:  3.1414\n",
      "epoch 985 / 1000, loss = 2.8363, valid loss:  3.1404\n",
      "epoch 986 / 1000, loss = 2.8366, valid loss:  3.1393\n",
      "epoch 987 / 1000, loss = 2.8369, valid loss:  3.1381\n",
      "epoch 988 / 1000, loss = 2.8371, valid loss:  3.1367\n",
      "epoch 989 / 1000, loss = 2.8374, valid loss:  3.1352\n",
      "epoch 990 / 1000, loss = 2.8376, valid loss:  3.1337\n",
      "epoch 991 / 1000, loss = 2.8378, valid loss:  3.1321\n",
      "epoch 992 / 1000, loss = 2.8380, valid loss:  3.1303\n",
      "epoch 993 / 1000, loss = 2.8381, valid loss:  3.1286\n",
      "epoch 994 / 1000, loss = 2.8382, valid loss:  3.1267\n",
      "epoch 995 / 1000, loss = 2.8382, valid loss:  3.1248\n",
      "epoch 996 / 1000, loss = 2.8381, valid loss:  3.1229\n",
      "epoch 997 / 1000, loss = 2.8380, valid loss:  3.1211\n",
      "epoch 998 / 1000, loss = 2.8379, valid loss:  3.1192\n",
      "epoch 999 / 1000, loss = 2.8378, valid loss:  3.1175\n",
      "epoch 1000 / 1000, loss = 2.8377, valid loss:  3.1158\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Thermal Accept Accuracy</td><td>▁▃▃▃▃▃▄▄▅▆▆▆▆▇▆▆▇▇▆▆▇▇▇▇▇▇▇▇▇▇▇▇████████</td></tr><tr><td>Thermal Accept Loss</td><td>█▇▆▆▆▆▅▅▅▄▄▄▄▃▃▃▃▃▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>Thermal Accept Valid Accuracy</td><td>▁▃▃▃▃▃▃▄▆▆▆▆▆▇▇▇▇▇▆▆▇▇███▇███████████▇█▇</td></tr><tr><td>Thermal Accept Valid Loss</td><td>█▇▆▆▅▅▅▄▄▄▃▃▃▃▃▂▃▂▃▃▂▃▂▂▂▂▂▂▂▂▁▂▂▁▁▁▂▂▂▂</td></tr><tr><td>Thermal Comfort Accuracy</td><td>▅▅▃▁▁▁▃▄▅▅▆▅▆▅▆▆▆▇▇▇▇▇▅▇▆▇██▇▇▆█████▇▇▇█</td></tr><tr><td>Thermal Comfort Loss</td><td>█▅▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Thermal Comfort Valid Accuracy</td><td>██▃▁▂▂▃▇█▇▅▅▄▆▅▅▆▄▄▃▄▆▆▃▅▅▆▅▅▅▆▆▆▆▆▆▅▆█▇</td></tr><tr><td>Thermal Comfort Valid Loss</td><td>█▄▄▄▄▄▃▃▃▃▃▃▃▃▃▃▂▂▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▂▂▁</td></tr><tr><td>Thermal Sensation Accuracy</td><td>▁▁▁▁▃▃▂▂▂▃▃▃▁▁▂▄▅▂▃▃▄▇█▆▆▅▅▅▆▄▅▄▃▃▅▄▄▅▅█</td></tr><tr><td>Thermal Sensation Loss</td><td>█▆▅▄▄▄▄▃▃▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>Thermal Sensation Valid Accuracy</td><td>▃▃▃▃▅▇█▇▇▇▇▇████████▇▅▅▆▆▆▆▆▆▆▅▅▆▅▅▃▃▁▁▁</td></tr><tr><td>Thermal Sensation Valid Loss</td><td>█▅▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▂▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Thermal Accept Accuracy</td><td>0.64915</td></tr><tr><td>Thermal Accept Loss</td><td>0.01217</td></tr><tr><td>Thermal Accept Valid Accuracy</td><td>0.63253</td></tr><tr><td>Thermal Accept Valid Loss</td><td>0.01284</td></tr><tr><td>Thermal Comfort Accuracy</td><td>0.59491</td></tr><tr><td>Thermal Comfort Loss</td><td>0.01454</td></tr><tr><td>Thermal Comfort Valid Accuracy</td><td>0.58534</td></tr><tr><td>Thermal Comfort Valid Loss</td><td>0.01503</td></tr><tr><td>Thermal Sensation Accuracy</td><td>0.47405</td></tr><tr><td>Thermal Sensation Loss</td><td>0.02081</td></tr><tr><td>Thermal Sensation Valid Accuracy</td><td>0.4488</td></tr><tr><td>Thermal Sensation Valid Loss</td><td>0.02218</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">stellar-sweep-2</strong> at: <a href='https://wandb.ai/lhk/Thermal-with-Fully-Chinese/runs/cvttr8jr' target=\"_blank\">https://wandb.ai/lhk/Thermal-with-Fully-Chinese/runs/cvttr8jr</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240322_004615-cvttr8jr\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.agent(sweep_id=sweep_id,function=train, count=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------ACCURACYYYYYYYY----------\n",
      "Thermal Accept: 64.9598% \n",
      "Thermal Comfort: 59.1365% \n",
      "Thermal Sensation: 45.6827%\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    n_accept_correct = 0\n",
    "    n_comfort_correct = 0\n",
    "    n_sensation_correct = 0\n",
    "    for index, data in enumerate(test_dataloader):\n",
    "        inputs = data[\"features\"].to(device=device)\n",
    "\n",
    "        thermal_accept = data[\"thermal_accept\"].to(device=device)\n",
    "        thermal_comfort = data[\"thermal_comfort\"].to(device=device)\n",
    "        thermal_sensation = data[\"thermal_sensation\"].to(device=device)\n",
    "\n",
    "        thermal_accept_output, thermal_comfort_output, thermal_sensation_output  = model(inputs)\n",
    "\n",
    "        _, thermal_accept_predicted = torch.max(thermal_accept_output,1)\n",
    "        n_accept_correct += (thermal_accept_predicted == thermal_accept).sum().item()\n",
    "\n",
    "        _, thermal_comfort_predicted = torch.max(thermal_comfort_output,1)\n",
    "        n_comfort_correct += (thermal_comfort_predicted == thermal_comfort).sum().item()\n",
    "\n",
    "        _, thermal_sensation_predicted = torch.max(thermal_sensation_output,1)\n",
    "        n_sensation_correct += (thermal_sensation_predicted == thermal_sensation).sum().item()\n",
    "\n",
    "\n",
    "accuracy_thermal_accept = 100*(n_accept_correct/len(test_data))\n",
    "accuracy_thermal_comfort = 100*(n_comfort_correct/len(test_data))\n",
    "accuracy_thermal_sensation = 100*(n_sensation_correct/len(test_data))\n",
    "\n",
    "\n",
    "print(f\"----------ACCURACYYYYYYYY----------\\nThermal Accept: {accuracy_thermal_accept:.4f}% \\nThermal Comfort: {accuracy_thermal_comfort:.4f}% \\nThermal Sensation: {accuracy_thermal_sensation:.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the model to ONNX Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),\"runs/3_output_v0.4/model/howdoyoufeel.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "import onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ThermalNeuralModel().to(device=device)\n",
    "model.load_state_dict(torch.load(\"runs/3_output_v0.3/model/howdoyoufeel.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 44])\n"
     ]
    }
   ],
   "source": [
    "for index, data in enumerate(test_dataloader):\n",
    "        inputs = data[\"features\"].to(device=device)\n",
    "        print(inputs.shape)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\LHK\\Study\\Lab\\SmartCity\\3.Others\\ThermalComfort\\.conda\\lib\\site-packages\\torch\\onnx\\utils.py:2095: UserWarning: Provided key output for dynamic axes is not a valid input/output name\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "torch_input = torch.randn(1,1,1,44,requires_grad=False).to('cuda')\n",
    "torch_output = model(torch_input)\n",
    "onnx_program = torch.onnx.export(model, torch_input, \"runs/3_output_v0.4/model/howdoyoufeel.onnx\",\n",
    "                                 export_params=True,\n",
    "                                 opset_version=10,\n",
    "                                 do_constant_folding=True,\n",
    "                                 input_names=['input'],\n",
    "                                 output_names=['thermal_accept', 'thermal_comfort', 'thermal_sensation'],\n",
    "                                 dynamic_axes={'input':{0:\"BATCH_SIZE\"},\n",
    "                                               'output':{0:'BATCH_SIZE'}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model = onnx.load(\"runs/3_output_v0.2/model/howdoyoufeel.onnx\")\n",
    "onnx.checker.check_model(onnx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.4826, -0.7066,  0.5693, -0.3838,  0.1825, -0.4439,  0.9115,\n",
       "           -1.5038,  1.6719, -0.6861,  1.5177, -0.5249,  2.2697, -1.3663,\n",
       "            0.3071, -0.5139, -1.0905, -1.2862, -0.8850, -0.1891, -0.2712,\n",
       "            0.3541, -1.1853, -1.1704,  0.8803,  0.1760,  0.4272, -1.1351,\n",
       "           -1.3588,  0.8517, -0.3747,  1.2604,  0.3989,  0.8105,  0.1295,\n",
       "            0.5229,  0.5804, -0.1092, -0.1623,  0.3918, -0.6512, -1.5955,\n",
       "           -1.1637, -1.0300]]]], device='cuda:0')"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output from Torch model: \n",
      "(tensor([[[[-1.2250,  0.3207, -3.1234,  2.3189]]]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>), tensor([[[[ 0.8645,  0.8995,  0.1781, -0.3676,  0.3345]]]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>), tensor([[[[-0.3605, -0.0172, -0.6571,  0.6817,  0.0479,  0.1444,  0.1339]]]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward0>))\n",
      "----------------------\n",
      "Output from ONNX model: \n",
      "[array([[[[-1.2249525 ,  0.32065603, -3.1233754 ,  2.3188586 ]]]],\n",
      "      dtype=float32), array([[[[ 0.86445314,  0.8995396 ,  0.17808047, -0.36761838,\n",
      "           0.33453998]]]], dtype=float32), array([[[[-0.36052948, -0.01719668, -0.657147  ,  0.68170255,\n",
      "           0.04792242,  0.14438152,  0.13392255]]]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "ort_session = onnxruntime.InferenceSession(\"runs/3_output_v0.2/model/howdoyoufeel.onnx\", providers=[\"CPUExecutionProvider\"])\n",
    "\n",
    "def to_numpy(tensor):\n",
    "    return tensor.cpu().detach().numpy() \n",
    "\n",
    "# compute ONNX Runtime output prediction\n",
    "ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(torch_input)}\n",
    "ort_outs = ort_session.run(None, ort_inputs)\n",
    "\n",
    "print(f\"Output from Torch model: \\n{torch_output}\\n----------------------\\nOutput from ONNX model: \\n{ort_outs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
